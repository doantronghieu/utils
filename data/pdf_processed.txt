

9 Testing an API Asynchronously with pytest and HTTPX
In software development, a significant part of the developer’s work should be dedicated to writing tests. At first, you may be tempted to manually test your application by running it, making a few requests, and arbitrarily deciding that “everything works.” However, this approach is flawed and can’t guarantee that your program works in every circumstance and that you didn’t break things along the way.
That’s why several disciplines have emerged regarding software testing: unit tests, integration tests, end-to-end tests, acceptance tests, and others. These techniques aim to validate the functionality of software from a micro level, where we test single functions (unit tests), to a macro level, where we test a global feature that delivers value to the user (acceptance tests). In this chapter, we’ll focus on the first level: unit testing.
Unit tests are short programs designed to verify that our code behaves the way it should in every circumstance. You may think that tests are time-consuming to write and that they don’t add value to your software, but this will save you time in the long run: first of all, tests can be run automatically in a few seconds, ensuring that all your software works, without you needing to manually go over every feature. Secondly, when you introduce new features or refactor the code, you’re ensuring that you don’t introduce bugs to existing parts of the software. In conclusion, tests are just as important as the program itself, and they help you deliver reliable and high-quality software.
In this chapter, you’ll learn how to write tests for your FastAPI application, both for HTTP endpoints and WebSockets. To help with this, you’ll learn how to configure pytest, a well-known Python test framework, and HTTPX, an asynchronous HTTP client for Python.
In this chapter, we’re going to cover the following main topics:
An introduction to unit testing with pytest
Setting up the testing tools for FastAPI with HTTPX
Writing tests for REST API endpoints
Writing tests for WebSocket endpoints
Testing an API Asynchronously with pytest and HTTPX
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
For the Communicating with a MongoDB database using Motor section, you’ll need a running MongoDB server on your local computer. The easiest way to do this is to run it as a Docker container. If you’ve never used Docker before, we recommend that you refer to the Getting started tutorial in the official documentation at https://docs.docker.com/get-started/. Once you have done this, you’ll be able to run a MongoDB server using this simple command:
$ docker run -d --name fastapi-mongo -p 27017:27017 mongo:6.0
The MongoDB server instance will then be available on your local computer at port 27017.
You’ll find all the code examples of this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter09.
An introduction to unit testing with pytest
As we mentioned in the introduction, writing unit tests is an essential task in software development to deliver high-quality software. To help us be productive and efficient, a lot of libraries exist that provide tools and shortcuts dedicated to testing. In the Python standard library, a module exists for unit testing called unittest. Even though it’s quite common in Python code bases, many Python developers tend to prefer pytest, which provides a more lightweight syntax and powerful tools for advanced use cases.
In the following examples, we’ll write a unit test for a function called add, both with unittest and pytest, so that you can see how they compare on a basic use case. First, we’ll install pytest:
(venv) $ pip install pytest
Now, let’s see our simple add function, which simply performs an addition:
chapter09_introduction.py
def add(a: int, b: int) -> int: return a + b
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction.py
An introduction to unit testing with pytest
Now, let’s implement a test that checks that 2 + 3 is indeed equal to 5 with unittest:
chapter09_introduction_unittest.py
import unittest
from chapter09.chapter09_introduction import add
class TestChapter09Introduction(unittest.TestCase): def test_add(self): self.assertEqual(add(2, 3), 5)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_unittest.py
As you can see, unittest expects us to define a class inheriting from TestCase. Then, each test lives in its own method. To assert that two values are equal, we must use the assertEqual method.
To run this test, we can call the unittest module from the command line and pass it through the dotted path to our test module:
(venv) $ python -m unittest chapter09.chapter09_introduction_unittest . ---------------------------------------------------------------------- Ran 1 test in 0.000s
OK
In the output, each successful test is represented by a dot. If one or several tests are not successful, you will get a detailed error report for each, highlighting the failing assertion. You can try it by changing the assertion in the test.
Now, let’s write the same test with pytest:
chapter09_introduction_pytest.py
from chapter09.chapter09_introduction import add
def test_add(): assert add(2, 3) == 5
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_pytest.py
Testing an API Asynchronously with pytest and HTTPX
As you can see, it’s much shorter! Indeed, with pytest, you don’t necessarily have to define a class: a simple function is enough. The only constraint to making it work is that the function name has to start with test_. This way, pytest can automatically discover the test functions. Secondly, it relies on the built-in assert statement instead of specific methods, allowing you to write comparisons more naturally.
To run this test, we must simply call the pytest executable with the path to our test file:
(venv) $ pytest chapter09/chapter09_introduction_pytest.py =============== test session starts =============== platform darwin -- Python 3.10.8, pytest-7.2.0, pluggy-1.0.0 rootdir: /Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition, configfile: pyproject.toml plugins: asyncio-0.20.2, cov-4.0.0, anyio-3.6.2 asyncio: mode=strict collected 1 item
chapter09/chapter09_introduction_pytest.py . [100%]
================ 1 passed in 0.01s ===============
Once again, the output represents each successful test with a dot. Of course, if you change the test to make it fail, you’ll get a detailed error for the failing assertion.
It’s worth noting that if you run pytest without any arguments, it’ll automatically discover all the tests living in your project, as long as their name starts with test_.
Here, we made a small comparison between unittest and pytest. For the rest of this chapter, we’ll stick with pytest, which should give you a more productive experience while writing tests.
Before focusing on FastAPI testing, let’s review two of the most powerful features of pytest: parametrize and fixtures.
Generating tests with parametrize
In our previous example, with the add function, we only tested one addition test, 2 + 3. Most of the time, we’ll want to check for more cases to ensure our function works in every circumstance. Our first approach could be to add more assertions to our test, like so:
def test_add(): assert add(2, 3) == 5 assert add(0, 0) == 0 assert add(100, 0) == 100 assert add(1, 1) == 2
An introduction to unit testing with pytest
While working, this method has two drawbacks: first, it may be a bit cumbersome to write the same assertion several times with only some parameters changing. In this example, it’s not too bad, but tests can be way more complex, as we’ll see with FastAPI. Second, we still only have one test: the first failing assertion will stop the test and the following ones won’t be executed. Thus, we’ll only know the result if we fix the failing assertion first and run the test again.
To help with this specific task, pytest provides the parametrize marker. In pytest, a marker is a special decorator that’s used to easily pass metadata to the test. Special behaviors can then be implemented, depending on the markers used by the test.
Here, parametrize allows us to define several sets of variables that will be passed as arguments to the test function. At runtime, each set will generate a new and independent test. To understand this better, let’s look at how to use this marker to generate several tests for our add function:
chapter09_introduction_pytest_parametrize.py
import pytest
from chapter09.chapter09_introduction import add
@pytest.mark.parametrize("a,b,result", [(2, 3, 5), (0, 0, 0), (100, 0, 100), (1, 1, 2)]) def test_add(a, b, result): assert add(a, b) == result
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_pytest_parametrize.py
Here, you can see that we simply decorated our test function with the parametrize marker. The basic usage is as follows: the first argument is a string with the name of each parameter separated by a comma. Then, the second argument is a list of tuples. Each tuple contains the values of the parameters in order.
Our test function receives those parameters in arguments, each one named the way you specified previously. Thus, you can use them at will in the test logic. As you can see, the great benefit here is that we only have to write the assert statement once. Besides, it’s very quick to add a new test case: we just have to add another tuple to the parametrize marker.
Now, let’s run this test to see what happens by using the following command:
(venv) $ pytest chapter09/chapter09_introduction_pytest_parametrize.py ================ test session starts ================ platform darwin -- Python 3.10.8, pytest-7.2.0, pluggy-1.0.0 rootdir: /Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition, configfile: pyproject.toml
Testing an API Asynchronously with pytest and HTTPX
plugins: asyncio-0.20.2, cov-4.0.0, anyio-3.6.2 asyncio: mode=strict collected 4 items
chapter09/chapter09_introduction_pytest_parametrize.py .... [100%]
================ 4 passed in 0.01s ================
As you can see, pytest executed four tests instead of one! This means that it generated four independent tests, along with their own sets of parameters. If several tests fail, we’ll be informed, and the output will tell us which set of parameters caused the error.
To conclude, parametrize is a very convenient way to test different outcomes when it’s given a different set of parameters.
While writing unit tests, you’ll often need variables and objects several times across your tests, such as app instances, fake data, and so on. To avoid having to repeat the same things over and over across your tests, pytest proposes an interesting feature: fixtures.
Reusing test logic by creating fixtures
When testing a large application, tests tend to become quite repetitive: lots of them will share the same boilerplate code before their actual assertion. Consider the following Pydantic models representing a person and their postal address:
chapter09_introduction_fixtures.py
from datetime import date from enum import Enum
from pydantic import BaseModel
class Gender(str, Enum): MALE = "MALE" FEMALE = "FEMALE" NON_BINARY = "NON_BINARY"
class Address(BaseModel): street_address: str postal_code: str city: str country: str
class Person(BaseModel): first_name: str
An introduction to unit testing with pytest
last_name: str gender: Gender birthdate: date interests: list[str] address: Address
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_fixtures.py
This example may look familiar: it was taken from Chapter 4, Managing Pydantic Data Models in FastAPI. Now, let’s say that we want to write tests with some instances of those models. Obviously, it would be a bit annoying to instantiate them in each test, filling them with fake data.
Fortunately, fixtures allow us to write them once and for all. The following example shows how to use them:
chapter09_introduction_fixtures_test.py
import pytest
from chapter09.chapter09_introduction_fixtures import Address, Gender, Person
@pytest.fixture def address(): return Address( street_address="12 Squirell Street", postal_code="424242", city="Woodtown", country="US", )
@pytest.fixture def person(address): return Person( first_name="John", last_name="Doe", gender=Gender.MALE, birthdate="1991-01-01", interests=["travel", "sports"], address=address, )
def test_address_country(address):
Testing an API Asynchronously with pytest and HTTPX
assert address.country == "US"
def test_person_first_name(person): assert person.first_name == "John"
def test_person_address_city(person): assert person.address.city == "Woodtown"
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_fixtures_test.py
Once again, pytest makes it very straightforward: fixtures are simple functions decorated with the fixture decorator. Inside, you can write any logic and return the object you’ll need in your tests. Here, in address, we instantiate an Address object with fake data and return it.
Now, how can we use this fixture? If you look at the test_address_country test, you’ll see some magic happening: by setting an address argument on the test function, pytest automatically detects that it corresponds to the address fixture, executes it, and passes its return value. Inside the test, we have our Address object ready to use. pytest calls this requesting a fixture.
You may have noticed that we also defined another fixture, person. Once again, we instantiate a Person model with dummy data. The interesting thing to note, however, is that we actually requested the address fixture to use it inside! That’s what makes this system so powerful: fixtures can depend on other fixtures, which can also depend on others, and so on. In some way, it’s quite similar to dependency injection, as we discussed in Chapter 5, Dependency Injection in FastAPI.
With that, our quick introduction to pytest has come to an end. Of course, there are so many more things to say, but this will be enough for you to get started. If you want to explore this topic further, you can read the official pytest documentation, which includes tons of examples showing you how you can benefit from all its features: https://docs.pytest.org/en/latest/.
Now, let’s focus on FastAPI. We’ll start by setting up the tools for testing our applications.
Setting up testing tools for FastAPI with HTTPX
If you look at the FastAPI documentation regarding testing, you’ll see that it recommends that you use TestClient provided by Starlette. In this book, we’ll show you a different approach involving an HTTP client called HTTPX.
Setting up testing tools for FastAPI with HTTPX
Why? The default TestClient is implemented in a way that makes it completely synchronous, meaning you can write tests without worrying about async and await. This might sound nice, but we found that it causes some problems in practice: since your FastAPI app is designed to work asynchronously, you’ll likely have lots of services working asynchronously, such as the database drivers we saw in Chapter 6, Databases and Asynchronous ORMs. Thus, in your tests, you’ll probably need to perform some actions on those asynchronous services, such as filling a database with dummy data, which will make your tests asynchronous anyway. Melding the two approaches often leads to strange errors that are hard to debug.
Fortunately, HTTPX, an HTTP client created by the same team as Starlette, allows us to have a pure asynchronous HTTP client able to make requests to our FastAPI app. To make this approach work, we’ll need three libraries:
HTTPX, the client that will perform HTTP requests
asgi-lifespan, a library for managing the lifespan events of your FastAPI app programmatically
pytest-asyncio, an extension for pytest that allows us to write asynchronous tests
Let’s install those libraries using the following command:
(venv) $ pip install httpx asgi-lifespan pytest-asyncio
Great! Now, let’s write some fixtures so that we can easily get an HTTP test client for a FastAPI application. This way, when writing a test, we’ll only have to request the fixture and we’ll be able to make a request right away.
In the following example, we are considering a simple FastAPI application that we want to test:
chapter09_app.py
import contextlib
from fastapi import FastAPI
@contextlib.asynccontextmanager async def lifespan(app: FastAPI): print("Startup") yield print("Shutdown")
app = FastAPI(lifespan=lifespan)
@app.get("/")
Testing an API Asynchronously with pytest and HTTPX
async def hello_world(): return {"hello": "world"}
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app.py
In a separate test file, we’ll implement two fixtures.
The first one, event_loop, will ensure that we always work with the same event loop instance. It’s automatically requested by pytest-asyncio before executing asynchronous tests. You can see its implementation in the following example:
chapter09_app_test.py
@pytest.fixture(scope="session") def event_loop(): loop = asyncio.new_event_loop() yield loop loop.close()
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_test.py
Here, you can see that we simply create a new event loop before yielding it. As we discussed in Chapter 2, Python Programming Specificities, using a generator allows us to “pause” the function’s execution and get back to the execution of its caller. This way, when the caller is done, we can execute cleanup operations, such as closing the loop. pytest is smart enough to handle this correctly in fixtures, so this is a very common pattern for setting up test data, using it, and destroying it after. We also use the same approach for lifespan functions in FastAPI.
Of course, this function is decorated with the fixture decorator to make it a fixture for pytest. You may have noticed that we set an argument called scope with the session value. This argument controls at which level the fixture should be instantiated. By default, it’s recreated at the beginning of each single test function. The session value is the highest level, meaning that the fixture is only created once at the beginning of the whole test run, which is relevant for our event loop. You can find out more about this more advanced feature in the official documentation: https://docs. pytest.org/en/latest/how-to/fixtures.html#scope-sharing-fixtures- across-classes-modules-packages-or-session.
Setting up testing tools for FastAPI with HTTPX
Next, we’ll implement our test_client fixture, which will create an instance of HTTPX for our FastAPI application. We must also remember to trigger the app events with asgi-lifespan. You can see what it looks like in the following example:
chapter09_app_test.py
@pytest_asyncio.fixture async def test_client(): async with LifespanManager(app): async with httpx.AsyncClient(app=app, base_url="http://app. io") as test_client: yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_test.py
Only three lines are needed. The first difference with fixtures we’ve seen so far is that this is an async function. In this case, notice that we used the @pytest_asyncio.fixture decorator instead of @pytest.fixture. It’s the async counterpart of this decorator provided by pytest-asyncio so async fixtures are correctly handled. In previous versions, using the standard decorator used to work but it’s now discouraged.
Then, we have two context managers: LifespanManager and httpx.AsyncClient. The first one ensures startup and shutdown events are executed, while the second one ensures that an HTTP session is ready. On both of them, we set the app variable: this is our FastAPI application instance we imported from its module, chapter09.chapter09_app import app.
Notice that we once again used a generator here, with yield. This is important because, even if we don’t have any more code after, we need to close the context managers after we use our client. If we used return, Python would have immediately closed them and we would end up with an unusable client.
Organizing tests and global fixtures in projects In larger projects, you’ll likely have several test files to keep your tests organized. Usually, those files are placed in a tests folder at the root of your project. If your test files are prefixed with test_, they will be automatically discovered by pytest. Figure 9.1 shows an example of this.
Besides this, you’ll need the fixtures we defined in this section for all your tests. Rather than repeating them again and again in all your test files, pytest allows you to write global fixtures in a file named conftest.py. After putting it in your tests folder, it will automatically be imported, allowing you to request all the fixtures you define inside it. You can read more about this in the official documentation at https://docs.pytest.org/en/latest/ reference/fixtures.html#conftest-py-sharing-fixtures-across- multiple-files.
Testing an API Asynchronously with pytest and HTTPX
As mentioned previously, Figure 9.1 shows the test files in the tests folder:
Figure 9.1 – Structure of a project with tests
That’s it! We now have all the fixtures ready to write tests for our REST API endpoints. That’s what we’ll do in the next section.
Writing tests for REST API endpoints
All the tools we need to test our FastAPI application are now ready. All these tests boil down to performing an HTTP request and checking the response to see whether it corresponds to what we expect.
Let’s start simply with a test for our hello_world path operation function. You can see it in the following code:
chapter09_app_test.py
@pytest.mark.asyncio async def test_hello_world(test_client: httpx.AsyncClient): response = await test_client.get("/")
assert response.status_code == status.HTTP_200_OK
Writing tests for REST API endpoints
json = response.json() assert json == {"hello": "world"}
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_test.py
First of all, notice that the test function is defined as async. As we mentioned previously, to make it work with pytest, we had to install pytest-asyncio. This extension provides the asyncio marker: each asynchronous test should be decorated with this marker to make it work properly.
Next, we request our test_client fixture, which we defined earlier. It gives us an HTTPX client instance ready to make requests to our FastAPI app. Note that we manually type hinted the fixture. While not strictly required, it’ll greatly help you if you use an IDE such as Visual Studio Code, which uses type hints to provide you with convenient auto-completion features.
Then, in the body of our test, we perform the request. Here, it’s a simple GET request to the / path. It returns an HTTPX Response object (which is different from the Response class of FastAPI) containing all the data of the HTTP response: the status code, the headers, and the body.
Finally, we make assertions based on this data. As you can see, we verify that the status code is indeed 200. We also check the content of the body, which is a simple JSON object. Notice that the Response object has a convenient method called json for automatically parsing JSON content.
Great! We wrote our first FastAPI test! Of course, you’ll likely have more complex tests, typically ones for POST endpoints.
Writing tests for POST endpoints
Testing a POST endpoint is not very different from what we’ve seen earlier. The difference is that we’ll likely have more cases to check whether data validation is working. In the following example, we are implementing a POST endpoint that accepts a Person model in the body:
chapter09_app_post.py
class Person(BaseModel): first_name: str last_name: str age: int
@app.post("/persons", status_code=status.HTTP_201_CREATED) async def create_person(person: Person): return person
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_post.py
Testing an API Asynchronously with pytest and HTTPX
An interesting test could be to ensure that an error is raised if some fields are missing in the request payload. In the following extract, we wrote two tests – one with an invalid payload and another with a valid one:
chapter09_app_post_test.py
@pytest.mark.asyncio class TestCreatePerson: async def test_invalid(self, test_client: httpx.AsyncClient): payload = {"first_name": "John", "last_name": "Doe"} response = await test_client.post("/persons", json=payload)
assert response.status_code == status.HTTP_422_UNPROCESSABLE_ ENTITY
async def test_valid(self, test_client: httpx.AsyncClient): payload = {"first_name": "John", "last_name": "Doe", "age": 30} response = await test_client.post("/persons", json=payload)
assert response.status_code == status.HTTP_201_CREATED
json = response.json() assert json == payload
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter09/chap- ter09_app_post_test.py
The first thing you may have noticed is that we wrapped our two tests inside a class. While not required in pytest, it could help you organize your tests – for example, to regroup tests that concern a single endpoint. Notice that, in this case, we only have to decorate the class with the asyncio marker; it will be automatically applied on single tests. Also, ensure that you add the self argument to each test: since we are now inside a class, they become methods.
These tests are not very different from our first example. As you can see, the HTTPX client makes it very easy to perform POST requests with a JSON payload: you just have to pass a dictionary to the json argument.
Of course, HTTPX helps you build all kinds of HTTP requests with headers, query parameters, and so on. Be sure to check its official documentation to learn more about its usage: https://www. python-httpx.org/quickstart/.
Writing tests for REST API endpoints
Testing with a database
Your application will likely have a database connection to read and store data. In this context, you’ll need to work with a fresh test database in each run to have a clean and predictable set of data to write your tests.
For this, we’ll use two things. The first one, dependency_overrides, is a FastAPI feature that allows us to replace some dependencies at runtime. For example, we can replace the dependency that returns the database instance with another one that returns a test database instance. The second one is, once again, fixtures, which will help us create fake data in the test database before we run the tests.
To show you a working example, we’ll consider the same example we built in the Communicating with a MongoDB database with Motor section of Chapter 6, Databases and Asynchronous ORMs. In that example, we built REST endpoints to manage blog posts. As you may recall, we had a get_database dependency that returned the database instance. As a reminder, we show it again here:
database.py
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase
# Connection to the whole server motor_client = AsyncIOMotorClient("mongodb://localhost:27017") # Single database instance database = motor_client["chapter6_mongo"]
def get_database() -> AsyncIOMotorDatabase: return database
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter6/mongodb/ database.py
Path operation functions and other dependencies would then use this dependency to retrieve the database instance.
For our tests, we’ll create a new instance of AsyncIOMotorDatabase that points to another database. Then, we’ll create a new dependency, directly in our test file, that returns this instance. You can see this in the following example:
chapter09_db_test.py
motor_client = AsyncIOMotorClient( os.getenv("MONGODB_CONNECTION_STRING", "mongodb:// localhost:27017") )
Testing an API Asynchronously with pytest and HTTPX
database_test = motor_client["chapter09_db_test"]
def get_test_database(): return database_test
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
Then, in our test_client fixture, we’ll override the default get_database dependency by using our current get_test_database dependency. The following example shows how this is done:
chapter09_db_test.py
@pytest_asyncio.fixture async def test_client(): app.dependency_overrides[get_database] = get_test_database async with LifespanManager(app): async with httpx.AsyncClient(app=app, base_url="http://app. io") as test_client: yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
FastAPI provides a property called dependency_overrides, which is a dictionary that maps original dependency functions with substitutes. Here, we directly used the get_database function as a key. The rest of the fixture doesn’t have to change. Now, whenever the get_database dependency is injected into the application code, FastAPI will automatically replace it with get_test_database. As a result, our endpoints will now work with the test database instance.
app and dependency_overrides are global Since we are directly importing app from its module, it’s instantiated only once for the whole test run. It means that dependency_overrides is common for every test. Keep this in mind if someday you want to override a dependency for a single test: once you’ve set it, it’ll be set for the rest of the execution. In this case, you can reset dependency_overrides by using app.dependency_overrides = {}.
Writing tests for REST API endpoints
To test some behaviors, such as retrieving a single post, it’s usually convenient to have some base data in our test database. To allow this, we’ll create a new fixture that will instantiate dummy PostDB objects and insert them into the test database. You can see this in the following example:
chapter09_db_test.py
@pytest_asyncio.fixture(autouse=True, scope="module") async def initial_posts(): initial_posts = [ Post(title="Post 1", content="Content 1"), Post(title="Post 2", content="Content 2"), Post(title="Post 3", content="Content 3"), ] await database_test["posts"].insert_many( [post.dict(by_alias=True) for post in initial_posts] )
yield initial_posts
await motor_client.drop_database("chapter09_db_test")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
Here, you can see that we just had to make an insert_many request to the MongoDB database to create the posts.
Notice that we used the autouse and scope arguments of the fixture decorator. The first one tells pytest to automatically call this fixture even if it’s not requested in any test. In this case, it’s convenient because we’ll always ensure that the data has been created in the database, without the risk of forgetting to request it in the tests. The other one, scope, allows us, as we mentioned previously, to not run this fixture at the beginning of each test. With the module value, the fixture will create the objects only once, at the beginning of this particular test file. It helps make the test fast because, in this case, it doesn’t make sense to recreate the posts before each test.
Once again, we yield the posts instead of returning them. This pattern allows us to delete the test database after the tests run. By doing this, we’re making sure that we always start with a fresh database when we’ve run the tests.
Testing an API Asynchronously with pytest and HTTPX
And we are done! We can now write tests while knowing exactly what we have in the database. In the following example, you can see tests that are used to verify the behavior of the endpoint retrieving a single post:
chapter09_db_test.py
@pytest.mark.asyncio class TestGetPost: async def test_not_existing(self, test_client: httpx.AsyncClient): response = await test_client.get("/posts/abc")
assert response.status_code == status.HTTP_404_NOT_FOUND
async def test_existing( self, test_client: httpx.AsyncClient, initial_posts: list[Post] ): response = await test_client.get(f"/posts/{initial_posts[0]. id}")
assert response.status_code == status.HTTP_200_OK
json = response.json() assert json["_id"] == str(initial_posts[0].id)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
Notice that we requested the initial_posts fixture in the second test to retrieve the identifier of the post that truly exists in our database.
Of course, we can also test our endpoints by creating data and checking whether it was correctly inserted into the database. You can see this in the following example:
chapter09_db_test.py
@pytest.mark.asyncio class TestCreatePost: async def test_invalid_payload(self, test_client: httpx. AsyncClient): payload = {"title": "New post"} response = await test_client.post("/posts", json=payload)
assert response.status_code == status.HTTP_422_UNPROCESSABLE_ ENTITY
Writing tests for REST API endpoints
async def test_valid_payload(self, test_client: httpx. AsyncClient): payload = {"title": "New post", "content": "New post content"} response = await test_client.post("/posts", json=payload)
assert response.status_code == status.HTTP_201_CREATED
json = response.json() post_id = ObjectId(json["_id"]) post_db = await database_test["posts"].find_one({"_id": post_ id}) assert post_db is not None
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
In the second test, we used the database_test instance to perform a request and check that the object was inserted correctly. This shows the benefit of using asynchronous tests: we can use the same libraries and tools inside our tests.
That’s all you need to know about dependency_overrides. This feature is also very helpful when you need to write tests for logic involving external services, such as external APIs. Instead of making real requests to those external services during your tests, which could cause issues or incur costs, you’ll be able to replace them with another dependency that fakes the requests. To understand this, we’ve built another example application with an endpoint for retrieving data from an external API:
chapter09_app_external_api.py
class ExternalAPI: def __init__(self) -> None: self.client = httpx.AsyncClient(base_url="https://dummyjson. com")
async def __call__(self) -> dict[str, Any]: async with self.client as client: response = await client.get("/products") return response.json()
external_api = ExternalAPI()
@app.get("/products")
Testing an API Asynchronously with pytest and HTTPX
async def external_products(products: dict[str, Any] = Depends(external_api)): return products
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_external_api.py
To call our external API, we’ve built a class dependency, as we saw in the Creating and using a parameterized dependency with a class section of Chapter 5, Dependency Injection in FastAPI. We use HTTPX as an HTTP client to make a request to the external API and retrieve the data. This external API is a dummy API containing fake data – very useful for experiments like this: https://dummyjson.com.
The /products endpoint is simply injected with this dependency and directly returns the data provided by the external API.
Of course, to test this endpoint, we don’t want to make real requests to the external API: it may take time and could be subject to rate limiting. Besides, you may want to test behavior that is not easy to reproduce in the real API, such as errors.
Thanks to dependency_overrides, it’s very easy to replace our ExternalAPI dependency class with another one that returns static data. In the following example, you can see how we implemented such a test:
chapter09_app_external_api_test.py
class MockExternalAPI: mock_data = { "products": [ { "id": 1, "title": "iPhone 9", "description": "An apple mobile which is nothing like apple", "thumbnail": "https://i.dummyjson.com/data/products/1/ thumbnail.jpg", }, ], "total": 1, "skip": 0, "limit": 30, }
async def __call__(self) -> dict[str, Any]: return MockExternalAPI.mock_data
Writing tests for WebSocket endpoints
@pytest_asyncio.fixture async def test_client(): app.dependency_overrides[external_api] = MockExternalAPI() async with LifespanManager(app): async with httpx.AsyncClient(app=app, base_url="http://app. io") as test_client: yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_external_api_test.py
Here, you can see that we wrote a simple class called MockExternalAPI that returns hardcoded data. All we have to do then is override the original dependency with this one. During the tests, the external API won’t be called; we’ll only work with the static data.
With the guidelines we’ve seen so far, you can now write tests for any HTTP endpoints in your FastAPI app. However, there is another kind of endpoint that behaves differently: WebSockets. As we’ll see in the next section, unit testing WebSockets is also quite different from what we described for REST endpoints.
Writing tests for WebSocket endpoints
In Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI, we explained how WebSockets work and how you can implement such endpoints in FastAPI. As you may have guessed, writing unit tests for WebSockets endpoints is quite different from what we’ve seen so far.
For this task, we’ll need to tweak our test_client fixture a little bit. Indeed, HTTPX doesn’t have built-in support to communicate with WebSockets. Hence, we’ll need to use a plugin, HTTPX WS. Let’s install it with the following command:
(venv) $ pip install httpx-ws
To enable support for WebSockets on our test client, we’ll change it like this:
chapter09_websocket_test.py
from httpx_ws.transport import ASGIWebSocketTransport
@pytest_asyncio.fixture async def test_client(): async with LifespanManager(app): async with httpx.AsyncClient( transport=ASGIWebSocketTransport(app), base_url="http:// app.io" ) as test_client:
Testing an API Asynchronously with pytest and HTTPX
yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ websocket_test.py
You can see that, instead of directly setting the app argument, we set transport with a class provided by HTTPX WS. This class provides support to test apps with WebSockets endpoints. Other than that, nothing changes. It’s worth noting that testing standard HTTP endpoints will still work correctly, so you can use this test client for all your tests.
Now, let’s consider a simple WebSocket endpoint example:
chapter09_websocket.py
@app.websocket("/ws") async def websocket_endpoint(websocket: WebSocket): await websocket.accept() try: while True: data = await websocket.receive_text() await websocket.send_text(f"Message text was: {data}") except WebSocketDisconnect: await websocket.close()
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ websocket.py
You may have recognized the “echo” example from Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI.
Now, let’s write a test for our WebSocket using our test client:
Chapter09_websocket_test.py
from httpx_ws import aconnect_ws
@pytest.mark.asyncio async def test_websocket_echo(test_client: httpx.AsyncClient): async with aconnect_ws("/ws", test_client) as websocket: await websocket.send_text("Hello")
message = await websocket.receive_text() assert message == "Message text was: Hello"
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ websocket_test.py
As you can see, HTTPX WS provides the aconnect_ws function to open a connection to a WebSocket endpoint. It expects the path of your WebSocket endpoint and a valid HTTPX client in an argument. By using test_client, we’ll make requests directly against our FastAPI application.
It opens a context manager, giving you the websocket variable. It’s an object that exposes several methods to either send or receive data. Each of those methods will block until a message has been sent or received.
Here, to test our “echo” server, we send a message thanks to the send_text method. Then, we retrieve a message with receive_text and assert that it corresponds to what we expect. Equivalent methods also exist for sending and receiving JSON data directly: send_json and receive_json.
This is what makes WebSocket testing a bit special: you have to think about the sequence of sent and received messages and implement them programmatically to test the behavior of your WebSocket.
Other than that, all the things we’ve seen so far regarding testing are applicable, especially dependency_ overrides, when you need to use a test database.
Summary
Congratulations! You are now ready to build high-quality FastAPI applications that have been well tested. In this chapter, you learned how to use pytest, a powerful and efficient testing framework for Python. Thanks to pytest fixtures, you saw how to create a reusable test client for your FastAPI application that can work asynchronously. Using this client, you learned how to make HTTP requests to assert the behavior of your REST API. Finally, we reviewed how to test WebSocket endpoints, which involves a fairly different way of thinking.
Now that you can build a reliable and efficient FastAPI application, it’s time to bring it to the whole world! In the next chapter, we’ll review the best practices and patterns for preparing a FastAPI application for the world before studying several deployment methods.
Summary
10 Deploying a FastAPI Project
Building a good application is great, but it’s even better if customers can enjoy it. In this chapter, you’ll look at different techniques and the best practices for deploying your FastAPI application to make it available on the web. First, you’ll learn how to structure your project to make it ready for deployment by using environment variables to set the configuration options you need, as well as by managing your dependencies properly with pip. Once that’s done, we’ll show you three ways to deploy your application: with a serverless cloud platform, with a Docker container, and with a traditional Linux server.
In this chapter, we’re going to cover the following main topics:
Setting and using environment variables
Managing Python dependencies
Deploying a FastAPI application on a serverless platform
Deploying a FastAPI application with Docker
Deploying a FastAPI application on a traditional server
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
You’ll find all the code examples for this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter10.
Deploying a FastAPI Project
Setting and using environment variables
Before deep-diving into the different deployment techniques, we need to structure our application to enable reliable, fast, and secure deployments. One of the key things in this process is handling configuration variables: a database URL, an external API token, a debug flag, and so on. When handling those variables, it’s necessary to handle them dynamically instead of hardcoding them into your source code. Why?
First of all, those variables will likely be different in your local environment and in production. Typically, your database URL will point to a local database on your computer while developing but will point to a proper production database in production. This is even more pertinent if you want to have other environments such as a staging or pre-production environment. Furthermore, if we need to change one of the values, we’ll have to change the code, commit it, and deploy it again. Thus, we need a convenient mechanism to set those values.
Secondly, it’s unsafe to write those values in your code. Values such as database connection strings or API tokens are extremely sensitive. If they appear in your code, they’ll likely be committed to your repository: they can be read by anyone who has access to your repository, which causes obvious security issues.
To solve this, we usually use environment variables. Environment variables are values that aren’t set in the program itself but in the whole operating system. Most programming languages have the required functions to read those variables from the system. You can try this very easily in a Unix command line:
$ export MY_ENVIRONMENT_VARIABLE="Hello" # Set a temporary variable on the system $ python >>> import os >>> os.getenv("MY_ENVIRONMENT_VARIABLE") # Get it in Python 'Hello'
In the Python source code, we can get the value dynamically from the system. During deployment, we’ll only have to make sure that we set the correct environment variables on the server. This way, we can easily change a value without redeploying the code and have several deployments of our application containing different configurations sharing the same source code. However, bear in mind that sensitive values that have been set in environment variables can still leak if you don’t pay attention – for example, in log files or error stack traces.
To help us with this task, we’ll use a very convenient feature of Pydantic: settings management. This allows us to structure and use our configuration variables as we do for any other data model. It even takes care of automatically retrieving the values from environment variables!
For the rest of this chapter, we’ll work with an application that you can find in chapter10/project within our example repository. It’s a simple FastAPI application that uses SQLAlchemy, very similar to the one we reviewed in the Communicating with a SQL database with the SQLAlchemy ORM section of Chapter 6, Databases and Asynchronous ORMs.
Setting and using environment variables
Running the commands from the project directory If you cloned the example repository, be sure to run the commands shown in this chapter from the project directory. On the command line, simply type cd chapter10/project.
To structure a settings model, all you need to do is create a class that inherits from pydantic. BaseSettings. The following example shows a configuration class with a debug flag, an environment name, and a database URL:
settings.py
from pydantic import BaseSettings
class Settings(BaseSettings): debug: bool = False environment: str database_url: str
class Config: env_file = ".env"
settings = Settings()
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ project/settings.py
As you can see, creating this class is very similar to creating a standard Pydantic model. We can even define default values, as we did for debug here.
To use it, we only have to create an instance of this class. We can then import it wherever we need it in our project. For example, here is how to retrieve the database URL to create our SQLAlchemy engine:
database.py
from project.settings import settings
engine = create_async_engine(settings.database_url)
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ project/database.py
Deploying a FastAPI Project
We also use the debug flag to print all the settings in the lifespan event at startup:
app.py
@contextlib.asynccontextmanager async def lifespan(app: FastAPI): if settings.debug: print(settings) yield
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ project/app.py
Since our application is designed to work with SQLAlchemy, we also took care of initializing a database migration environment with Alembic, as we showed in Chapter 6, Databases and Asynchronous ORMs. The difference here is that we use our settings object to dynamically configure the database URL; instead of hardcoding it in alembic.ini, we can set it from our settings in env.py, as you can see here:
env.py
config.set_main_option( "sqlalchemy.url", settings.database_url.replace("+aiosqlite", "") )
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ alembic/env.py
Notice that we take care of manually removing the aiosqlite driver part of the URL. Indeed, as we mentioned previously, Alembic is designed to work synchronously, so we need to pass it a standard URL. Now, we can generate migrations from our development database and apply them in production without changing anything in our Alembic configuration!
The good thing with this Settings model is that it works just like any other Pydantic model: it automatically parses the values it finds in environment variables and raises an error if one value is missing in your environment. This way, you can ensure you don’t forget any values directly when the app starts. You can test this behavior by running the application:
(venv) $ uvicorn project.app:app pydantic.error_wrappers.ValidationError: 2 validation errors for Settings environment field required (type=value_error.missing)
Setting and using environment variables
database_url field required (type=value_error.missing)
We have a clear list of the missing variables. Let’s set those variables in our environment and try again:
(venv) $ export DEBUG="true" ENVIRONMENT="development" DATABASE_ URL="sqlite+aiosqlite:///chapter10_project.db" (venv) $ uvicorn project.app:app INFO: Started server process [34880] INFO: Waiting for application startup. debug=True environment='development' database_ url='sqlite+aiosqlite:///chapter10_project.db' INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
The application started! You can even see that our lifespan handler printed our settings values. Notice that Pydantic is case-insensitive (by default) when retrieving environment variables. By convention, environment variables are usually set in all caps on the system.
Using a .env file
In local development, it’s a bit annoying to set environment variables by hand, especially if you’re working on several projects at the same time on your machine. To solve this, Pydantic allows you to read the values from a .env file. This file contains a simple list of environment variables and their associated values. It’s usually easier to edit and manipulate during development.
To make this work, we’ll need a new library, python-dotenv, whose task is to parse those .env files. You can install it as usual with the following command:
(venv) $ pip install python-dotenv
To enable this feature, notice how we added the Config subclass with the env_file property:
settings.py
class Settings(BaseSettings): debug: bool = False environment: str database_url: str
class Config: env_file = ".env"
Deploying a FastAPI Project
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ project/settings.py
By doing this, we simply tell Pydantic to look for environment variables set in a file named .env, if it’s available.
Finally, you can create your .env file at the root of the project with the following content:
DEBUG=true ENVIRONMENT=development DATABASE_URL=sqlite+aiosqlite:///chapter10_project.db
And that’s it! The values will now be read from this .env file. If the file is missing, Settings will try to read them from the environment variables as usual. Of course, this is only for convenience while developing: this file shouldn’t be committed and you should rely on properly set environment variables in production. To ensure you don’t commit this file by accident, it’s usually recommended that you add it to your .gitignore file.
Creating hidden files such as .env files In Unix systems, files starting with a dot, such as .env, are considered hidden files. If you try to create them from the operating system’s file explorer, it might show you warnings or even prevent you from doing so. Thus, it’s usually more convenient to create them from your IDE, such as Visual Studio Code, or from the command line by executing the following command: touch .env.
Great! Our application now supports dynamic configuration variables, which are now easy to set and change on our deployment platforms. Another important thing to take care of is dependencies: we’ve installed quite a lot of them at this point, but we must make sure they are installed properly during deployments!
Managing Python dependencies
Throughout this book, we’ve installed libraries using pip to add some useful features to our application: FastAPI, of course, but also SQLAlchemy, pytest, and so on. When deploying a project to a new environment, such as a production server, we have to make sure all those dependencies are installed for our application to work properly. This is also true if you have colleagues that also need to work on the project: they need to know the dependencies they must install on their machines.
Fortunately, pip comes with a solution for this so that we don’t have to remember all this in our heads. Indeed, most Python projects define a requirements.txt file, which contains a list of all Python dependencies. It usually lives at the root of your project. pip has a special option for reading this file and installing all the needed dependencies.
Managing Python dependencies
When you already have a working environment, such as the one we’ve used since the beginning of this book, people usually recommend that you run the following command:
(venv) $ pip freeze aiosqlite==0.17.0 alembic==1.8.1 anyio==3.6.2 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgi-lifespan==2.0.0 asyncio-redis==0.16.0 attrs==22.1.0 ...
The result of pip freeze is a list of every Python package currently installed in your environment, along with their corresponding versions. This list can be directly used in the requirements.txt file.
The problem with this approach is that it lists every package, including the sub-dependencies of the libraries you install. Said another way, in this list, you’ll see packages that you don’t directly use but that are needed by the ones you installed. If, for some reason, you decide to not use a library anymore, you’ll be able to remove it, but it’ll be very hard to guess which sub-dependencies it has installed. In the long term, your requirements.txt file will grow larger and larger, with lots of dependencies that are useless in your project.
To solve this, some people recommend that you manually maintain your requirements.txt file. With this approach, you have to list yourself all the libraries you use, along with their respective versions. During installation, pip will take care of installing the sub-dependencies, but they’ll never appear in requirements.txt. This way, when you remove one of your dependencies, you make sure any useless packages are not kept.
In the following example, you can see the requirements.txt file for the project we are working on in this chapter:
requirements.txt
aiosqlite==0.17.0 alembic==1.8.1 fastapi==0.88.0 sqlalchemy[asyncio]==1.4.44 uvicorn[standard]==0.20.0 gunicorn==20.1.0
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ requirements.txt
Deploying a FastAPI Project
As you can see, the list is much shorter! Now, whenever we install a new dependency, our responsibility is to add it manually to requirements.txt.
A word on alternate package managers such as Poetry, Pipenv, and Conda While exploring the Python community, you may hear about alternate package managers such as Poetry, Pipenv, and Conda. These managers were created to solve some issues posed by pip, especially related to sub-dependency management. While they are very good tools, lots of cloud platforms expect a traditional requirements.txt file to specify the dependencies, rather than those more modern tools. Therefore, they may not be the best choice for a FastAPI application.
The requirements.txt file should be committed along with your source code. When you need to install the dependencies on a new computer or server, you’ll simply need to run this command:
(venv) $ pip install -r requirements.txt
Of course, make sure that you’re working on proper virtual environments when doing this, as we described in Chapter 1, Python Development Environment Setup.
You have probably noticed the gunicorn dependency in requirements.txt. Let’s look at what it is and why it’s needed.
Adding Gunicorn as a server process for deployment
In Chapter 2, Python Programming Specificities, we briefly introduced the WSGI and ASGI protocols. They define the norm and data structure for building web servers in Python. Traditional Python web frameworks, such as Django and Flask, rely on the WSGI protocol. ASGI appeared recently and is presented as the “spiritual successor” of WSGI, providing a protocol for developing web servers running asynchronously. This protocol is at the heart of FastAPI and Starlette.
As we mentioned in Chapter 3, Developing RESTful APIs with FastAPI, we use Uvicorn to run our FastAPI applications: its role is to accept HTTP requests, transform them according to the ASGI protocol, and pass them to the FastAPI application, which returns an ASGI-compliant response object. Then, Uvicorn can form a proper HTTP response from this object.
In the WSGI world, the most widely used server is Gunicorn. It has the same role in the context of a Django or Flask application. Why are we talking about it, then? Gunicorn has lots of refinements and features that make it more robust and reliable in production than Uvicorn. However, Gunicorn is designed to work for WSGI applications. So, what can we do?
Deploying a FastAPI application on a serverless platform
Actually, we can use both: Gunicorn will be used as a robust process manager for our production server. However, we’ll specify a special worker class provided by Uvicorn, which will allow us to run ASGI applications such as FastAPI. This is the recommended way of doing deployments in the official Uvicorn documentation: https://www.uvicorn.org/deployment/#using-a- process-manager.
So, let’s install Gunicorn to our dependencies by using the following command (remember to add it to your requirements.txt file):
(venv) $ pip install gunicorn
If you wish, you can try to run our FastAPI project using Gunicorn by using the following command:
(venv) $ gunicorn -w 4 -k uvicorn.workers.UvicornWorker project. app:app
Its usage is quite similar to Uvicorn, except that we tell it to use a Uvicorn worker. Once again, this is necessary to make it work with an ASGI application. Also, notice the -w option. This allows us to set the number of workers to launch for our server. Here, we launch four instances of our application. Then, Gunicorn takes care of load-balancing the incoming requests between each worker. This is what makes Gunicorn more robust: if, for any reason, your application blocks the event loop with a synchronous operation, other workers will be able to process other requests while this is happening.
Now, we are ready to deploy our FastAPI application! In the next section, you’ll learn how to deploy one on a serverless platform.
Deploying a FastAPI application on a serverless platform
In recent years, serverless platforms have gained a lot of popularity and have become a very common way to deploy web applications. Those platforms completely hide the complexity of setting up and managing a server, giving you the tools to automatically build and deploy your application in minutes. Google App Engine, Heroku, and Azure App Service are among the most popular. Even though they have their own specificities, all these serverless platforms work on the same principles. This is why, in this section, we’ll outline the common steps you should follow.
Usually, serverless platforms expect you to provide the source code in the form of a GitHub repository, which you push directly to their servers or which they pull automatically from GitHub. Here, we’ll assume that you have a GitHub repository with the source code structured like so:
Deploying a FastAPI Project
Figure 10.1 – Project structure for serverless deployment
Here are the general steps you should follow to deploy your projects on this kind of platform:
1. Create an account on a cloud platform of your choice. You must do this before you can start any work. It’s worth noting that most cloud platforms offer free credits when you are getting started so that you can try their services for free. Install the necessary command-line tools. Most cloud providers supply a complete CLI for managing their services. Typically, this is required for deploying your application. Here are the relevant documentation pages for the most popular cloud providers:
 Google Cloud: https://cloud.google.com/sdk/gcloud
 Microsoft Azure: https://docs.microsoft.com/en-us/cli/azure/install-
azure-cli
 Heroku: https://devcenter.heroku.com/articles/heroku-cli
3. Set up the application configuration. Depending on the platform, you’ll either have to create a configuration file or use the CLI or the web interface to do this. Here are the relevant documentation pages for the most popular cloud providers:  Google App Engine (configuration file): https://cloud.google.com/appengine/ docs/standard/python3/configuring-your-app-with-app-yaml
 Azure App Service (web interface and CLI): https://docs.microsoft.com/en-us/ azure/app-service/quickstart-python and https://docs.microsoft. com/en-us/azure/app-service/configure-language-python
 Heroku (configuration file): https://devcenter.heroku.com/articles/
getting-started-with-python#define-a-procfile
Deploying a FastAPI application on a serverless platform
The key point in this step is to correctly set the startup command. As we saw in the previous section, it’s essential to set the Uvicorn worker class using the gunicorn command, as well as set the correct path to your application.
4. Set the environment variables. Depending on the cloud provider, you should be able to do so during configuration or deployment. Remember that they are key for your application to work. Here are the relevant documentation pages for the most popular cloud providers:  Google App Engine (configuration file): https://cloud.google.com/appengine/
docs/standard/python/config/appref
 Azure App Service (web interface): https://docs.microsoft.com/en-us/azure/
app-service/configure-common#configure-app-settings
 Heroku (CLI or web interface): https://devcenter.heroku.com/articles/
config-vars
5. Deploy the application. Some platforms can automatically deploy when they detect changes on a hosted repository, such as GitHub. Others require that you start deployment from the command- line tools. Here are the relevant documentation pages for the most popular cloud providers:  Google App Engine (CLI): https://cloud.google.com/appengine/docs/ standard/python3/testing-and-deploying-your-app#deploying_ your_application
 Azure App Service (continuous deployment or manual Git deployment): https:// docs.microsoft.com/en-us/azure/app-service/deploy-continuous- deployment?tabs=github and https://docs.microsoft.com/en-us/ azure/app-service/deploy-local-git?tabs=cli
 Heroku (CLI): https://devcenter.heroku.com/articles/getting-
started-with-python#deploy-the-app
Your application should now be live on the platform. Under the hood, most cloud platforms actually automatically build and deploy Docker containers while following the configuration you provide.
They will make your application available on a generic subdomain such as myapplication. herokuapp.com. Of course, they also provide mechanisms for binding it to your own domain or subdomain. Here are the relevant documentation pages for the most popular cloud providers:
 Google App Engine: https://cloud.google.com/appengine/docs/standard/
python3/mapping-custom-domains
 Azure App Service: https://docs.microsoft.com/en-us/azure/app-service/
manage-custom-dns-migrate-domain
 Heroku: https://devcenter.heroku.com/articles/custom-domains
Deploying a FastAPI Project
Adding database servers
Most of the time, your application will be backed by a database engine, such as PostgreSQL. Fortunately, cloud providers propose fully managed databases, billed according to the computing power, memory, and storage you need. Once created, you’ll have access to a connection string to connect to the database instance. All you have to do then is set it in the environment variables of your application. Here are the relevant documentation pages for getting started with managed databases with the most popular cloud providers:
Google Cloud SQL: https://cloud.google.com/sql/docs/postgres/create- instance
Azure Database for PostgreSQL: https://docs.microsoft.com/en-us/azure/ postgresql/quickstart-create-server-database-portal
Amazon RDS: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/ CHAP_GettingStarted.html
Heroku Postgres: https://devcenter.heroku.com/articles/heroku-postgresql
As we’ve seen, serverless platforms are the quickest and easiest way to deploy a FastAPI application. However, in some situations, you may wish to have more control of how things are deployed, or you may need system packages that are not available on serverless platforms. In those cases, it may be worthwhile to use a Docker container.
Deploying a FastAPI application with Docker
Docker is a widely used technology for containerization. Containers are small, self-contained systems running on a computer. Each container contains all the files and configurations necessary for running a single application: a web server, a database engine, a data processing application, and so on. The main goal is to be able to run those applications without worrying about the dependency and version conflicts that often happen when trying to install and configure them on the system.
Besides, Docker containers are designed to be portable and reproducible: to create a Docker container, you simply have to write a Dockerfile containing all the necessary instructions to build the small system, along with all the files and configuration you need. Those instructions are executed during a build, which results in a Docker image. This image is a package containing your small system, ready to use, which you can easily share on the internet through registries. Any developer who has a working Docker installation can then download this image and run it on their system in a container.
Docker has been quickly adopted by developers as it greatly eases the setup of complex development environments, allowing them to have several projects with different system package versions, all without worrying about their installation on their local machine.
Deploying a FastAPI application with Docker
However, Docker is not only for local development: it’s also widely used for deploying applications to production. Since the builds are reproducible, we can ensure that the local and production environments remain the same, which limits any issues when moving to production.
In this section, we’ll learn how to write a Dockerfile for a FastAPI application, how to build an image, and how to deploy it on a cloud platform.
Writing a Dockerfile
As we mentioned in the introduction to this section, a Dockerfile is a set of instructions for building your Docker image, a self-contained system containing all the required components to run your applications. To begin with, all Dockerfiles derive from a base image; usually, this is a standard Linux installation, such as Debian or Ubuntu. From this base, we can copy files from our local machine into the image (usually, the source code of our application) and execute Unix commands – for example, to install packages or execute scripts.
In our case, the creator of FastAPI has created a base Docker image that contains all the necessary tools to run a FastAPI app! All we have to do is start from this image, copy our source files, and install our dependencies! Let’s learn how to do that!
First of all, you’ll need a working Docker installation on your machine. Follow the official Getting Started tutorial, which should guide you in this process: https://docs.docker.com/get-started/.
To create a Docker image, we simply have to create a file named Dockerfile at the root of our project. The following example shows the content of this file for our current project:
Dockerfile
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10
ENV APP_MODULE project.app:app
COPY requirements.txt /app
RUN pip install --upgrade pip && \ pip install -r /app/requirements.txt
COPY ./ /app
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ Dockerfile
Deploying a FastAPI Project
Let’s go through each instruction. The first instruction, FROM, is the base image we derive from. Here, we took the uvicorn-gunicorn-fastapi image, which was created by the creator of FastAPI. Docker images have tags, which can be used to pick a specific version of the image. Here, we chose Python version 3.10. Lots of variations exist for this image, including ones with other versions of Python. You can check them out in the official README file: https://github.com/tiangolo/ uvicorn-gunicorn-fastapi-docker.
Then, we set the APP_MODULE environment variable thanks to the ENV instruction. In a Docker image, environment variables can be set at build time, as we did here, or at runtime. APP_MODULE is an environment variable defined by the base image. It should point to the path of your FastAPI application: it’s the same argument that we set at the end of Uvicorn and Gunicorn commands to launch the application. You can find the list of all the accepted environment variables for the base image in the official README file.
Next, we have our first COPY statement. As you may have guessed, this instruction will copy a file from your local system to the image. Here, we only copied our requirements.txt file. We’ll explain why shortly. Notice that we copied the file into the /app directory of the image; it’s the main working directory defined by the base image.
We then have a RUN statement. This instruction is used to execute Unix commands. In our case, we ran pip to install our dependencies, following the requirements.txt file we just copied. This is essential to make sure all our Python dependencies are present.
Finally, we copied the rest of our source code files into the /app directory. Now, let’s explain why we separately copied requirements.txt. The important thing to understand is that Docker images are built using layers: each instruction will create a new layer in the build system. To improve performance, Docker does its best to reuse layers it has already built. Therefore, if it detects no changes from the previous build, it’ll reuse the ones it has in memory without rebuilding them.
By copying the requirements.txt file alone and installing the Python dependencies before the rest of the source code, we allow Docker to reuse the layer where the dependencies have been installed. If we edit our source code but not requirements.txt, the Docker build will only execute the last COPY instruction, reusing all the previous layers. Thus, the image is built in a few seconds instead of minutes.
Most of the time, Dockerfiles end with a CMD instruction, which should be the command to execute when the container is started. In our case, we would have used the Gunicorn command we saw in the Adding Gunicorn as a server section. However, in our case, the base image already handles this for us.
Adding a prestart script
When deploying an application, it’s quite common to run several commands before the application starts. The most typical case is to execute database migrations so that our production database has the correct set of tables and columns. To help us with this, our base Docker image allows us to create a bash script named prestart.sh. If this file is present, it’ll be automatically run before the FastAPI application is started.
Deploying a FastAPI application with Docker
In our case, we just run the Alembic command to execute migrations:
prestart.sh
#! /usr/bin/env bash
# Let the DB start sleep 10; # Run migrations alembic upgrade head
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter10/project/ prestart.sh
Bear in mind that this is a mechanism provided only for convenience by the tiangolo/uvicorn- gunicorn-fastapi image. If you start from a more basic image, you’ll have to come up with your own solution to run a prestart script.
Building a Docker image
We can now build our Docker image! From the root of your project, just run the following command:
$ docker build -t fastapi-app .
The dot (.) denotes the path of the root context to build your image – in this case, the current directory. The -t option is here to tag the image and give it a practical name.
Docker will then perform the build. You’ll see that it’ll download the base image and sequentially run your instructions. This should take a few minutes. If you run the command again, you’ll experience what we explained earlier about layers: if there is no change, layers are reused and the build takes only a few seconds.
Running a Docker image locally
Before deploying it to production, you can try to run your image locally. To do this, run the following command:
$ docker run -p 8000:80 -e ENVIRONMENT=production -e DATABASE_ URL=sqlite+aiosqlite:///app.db fastapi-app
Deploying a FastAPI Project
Here, we used the run command with the name of the image we just built. There are, of course, a few options here:
-p allows you to publish ports on your local machine. By default, Docker containers are not accessible on your local machine. If you publish ports, they will be available through localhost. On the container side, the FastAPI application is executed on port 80. We publish it on port 8000 on our local machine – that is, 8000:80.
-e is used to set environment variables. As we mentioned in the Setting and using environment variables section, we need those variables to configure our application. Docker allows us to set them easily and dynamically at runtime. Notice that we set a simple SQLite database for testing purposes. However, in production, it should point to a proper database.
You can review the numerous options of this command in the official Docker documentation: https://docs.docker.com/engine/reference/commandline/ run/#options.
This command will run your application, which will be accessible through http://localhost:8000. Docker will show you the logs in the terminal.
Deploying a Docker image
Now that you have a working Docker image, you can deploy it on virtually any machine that runs Docker. This can be your own server or a dedicated platform. Lots of serverless platforms have emerged to help you deploy container images automatically: Google Cloud Run, Amazon Elastic Container Service, and Microsoft Azure Container Instances are just a few.
Usually, what you have to do is upload (push, in Docker jargon) your image to a registry. By default, Docker pulls and pushes images from Docker Hub, the official Docker registry, but lots of services and platforms propose their own registries. Usually, using the private cloud registry proposed by the cloud platform is necessary to deploy it on this platform. Here are the relevant documentation pages for getting started with private registries with the most popular cloud providers:
Google Artifact Registry: https://cloud.google.com/artifact-registry/ docs/docker/quickstart
Amazon ECR: https://docs.aws.amazon.com/AmazonECR/latest/userguide/ getting-started-console.html
Microsoft Azure Container Registry: https://docs.microsoft.com/en-us/ azure/container-registry/container-registry-get-started-docker- cli?tabs=azure-cli
Deploying a FastAPI application on a traditional server
If you followed the relevant instructions, you should have a private registry for storing Docker images. The instructions probably showed you how to authenticate your local Docker command line with it and how to push your first image. Basically, all you have to do is tag the image you built with the path to your private registry:
$ docker tag fastapi-app aws_account_id.dkr.ecr.region.amazonaws.com/ fastapi-app
Then, you need to push it to the registry:
$ docker push fastapi-app aws_account_id.dkr.ecr.region.amazonaws.com/ fastapi-app
Your image is now safely stored in the cloud platform registry. You can now use a serverless container platform to deploy it automatically. Here are the relevant documentation pages for getting started with private registries with the most popular cloud providers:
Google Cloud Run: https://cloud.google.com/run/docs/quickstarts/ build-and-deploy/python
Amazon Elastic Container Service: https://docs.aws.amazon.com/AmazonECS/ latest/developerguide/getting-started-ecs-ec2.html
Microsoft Azure Container Instances: https://docs.microsoft.com/en-us/azure/ container-instances/container-instances-tutorial-deploy-app
Of course, you’ll be able to set the environment variables just like you can for fully managed apps. Those environments also provide lots of options for tuning the scalability of your containers, both vertically (using more powerful instances) and horizontally (spawning more instances).
Once done, your application should be live on the web! The great thing about deploying Docker images compared to automated serverless platforms is that you are not limited to the features supported by the platform: you can deploy anything, even complex applications that require a lot of exotic packages, without worrying about compatibility.
At this point, we’ve seen the easiest and most efficient ways to deploy a FastAPI application. However, you may wish to deploy one the old-fashioned way and manually set up your server. In the next section, we’ll provide some guidelines for doing so.
Deploying a FastAPI application on a traditional server
In some situations, you may not have the chance to use a serverless platform to deploy your application. Some security or regulatory policies may force you to deploy on physical servers with specific configurations. In this case, it’s worth knowing some basic things so that you can deploy your application on traditional servers.
Deploying a FastAPI Project
In this section, we’ll consider you are working on a Linux server:
1. First of all, make sure a recent version of Python has been installed on your server, ideally with the version matching the one you used in development. The easiest way to do this is to set up pyenv, as we saw in Chapter 1, Python Development Environment Setup.
2. To retrieve your source code and keep it in sync with your latest developments, you can clone your Git repository on your server. This way, you only have to pull the changes and restart the server process to deploy a new version.
3. Set up a Python virtual environment, as we explained in Chapter 1, Python Development Environment Setup. You can install the dependencies with pip thanks to your requirements.txt file.
4. At that point, you should be able to run Gunicorn and start serving your FastAPI application. However, some improvements are strongly recommended.
5. Use a process manager to ensure your Gunicorn process is always running and restarted when the server is restarted. A good option for this is Supervisor. The Gunicorn documentation provides good guidelines for this: https://docs.gunicorn.org/en/stable/ deploy.html#supervisor. It’s also recommended to put Gunicorn behind an HTTP proxy instead of directly putting it on the front line. Its role is to handle SSL connections, perform load balancing, and serve static files such as images or documents. The Gunicorn documentation recommends using nginx for this task and provides a basic configuration: https://docs.gunicorn.org/en/ stable/deploy.html#nginx-configuration.
As you can see, in this context, there are quite a lot of configurations and decisions to make regarding your server configuration. Of course, you should also pay attention to security and make sure your server is well protected against the usual attacks. In the following DigitalOcean tutorial, you’ll find some guidelines for securing your server: https://www.digitalocean.com/community/ tutorials/recommended-security-measures-to-protect-your-servers.
If you’re not an experienced system administrator, we recommend that you favor serverless platforms; professional teams handle security, system updates, and server scalability for you, letting you focus on what matters most to you: developing a great application!
Summary
Your application is now live on the web! In this chapter, we covered the best practices to apply before deploying your application to production: use environment variables to set configuration options, such as database URLs, and manage your Python dependencies with a requirements.txt file. Then, we showed you how to deploy your application to a serverless platform, which handles everything for you by retrieving your source code, packaging it with its dependencies, and serving it on the web. Next, you learned how to build a Docker image for FastAPI using the base image created by the creator of FastAPI. As you saw, it allows you to be flexible while configuring the system, but you can still deploy it in a few minutes with a serverless platform that’s compatible with containers. Finally, we provided you with some guidelines for manual deployment on a traditional Linux server.
This marks the end of the second part of this book. You should now be confident in writing efficient, reliable FastAPI applications and be able to deploy them on the web.
In the next chapter, we will begin some data science tasks and integrate them efficiently into a FastAPI project.
Summary
Part 3: Building Resilient and Distributed Data Science Systems with FastAPI
This part will introduce you to the basic concepts of data science and machine learning, as well as the most popular Python tools and libraries for those tasks. We’ll see how to integrate those tools into a FastAPI backend and how to build a distributed system to perform resource-intensive tasks in a scalable way.
This section comprises the following chapters:
Chapter 11, Introduction to Data Science in Python
Chapter 12, Creating an Efficient Prediction API Endpoint with FastAPI
Chapter 13, Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
Chapter 14, Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Chapter 15, Monitoring the Health and Performance of a Data Science System
11 Introduction to Data Science in Python
In recent years, Python has gained a lot of popularity in the data science field. Its very efficient and readable syntax makes the language a very good choice for scientific research, while still being suitable for production workloads; it’s very easy to deploy research projects into real applications that will bring value to users. Thanks to this growing interest, a lot of specialized Python libraries have emerged and are now standards in the industry. In this chapter, we’ll introduce the fundamental concepts of machine learning before diving into the Python libraries used daily by data scientists.
In this chapter, we’re going to cover the following main topics:
Understanding the basic concepts of machine learning
Creating and manipulating NumPy arrays and pandas datasets
Training and evaluating machine learning models with scikit-learn
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
You’ll find all the code examples for this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter11.
What is machine learning?
Machine learning (ML) is often seen as a subfield of artificial intelligence. While this categorization is the subject of debate, ML has had a lot of exposure in recent years due to its vast and visible field of applications, such as spam filters, natural language processing, and image generation.
Introduction to Data Science in Python
ML is a field where we build mathematical models from existing data so that the machine can understand this data by itself. The machine is “learning” in the sense that the developer doesn’t have to program a step-by-step algorithm to solve the problem, which would be impossible for complex tasks. Once a model has been “trained” on existing data, it can be used to predict new data or understand new observations.
Consider the spam filter example: if we have a sufficiently large collection of emails manually labeled “spam” or “not spam,” we can use ML techniques to build a model that can tell us whether a new incoming email is spam or not.
In this section, we’ll review the most fundamental concepts of ML.
Supervised versus unsupervised learning
ML techniques can be divided into two main categories: supervised learning and unsupervised learning.
With supervised learning, the existing dataset is already labeled, which means we have both the input (the characteristics of an observation), known as features, and the output. If we consider the spam filter example here, the features could be the frequencies of each word and the label could be the category – that is, “spam” or “not spam.” Supervised learning is subdivided into two groups:
Classification problems, to classify data with a finite set of categories – for example, the spam filter
Regression problems, to predict continuous numerical values – for example, the number of rented electric scooters, given the day of the week, the weather, and the location
Unsupervised learning, on the other hand, operates on data without any reference to a label. The goal here is to discover interesting patterns from the features themselves. The two main problems that unsupervised learning tries to solve are as follows:
Clustering, where we want to find groups of similar data points – for example, a recommender system to suggest products that you might like, given what other people similar to you like.
Dimensionality reduction, where the goal is to find a more compact representation of datasets that contain a lot of different features. Doing this will allow us to keep only the most meaningful and discriminant features while working with smaller dataset dimensions.
Model validation
One of the key aspects of ML is evaluating whether your model is performing well or not. How can you say that your model will perform well on newly observed data? When building your model, how can you tell whether one algorithm performs better than another? All of these questions can and should be answered with model validation techniques.
What is machine learning?
As we mentioned previously, ML methods start with an existing set of data that we’ll use to train a model.
Intuitively, we may want to use all the data we have to train our model. Once done, what can we do to test it? We could apply our model to the same data and see whether the output was correct... and we would get a surprisingly good result! Here, we are testing the model with the same data we used to train it. Obviously, the model will overperform on this data because it has already seen it. As you may have guessed, this is not a reliable way to measure the accuracy of our model.
The right way to validate a model is to split the data into two: we keep one part for training the data and another for testing it. This is known as the holdout set. This way, we’ll test the model on data that it has never seen before and compare the result that’s predicted by the model with the real value. Hence, the accuracy we are measuring is much more sensible.
This technique works well; however, it poses a problem: by retaining some data, we are losing precious information that could have helped us build a better model. This is especially true if our initial dataset is small. To solve this, we can use cross-validation. With this method, we once again split the data into two sets. This time, we are training the model twice, using each set as training and testing sets. You can see a schematic representation of this operation in the following diagram:
Figure 11.1 – Two-fold cross-validation
At the end of the operation, we obtain two accuracies, which will give us a better overview of how our model performs on the whole dataset. This technique can be applied to help us perform more trials with a smaller testing set, as shown in the following diagram:
Introduction to Data Science in Python
Figure 11.2 – Five-fold cross-validation
We’ll stop here regarding this very quick introduction to ML. We’ve barely scratched the surface: ML is a vast and complex field, and there are lots of books dedicated to this subject. Still, this information should be sufficient to help you understand the basic concepts we’ll show throughout the rest of this chapter.
Manipulating arrays with NumPy and pandas
As we said in the introduction, numerous Python libraries have been developed to help with common data science tasks. The most fundamental ones are probably NumPy and pandas. Their goal is to provide a set of tools to manipulate a big set of data in an efficient way, much more than what we could actually achieve with standard Python, and we’ll show how and why in this section. NumPy and pandas are at the heart of most data science applications in Python; knowing about them is therefore the first step on your journey into Python for data science.
Before starting to use them, let’s explain why such libraries are needed. In Chapter 2, Python Programming Specificities, we stated that Python is a dynamically typed language. This means that the interpreter automatically detects the type of a variable at runtime, and this type can even change throughout the program. For example, you can do something like this in Python:
$ python >>> x = 1 >>> type(x)
Manipulating arrays with NumPy and pandas
<class 'int'> >>> x = "hello" >>> type(x) <class 'str'>
The interpreter was able to determine the type of x at each assignation.
Under the hood, the standard implementation of Python, CPython, is written in C. The C language is a compiled and statically typed language. This means that the nature of the variables is fixed at compile time, and they can’t change during execution. Thus, in the Python implementation, a variable doesn’t only consist of its value: it’s actually a structure containing information about the variable, including its type and size, in addition to its value.
Thanks to this, we can manipulate variables very dynamically in Python. However, it comes at a cost: each variable has a significantly higher memory footprint to store all its metadata than just the plain value.
This is particularly true for data structures. Say we consider a simple list like this:
$ python >>> l = [1, 2, 3, 4, 5]
Each item in the list is a Python integer, with all the metadata associated. In a statically typed language such as C, the same list would only be a suite of values in memory sharing the same type.
Let’s now imagine a big set of data, like the kind we usually encounter in data science: the cost of storing it in memory would be huge. That’s exactly the purpose of NumPy: to provide a powerful and efficient array structure for manipulating a big set of data. Under the hood, it uses a fixed-type array, meaning all elements of the structure are of the same type, which allows NumPy to get rid of the costly metadata of every single element. Moreover, common arithmetic operations, such as additions or multiplications, are much faster. In the Manipulating arrays with NumPy – computation, aggregations, and comparisons section, we’ll make a speed comparison to show you the difference with standard Python lists.
Getting started with NumPy
Let’s see how NumPy works! The first thing is to install it using the following command:
(venv) $ pip install numpy
In a Python interpreter, we can now import the library:
(venv) $ python >>> import numpy as np
Notice that, by convention, NumPy is generally imported with the alias np. Let’s now discover its basic features!
Introduction to Data Science in Python
Creating arrays
To create an array with NumPy, we can simply use the array function and pass it a Python list:
>>> np.array([1, 2, 3, 4, 5]) array([1, 2, 3, 4, 5])
NumPy will detect the nature of the Python list. However, we can force the resulting type by using the dtype argument:
>>> np.array([1, 2, 3, 4, 5], dtype=np.float64) array([1., 2., 3., 4., 5.])
All elements were upcasted to the specified type. It is key to remember that a NumPy array is of a fixed type. This means that every element will have the same type and NumPy will silently cast a value to the array type. For example, let’s consider an integer list into which we want to insert a floating-point value:
>>> l = np.array([1, 2, 3, 4, 5]) >>> l[0] = 13.37 >>> l array([13, 2, 3, 4, 5])
The 13.37 value has been truncated to fit into an integer.
If the value cannot be cast to the type of array, an error is raised. For example, let’s try to change the first element with a string:
>>> l[0] = "a" Traceback (most recent call last): File "<stdin>", line 1, in <module> ValueError: invalid literal for int() with base 10: 'a'
As we said in the introduction to this section, Python lists are not very efficient for large datasets. This is why it’s generally more efficient to use NumPy functions to create arrays. The most commonly used ones are generally the following:
np.zeros, to create an array filled with zeros
np.ones, to create an array filled with ones
np.empty, to create an empty array of the desired size in memory, without initializing the values
np.arange, to create an array with a range of elements
Manipulating arrays with NumPy and pandas
Let’s see them in action:
>>> np.zeros(5) array([0., 0., 0., 0., 0.]) >>> np.ones(5) array([1., 1., 1., 1., 1.]) >>> np.empty(5) array([1., 1., 1., 1., 1.]) >>> np.arange(5) array([0, 1, 2, 3, 4])
Notice that the result of np.empty can vary: since the values in the array are not initialized, they take whatever value there is currently in this memory block. The main motivation behind this function is speed, allowing you to quickly allocate memory, but don’t forget to fill every element after.
By default, NumPy creates arrays with a floating-point type (float64). Once again, by using the dtype argument, you can force another type to be used:
>>> np.ones(5, dtype=np.int32) array([1, 1, 1, 1, 1], dtype=int32)
NumPy provides a wide range of types, allowing you to finely optimize the memory consumption of your program by selecting the right type for your data. You can find the whole list of types supported by NumPy in the official documentation: https://numpy.org/doc/stable/reference/ arrays.scalars.html#sized-aliases.
NumPy also proposes a function to create an array with random values:
>>> np.random.seed(0) # Set the random seed to make examples reproducible >>> np.random.randint(10, size=5) array([5, 0, 3, 3, 7])
The first argument is the maximum range of the random value, and the size argument sets the number of values to generate.
Until now, we showed how to create one-dimensional arrays. However, the great strength of NumPy is that it natively handles multi-dimensional arrays! For example, let’s create a 3 x 4 matrix:
>>> m = np.ones((3,4)) >>> m array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]])
Introduction to Data Science in Python
NumPy did create an array with three rows and four columns! All we had to do was to pass a tuple to the NumPy function to specify our dimensions. When having such an array, NumPy gives us access to properties for knowing the number of dimensions, as well as the shape and size of it:
>>> m.ndim 2 >>> m.shape (3, 4) >>> m.size 12
Accessing elements and sub-arrays
NumPy arrays closely follow the standard Python syntax to manipulate lists. Therefore, to access an element in a one-dimensional array, just do the following:
>>> l = np.arange(5) >>> l[2] 2
For multi-dimensional arrays, we just have to add another index:
>>> np.random.seed(0) >>> m = np.random.randint(10, size=(3,4)) >>> m array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) >>> m[1][2] 3
Of course, this can be used to re-assign elements:
>>> m[1][2] = 42 >>> m array([[ 5, 0, 3, 3], [ 7, 9, 42, 5], [ 2, 4, 7, 6]])
But that’s not all. Thanks to the slicing syntax, we can access sub-arrays with a start index, an end index, and even a step. For example, on a one-dimensional array, we can do the following:
>>> l = np.arange(5) >>> l array([0, 1, 2, 3, 4]) >>> l[1:4] # From index 1 (inclusive) to 4 (exclusive) array([1, 2, 3])
Manipulating arrays with NumPy and pandas
>>> l[::2] # Every second element array([0, 2, 4])
This is exactly what we saw for standard Python lists in Chapter 2, Python Programming Specificities. Of course, it also works for multi-dimensional arrays, with one slice for each dimension:
>>> np.random.seed(0) >>> m = np.random.randint(10, size=(3,4)) >>> m array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) >>> m[1:, 0:2] # From row 1 to end and column 0 to 2 array([[7, 9], [2, 4]]) >>> m[::, 3:] # Every row, only last column array([[3], [5], [6]])
You can assign those sub-arrays to variables. However, for performance reasons, NumPy doesn’t copy the values by default: it’s only a view (or shallow copy), a representation of the existing data. This is important to bear in mind because if you change a value in the view, it will also change the value in the original array:
>>> v = m[::, 3:] >>> v[0][0] = 42 >>> v array([[42], [ 5], [ 6]]) >>> m array([[ 5, 0, 3, 42], [ 7, 9, 3, 5], [ 2, 4, 7, 6]])
If you need to really copy the values in memory, you just have to use the copy method on the array:
>>> m2 = m[::, 3:].copy()
m2 is now a separate copy of m, and changes in its values won’t change the values in m.
Introduction to Data Science in Python
You now have the basics for handling arrays with NumPy. As we’ve seen, the syntax is very similar to standard Python. The key points to remember when working with NumPy are the following:
NumPy arrays are of fixed types, meaning all items in the array are of the same type
NumPy natively handles multi-dimensional arrays and allows us to subset them using the standard slicing notation
Of course, NumPy can do much more than that: actually, it can apply common computations to those arrays in a very performant way.
Manipulating arrays with NumPy – computation, aggregations, and comparisons
As we said, NumPy is all about manipulating large arrays with great performance and controlled memory consumption. Let’s say, for example, that we want to compute the double of each element in a large array. In the following example, you can see an implementation of such a function with a standard Python loop:
chapter11_compare_operations.py
import numpy as np np.random.seed(0) # Set the random seed to make examples reproducible m = np.random.randint(10, size=1000000) # An array with a million of elements def standard_double(array): output = np.empty(array.size) for i in range(array.size): output[i] = array[i] * 2 return output
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ compare_operations.py
We instantiate an array with a million random integers. Then, we have our function building an array with the double of each element. Basically, we first instantiate an empty array of the same size before looping over each element to set the double.
Manipulating arrays with NumPy and pandas
Let’s measure the performance of this function. In Python, there is a standard module, timeit, dedicated to this purpose. We can use it directly from the command line and pass valid Python statements we want to measure performance for. The following command will measure the performance of standard_double with our big array:
python -m timeit "from chapter11.chapter11_compare_operations import m, standard_double; standard_double(m)" 1 loop, best of 5: 146 msec per loop
The results will vary depending on your machine, but the magnitude should be equivalent. What timeit does is repeat your code a certain number of times and measure its execution time. Here, our function took around 150 milliseconds to compute the double of each element in our array. For such simple computations on a modern computer, that’s not very impressive.
Let’s compare this with the equivalent operation using NumPy syntax. You can see it in the next sample:
chapter11_compare_operations.py
def numpy_double(array): return array * 2
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ compare_operations.py
The code is much shorter! NumPy implements the basic arithmetic operations and can apply them to each element of the array. By multiplying the array by a value directly, we implicitly tell NumPy to multiply each element by this value. Let’s measure the performance with timeit:
python -m timeit "from chapter11.chapter11_compare_operations import m, numpy_double; numpy_double(m)" 500 loops, best of 5: 611 usec per loop
Here, the best loop achieved the computation in 600 microseconds! That’s 250 times faster than the previous function! How can we explain such a variation? In a standard loop, Python (because of its dynamic nature) has to check for the type of value at each iteration to apply the right function for this type, which adds significant overhead. With NumPy, the operation is deferred to an optimized and compiled loop where types are known ahead of time, which saves a lot of useless checks.
We once again see here the benefits of NumPy arrays over standard lists when working on a large dataset: it implements operations natively to help you make computations very fast.
Adding and multiplying arrays
As you saw in the previous example, NumPy supports the arithmetic operators to make operations over arrays.
Introduction to Data Science in Python
This means that you can operate directly over two arrays of the same dimensions:
>>> np.array([1, 2, 3]) + np.array([4, 5, 6]) array([5, 7, 9])
In this case, NumPy applies the operation element-wise. But it also works in certain situations if one of the operands is not of the same shape:
>>> np.array([1, 2, 3]) * 2 array([2, 4, 6])
NumPy automatically understands that it should multiply each element by two. This is called broadcasting: NumPy “expands” the smaller array to match the shape of the larger array. The previous example is equivalent to this one:
>>> np.array([1, 2, 3]) * np.array([2, 2, 2]) array([2, 4, 6])
Note that even if those two examples are conceptually equivalent, the first one is more memory-efficient and computationally efficient: NumPy is smart enough to use only one 2 value, without having to create a full array of 2.
More generally, broadcasting works if the rightmost dimensions of the arrays are of the same size or if one of them is 1. For example, we can add an array of dimensions 4 x 3 to an array of dimensions 1 x 3:
>>> a1 = np.ones((4, 3)) >>> a1 array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) >>> a2 = np.ones((1, 3)) >>> a2 array([[1., 1., 1.]]) >>> a1 + a2 array([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.], [2., 2., 2.]])
However, adding an array of dimensions 4 x 3 to an array of dimensions 1 x 4 is not possible:
>>> a3 = np.ones((1, 4)) >>> a3 array([[1., 1., 1., 1.]]) >>> a1 + a3 Traceback (most recent call last):
Manipulating arrays with NumPy and pandas
File "<stdin>", line 1, in <module> ValueError: operands could not be broadcast together with shapes (4,3) (1,4)
If this sounds complicated or confusing, that’s normal; it takes time to understand it conceptually, especially in three or more dimensions. For a more detailed explanation of the concept, take time to read the related article in the official documentation: https://numpy.org/doc/stable/ user/basics.broadcasting.html.
Aggregating arrays – sum, min, max, mean, and so on
When working with arrays, we often need to summarize the data to extract some meaningful statistics: the mean, the minimum, the maximum, and so on. Fortunately, NumPy also provides those operations natively. Quite simply, they are provided as methods that you can call directly from an array:
>>> np.arange(10).mean() 4.5 >>> np.ones((4,4)).sum() 16.0
You can find the whole list of aggregating operations in the official documentation: https://numpy. org/doc/stable/reference/arrays.ndarray.html#calculation.
Comparing arrays
NumPy also implements the standard comparison operators to compare arrays. As with arithmetic operators, which we saw in the Adding and multiplying arrays section, broadcasting rules apply. This means that you can compare an array with a single value:
>>> l = np.array([1, 2, 3, 4]) >>> l < 3 array([ True, True, False, False])
And you can also compare arrays with arrays, given that they are compatible on the basis of the broadcasting rules:
>>> m = np.array( [[1., 5., 9., 13.], [2., 6., 10., 14.], [3., 7., 11., 15.], [4., 8., 12., 16.]] ) >>> m <= np.array([1, 5, 9, 13]) array([[ True, True, True, True], [False, False, False, False], [False, False, False, False],
Introduction to Data Science in Python
[False, False, False, False]])
The resulting array is filled with the Boolean result of the comparison for each element.
That’s it for this very quick introduction to NumPy. There is a lot more to know and discover with this library, so we strongly encourage you to read the official user guide: https://numpy.org/ doc/stable/user/index.html.
For the rest of this book, this should be enough for you to understand future examples. Let’s now have a look at a library often cited and used alongside NumPy: pandas.
Getting started with pandas
In the previous section, we introduced NumPy and its ability to efficiently store and work with a large array of data. We’ll now introduce another widely used library in data science: pandas. This library is built on top of NumPy to provide convenient data structures able to efficiently store large datasets with labeled rows and columns. This is, of course, especially handy when working with most datasets representing real-world data that we want to analyze and use in data science projects.
To get started, we will, of course, install the library with the usual command:
(venv) $ pip install pandas
Once done, we can start to use it in a Python interpreter:
(venv) $ python >>> import pandas as pd
Just like we alias numpy as np, the convention is to alias pandas as pd when importing it.
Using pandas Series for one-dimensional data
The first pandas data structure we’ll introduce is Series. This data structure behaves very similarly to a one-dimensional array in NumPy. To create one, we can simply initialize it with a list of values:
>>> s = pd.Series([1, 2, 3, 4, 5]) >>> s 0 1 1 2 2 3 3 4 4 5 dtype: int64
Manipulating arrays with NumPy and pandas
Under the hood, pandas creates a NumPy array. As such, it uses the same data types to store the data. You can verify this by accessing the values property of the Series object and checking its type:
>>> type(s.values) <class 'numpy.ndarray'>
Indexing and slicing work exactly the same way as in NumPy:
>>> s[0] 1 >>> s[1:3] 1 2 2 3 dtype: int64
So far, this is not very different from a regular NumPy array. As we said, the main purpose of pandas is to label the data. To allow this, pandas data structures maintain an index to allow this data labeling. It is accessible through the index property:
>>> s.index RangeIndex(start=0, stop=5, step=1)
Here, we have a simple range integer index, but we can actually have any arbitrary index. In the next example, we create the same series, labeling each value with a letter:
>>> s = pd.Series([1, 2, 3, 4, 5], index=["a", "b", "c", "d", "e"]) >>> s a 1 b 2 c 3 d 4 e 5
The index argument on the Series initializer allows us to set the list of labels. We can now access values with those labels instead:
>>> s["c"] 3
Surprisingly, even slicing notation works with those kinds of labels:
>>> s["b":"d"] b 2 c 3 d 4 dtype: int64
Introduction to Data Science in Python
Under the hood, pandas keep the order of the index to allow such useful notations. Notice, however, that with this notation, the last index is inclusive (d is included in the result), unlike standard index notation, where the last index is exclusive:
>>> s[1:3] b 2 c 3 dtype: int64
To avoid confusion between those two styles, pandas exposes two special notations to explicitly indicate which indexing style you wish to use: loc (label notation with the last index being inclusive) and iloc (standard index notation). You can read more about this in the official documentation: https:// pandas.pydata.org/docs/user_guide/indexing.html#different-choices- for-indexing.
Series can also be instantiated directly from dictionaries:
>>> s = pd.Series({"a": 1, "b": 2, "c": 3, "d": 4, "e": 5}) >>> s a 1 b 2 c 3 d 4 e 5 dtype: int64
In this case, the keys of the dictionaries are used as labels.
Of course, in the real world, you’ll more likely have to work with two-dimensional (or more!) datasets. This is exactly what DataFrames are for!
Using pandas DataFrames for multi-dimensional data
Most of the time, datasets consist of two-dimensional data, where you have several columns for each row, as in a classic spreadsheet application. In Pandas, DataFrames are designed to work with this kind of data. As for Series, it can work with a large set of data that is labeled both by rows and columns.
The following examples will use a tiny dataset representing the number of tickets (paid and free) delivered in French museums in 2018. Let’s consider we have this data in the form of two dictionaries:
>>> paid = {"Louvre Museum": 5988065, "Orsay Museum": 1850092, "Pompidou Centre": 2620481, "National Natural History Museum": 404497} >>> free = {"Louvre Museum": 4117897, "Orsay Museum": 1436132, "Pompidou Centre": 1070337, "National Natural History Museum": 344572}
Manipulating arrays with NumPy and pandas
Each key in those dictionaries is a label for a row. We can build a DataFrame directly from those two dictionaries like this:
>>> museums = pd.DataFrame({"paid": paid, "free": free}) >>> museums paid free Louvre Museum 5988065 4117897 Orsay Museum 1850092 1436132 Pompidou Centre 2620481 1070337 National Natural History Museum 404497 344572
The DataFrame initializer accepts a dictionary of dictionaries, where keys represent the label for the columns.
We can have a look at the index property, storing the rows index, and the columns property, storing the columns index:
>>> museums.index Index(['Louvre Museum', 'Orsay Museum', 'Pompidou Centre', 'National Natural History Museum'], dtype='object') >>> museums.columns Index(['paid', 'free'], dtype='object')
Once again, we can now use indexing and slicing notation to get subsets of columns or rows:
>>> museums["free"] Louvre Museum 4117897 Orsay Museum 1436132 Pompidou Centre 1070337 National Natural History Museum 344572 Name: free, dtype: int64 >>> museums["Louvre Museum":"Orsay Museum"] paid free Louvre Museum 5988065 4117897 Orsay Museum 1850092 1436132 >>> museums["Louvre Museum":"Orsay Museum"]["paid"] Louvre Museum 5988065 Orsay Museum 1850092 Name: paid, dtype: int64
Introduction to Data Science in Python
Something that is even more powerful: you can write a Boolean condition inside the brackets to match some data. This operation is called masking:
>>> museums[museums["paid"] > 2000000] paid free Louvre Museum 5988065 4117897 Pompidou Centre 2620481 1070337
Finally, you can easily set new columns with this very same indexing notation:
>>> museums["total"] = museums["paid"] + museums["free"] >>> museums paid free total Louvre Museum 5988065 4117897 10105962 Orsay Museum 1850092 1436132 3286224 Pompidou Centre 2620481 1070337 3690818 National Natural History Museum 404497 344572 749069
As you can see, just like NumPy arrays, pandas fully supports arithmetic operations over two DataFrames.
Of course, all the basic aggregation operations are supported, including mean and sum:
>>> museums["total"].sum() 17832073 >>> museums["total"].mean() 4458018.25
You can find the whole list of operations available in the official documentation: https://pandas. pydata.org/pandas-docs/stable/user_guide/basics.html#descriptive- statistics.
Importing and exporting CSV data
One very common way of sharing datasets is through CSV files. This format is very convenient because it only consists of a simple text file, each line representing a row of data, with each column separated by a comma. Our simple museums dataset is available in the examples repository as a CSV file, which you can see in the next sample:
museums.csv
name,paid,free Louvre Museum,5988065,4117897 Orsay Museum,1850092,1436132 Pompidou Centre,2620481,1070337 National Natural History Museum,404497,344572
Training models with scikit-learn
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/museums.csv
Importing CSV files is so common that pandas provides a function to load a CSV file into a DataFrame directly:
>>> museums = pd.read_csv("./chapter11/museums.csv", index_col=0) >>> museums paid free name Louvre Museum 5988065 4117897 Orsay Museum 1850092 1436132 Pompidou Centre 2620481 1070337 National Natural History Museum 404497 344572
The function simply expects the path to the CSV file. Several arguments are available to finely control the operation: here, we used index_col to specify the index of the column that should be used as row labels. You can find the whole list of arguments in the official documentation: https://pandas. pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.
Of course, the opposite operation exists to export a DataFrame to a CSV file:
>>> museums["total"] = museums["paid"] + museums["free"] >>> museums.to_csv("museums_with_total.csv")
We will conclude this very quick introduction to pandas here. Of course, we’ve only covered the tip of the iceberg, and we recommend that you go through the official user guide to know more: https:// pandas.pydata.org/pandas-docs/stable/user_guide/index.html.
Still, you should now be able to perform basic operations and operate efficiently on large datasets. In the next section, we’ll introduce scikit-learn, one of the fundamental Python toolkits for data science, and you’ll see that it relies a lot on NumPy and pandas.
Training models with scikit-learn
scikit-learn is one of the most widely used Python libraries for data science. It implements dozens of classic ML models, but also numerous tools to help you while training them, such as preprocessing methods and cross-validation. Nowadays, you’ll probably hear about more modern approaches, such as PyTorch, but scikit-learn is still a solid tool for a lot of use cases.
The first thing you must do to get started is to install it in your Python environment:
(venv) $ pip install scikit-learn
We can now start our scikit-learn journey!
Introduction to Data Science in Python
Training models and predicting
In scikit-learn, ML models and algorithms are called estimators. Each is a Python class that implements the same methods. In particular, we have fit, which is used to train a model, and predict, which is used to run the trained model on new data.
To try this, we’ll load a sample dataset. scikit-learn comes with a few toy datasets that are very useful for performing experiments. You can find out more about them in the official documentation: https:// scikit-learn.org/stable/datasets.html.
Here, we’ll use the digits dataset, a collection of pixel matrices representing handwritten digits. As you may have guessed, the goal of this dataset is to train a model to automatically recognize handwritten digits. The following example shows how to load this dataset:
chapter11_load_digits.py
from sklearn.datasets import load_digits
digits = load_digits()
data = digits.data targets = digits.target
print(data[0].reshape((8, 8))) # First handwritten digit 8 x 8 matrix print(targets[0]) # Label of first handwritten digit
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ load_digits.py
Notice that the toy dataset’s functions are imported from the datasets package of scikit-learn. The load_digits function returns an object that contains the data and some metadata.
The most interesting parts of this object are data, which contains the handwritten digit pixels matrices, and targets, which contains the corresponding label for those digits. Both are NumPy arrays.
To get a grasp of what this looks like, we will take the first digit in the data and reshape it into an 8 x 8 matrix; this is the size of the source images. Each value represents a pixel on a grayscale, from 0 to 16.
Then, we print the label of this first digit, which is 0. If you run this code, you’ll get the following output:
[[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.]
Training models with scikit-learn
[ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]] 0
Somehow, we can guess the shape of the zero from the matrix.
Now, let’s try to build a model that recognizes handwritten digits. To start simple, we’ll use a Gaussian Naive Bayes model, a classic and easy-to-use algorithm that can quickly yield good results. The following example shows the entire process:
chapter11_fit_predict.py
from sklearn.datasets import load_digits from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB
digits = load_digits()
data = digits.data targets = digits.target
# Split into training and testing sets training_data, testing_data, training_targets, testing_targets = train_test_split( data, targets, random_state=0 )
# Train the model model = GaussianNB() model.fit(training_data, training_targets)
# Run prediction with the testing set predicted_targets = model.predict(testing_data)
# Compute the accuracy accuracy = accuracy_score(testing_targets, predicted_targets) print(accuracy)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ fit_predict.py
Introduction to Data Science in Python
Now that we’ve loaded the dataset, you can see that we take care of splitting it into a training and a testing set. As we mentioned in the Model validation section, this is essential for computing meaningful accuracy scores to check how our model performs.
To do this, we can rely on the train_test_split function, which is provided in the model_ selection package. It selects random instances from our dataset to form the two sets. By default, it keeps 25% of the data to create a testing set, but this can be customized. The random_state argument allows us to set the random seed to make the example reproducible. You can find out more about this function in the official documentation: https://scikit-learn.org/stable/modules/ generated/sklearn.model_selection.train_test_split.html#sklearn- model-selection-train-test-split.
Then, we must instantiate the GaussianNB class. This class is one of the numerous ML estimators that’s implemented in scikit-learn. Each has its own set of parameters, to finely tune the behavior of the algorithm. However, scikit-learn is designed to provide sensible defaults for all the estimators, so it’s usually good to start with the defaults before tinkering with them.
After that, we must call the fit method to train our model. It expects an argument and two arrays: the first one is the actual data, with all its features, while the second one is the corresponding labels. And that’s it! You’ve trained your first ML model!
Now, let’s see how it behaves: we’ll call predict on our model with the testing set so that it automatically classifies the digits of the testing set. The result of this is a new array with the predicted labels.
All we have to do now is compare it with the actual labels of our testing set. Once again, scikit-learn helps by providing the accuracy_score function in the metrics package. The first argument is the true labels, while the second is the predicted labels.
If you run this code, you’ll get an accuracy score of around 83%. That isn’t too bad for a first approach! As you have seen, training and running prediction on an ML model is straightforward with scikit-learn.
In practice, we often need to perform preprocessing steps on the data before feeding it to an estimator. Rather than doing this sequentially by hand, scikit-learn proposes a convenient feature that can automate this process: pipelines.
Chaining preprocessors and estimators with pipelines
Quite often, you’ll need to preprocess your data so that it can be used by the estimator you wish to use. Typically, you’ll want to transform an image into an array of pixel values or, as we’ll see in the following example, transform raw text into numerical values so that we can apply some math to them.
Rather than writing those steps by hand, scikit-learn proposes a feature that can automatically chain preprocessors and estimators: pipelines. Once created, they expose the very same interface as any other estimator, allowing you to run training and prediction in one operation.
Training models with scikit-learn
To show you what this looks like, we’ll look at an example of another classic dataset, the 20 newsgroups text dataset. It consists of 18,000 newsgroup articles categorized into 20 topics. The goal of this dataset is to build a model that will automatically categorize an article in one of those topics.
The following example shows how we can load this data thanks to the fetch_20newsgroups function:
chapter11_pipelines.py
import pandas as pd from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline
# Load some categories of newsgroups dataset categories = [ "soc.religion.christian", "talk.religion.misc", "comp.sys.mac.hardware", "sci.crypt", ] newsgroups_training = fetch_20newsgroups( subset="train", categories=categories, random_state=0 ) newsgroups_testing = fetch_20newsgroups( subset="test", categories=categories, random_state=0 )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ pipelines.py
Since the dataset is rather large, we’ll only load a subset of the categories. Also, notice that it’s already been split into training and testing sets, so we only have to load them with the corresponding argument. You can find out more about the functionality of this dataset in the official documentation: https:// scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups- text-dataset.
Before moving on, it’s important to understand what the underlying data is. Actually, this is the raw text of an article. You can check this by printing one of the samples in the data:
>>> newsgroups_training.data[0] "From: sandvik@newton.apple.com (Kent Sandvik)\nSubject: Re: Ignorance is BLISS, was Is it good that Jesus died?\nOrganization: Cookamunga Tourist Bureau\nLines: 17\n\nIn article <f1682Ap@quack.kfu.com>,
Introduction to Data Science in Python
pharvey@quack.kfu.com (Paul Harvey)\nwrote:\n> In article <sandvik- 170493104859@sandvik-kent.apple.com> \n> sandvik@newton.apple.com (Kent Sandvik) writes:\n> >Ignorance is not bliss!\n \n> Ignorance is STRENGTH!\n> Help spread the TRUTH of IGNORANCE!\n\nHuh, if ignorance is strength, then I won't distribute this piece\nof information if I want to follow your advice (contradiction above).\n\n\nCheers,\nKent\ n---\nsandvik@newton.apple.com. ALink: KSAND -- Private activities on the net.\n"
So, we need to extract some features from this text before feeding it to an estimator. A common approach for this when working with textual data is to use the Term Frequency-Inverse Document Frequency (TF-IDF). Without going into too much detail, this technique will count the occurrences of each word in all the documents (term frequency), weighted by the importance of this word in every document (inverse document frequency). The idea is to give more weight to rarer words, which should convey more sense than frequent words such as “the.” You can find out more about this in the scikit-learn documentation: https://scikit-learn.org/dev/modules/feature_extraction. html#tfidf-term-weighting.
This operation consists of splitting each word in the text samples and counting them. Usually, we apply a lot of techniques to refine this, such as removing stop words (common words such as “and” or “is” that don’t bring much information). Fortunately, scikit-learn provides an all-in-one tool for this: TfidfVectorizer.
This preprocessor can take an array of text, tokenize each word, and compute the TF-IDF for each of them. A lot of options are available for finely tuning its behavior, but the defaults are a good start for English text. The following example shows how to use it with an estimator in a pipeline:
chapter11_pipelines.py
# Make the pipeline model = make_pipeline( TfidfVectorizer(), MultinomialNB(), )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ pipelines.py
The make_pipeline function accepts any number of preprocessors and an estimator in its argument. Here, we’re using the Multinomial Naive Bayes classifier, which is suitable for features representing frequency.
Training models with scikit-learn
Then, we can simply train our model and run prediction to check its accuracy, as we did previously. You can see this in the following example:
chapter11_pipelines.py
# Train the model model.fit(newsgroups_training.data, newsgroups_training.target)
# Run prediction with the testing set predicted_targets = model.predict(newsgroups_testing.data)
# Compute the accuracy accuracy = accuracy_score(newsgroups_testing.target, predicted_ targets) print(accuracy)
# Show the confusion matrix confusion = confusion_matrix(newsgroups_testing.target, predicted_ targets) confusion_df = pd.DataFrame( confusion, index=pd.Index(newsgroups_testing.target_names, name="True"), columns=pd.Index(newsgroups_testing.target_names, name="Predicted"), ) print(confusion_df)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ pipelines.py
Notice that we also printed a confusion matrix, which is a very convenient representation of the global results. Scikit-learn has a dedicated function for this called confusion_matrix. Then, we wrap the result in a pandas DataFrame so that we can set the axis labels to improve readability. If you run this example, you’ll get an output similar to what’s shown in the following screenshot. Depending on your machine and system, it could take a couple of minutes to run:
Introduction to Data Science in Python
Figure 11.3 – Confusion matrix on the 20 newsgroups dataset
Here, you can see that our results weren’t too bad for our first try. Notice that there is one big area of confusion between the soc.religion.christian and talk.religion.misc categories, which is not very surprising, given their similarity.
As you’ve seen, building a pipeline with a preprocessor is very straightforward. The nice thing about this is that it automatically applies it to the training data, but also when you’re predicting the results.
Before moving on, let’s look at one more important feature of scikit-learn: cross-validation.
Validating the model with cross-validation
In the Model validation section, we introduced the cross-validation technique, which allows us to use data in training or testing sets. As you may have guessed, this technique is so common that it’s implemented natively in scikit-learn!
Let’s take another look at the handwritten digit example and apply cross-validation:
chapter11_cross_validation.py
from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score from sklearn.naive_bayes import GaussianNB
digits = load_digits()
data = digits.data targets = digits.target
# Create the model model = GaussianNB()
# Run cross-validation score = cross_val_score(model, data, targets)
print(score) print(score.mean())
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_ cross_validation.py
This time, we don’t have to split the data ourselves: the cross_val_score function performs the folds automatically. In argument, it expects the estimator, data, which contains the handwritten digits’ pixels matrices, and targets, which contains the corresponding label for those digits. By default, it performs five folds.
The result of this operation is an array that provides the accuracy score of the five folds. To get a global overview of this result, we can take, for example, the mean. If you run this example, you’ll get the following output:
python chapter11/chapter11_cross_validation.py [0.78055556 0.78333333 0.79387187 0.8718663 0.80501393] 0.8069281956050759
As you can see, our mean accuracy is around 80%, which is a bit lower than the 83% we obtained with single training and testing sets. That’s the main benefit of cross-validation: we obtain a more statistically accurate metric regarding the performance of our model.
With that, you have learned the basics of working with scikit-learn. It’s obviously a very quick introduction to this vast framework, but it’ll give you the keys to train and evaluate your first ML models.
Summary
Congratulations! You’ve discovered the basic concepts of ML and made your first experiments with the fundamental toolkits of the data scientist. Now, you should be able to explore your first data science problems in Python. Of course, this was by no means a complete lesson on ML: the field is vast and there are tons of algorithms and techniques to explore. However, I hope that this has sparked your curiosity and that you’ll deepen your knowledge of this subject.
Now, it’s time to get back to FastAPI! With our new ML tools at hand, we’ll be able to leverage the power of FastAPI to serve our estimators and propose a reliable and efficient prediction API to our users.
Summary
12 Creating an Efficient Prediction API Endpoint with FastAPI
In the previous chapter, we introduced the most common data science techniques and libraries largely used in the Python community. Thanks to those tools, we can now build machine learning models that can make efficient predictions and classify data. Of course, we now have to think about a convenient interface so that we can take advantage of their intelligence. This way, microservices or frontend applications can ask our model to make predictions to improve the user experience or business operations. In this chapter, we’ll learn how to do that with FastAPI.
As we’ve seen throughout this book, FastAPI allows us to implement very efficient REST APIs with a clear and lightweight syntax. In this chapter, you’ll learn how to use them as efficiently as possible in order to serve thousands of prediction requests. To help us with this task, we’ll introduce another library, Joblib, which provides tools to help us serialize a trained model and cache predicted results.
In this chapter, we’re going to cover the following main topics:
Persisting a trained model with Joblib
Implementing an efficient prediction endpoint
Caching results with Joblib
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
You’ll find all the code examples for this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter12.
Creating an Efficient Prediction API Endpoint with FastAPI
Persisting a trained model with Joblib
In the previous chapter, you learned how to train an estimator with scikit-learn. When building such models, you’ll likely obtain a rather complex Python script to load your training data, pre-process it, and train your model with the best set of parameters. However, when deploying your model in a web application such as FastAPI, you don’t want to repeat this script and run all those operations when the server is starting. Instead, you need a ready-to-use representation of your trained model that you can just load and use.
This is what Joblib does. This library aims to provide tools for efficiently saving Python objects to disk, such as large arrays of data or function results: this operation is generally called dumping. Joblib is already a dependency of scikit-learn, so we don’t even need to install it. Actually, scikit-learn itself uses it internally to load the bundled toy datasets.
As we’ll see, dumping a trained model involves just one line of code with Joblib.
Dumping a trained model
In this example, we’re using the newsgroups example we saw in the Chaining preprocessors and estimators with pipelines section of Chapter 11, Introduction to Data Science in Python. As a reminder, we load 4 of the 20 categories in the newsgroups dataset and build a model to automatically categorize news articles into those categories. Once we’ve done this, we dump the model into a file called newsgroups_model.joblib:
chapter12_dump_joblib.py
# Make the pipeline model = make_pipeline( TfidfVectorizer(), MultinomialNB(), )
# Train the model model.fit(newsgroups_training.data, newsgroups_training.target)
# Serialize the model and the target names model_file = "newsgroups_model.joblib" model_targets_tuple = (model, newsgroups_training.target_names) joblib.dump(model_targets_tuple, model_file)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ dump_joblib.py
Persisting a trained model with Joblib
As you can see, Joblib exposes a function called dump, which simply expects two arguments: the Python object to save and the path of the file.
Notice that we don’t dump the model variable alone: instead, we wrap it in a tuple, along with the name of the categories, target_names. This allows us to retrieve the actual name of the category after the prediction has been made without us having to reload the training dataset.
If you run this script, you’ll see that the newsgroups_model.joblib file was created:
(venv) $ python chapter12/chapter12_dump_joblib.py $ ls -lh *.joblib -rw-r--r-- 1 fvoron staff 3,0M 10 jan 08:27 newsgroups_ model.joblib
Notice that this file is rather large: it’s more than 3 MB! It stores all the probabilities of each word in each category, as computed by the multinomial Naive Bayes model.
That’s all we need to do. This file now contains a static representation of our Python model, which will be easy to store, share, and load. Now, let’s learn how to load it and check that we can run predictions on it.
Loading a dumped model
Now that we have our dumped model file, let’s learn how to load it again using Joblib and check that everything is working. In the following example, we’re loading the Joblib dump present in the chapter12 directory of the examples repository and running a prediction:
chapter12_load_joblib.py
import os
import joblib from sklearn.pipeline import Pipeline
# Load the model model_file = os.path.join(os.path.dirname(__file__), "newsgroups_ model.joblib") loaded_model: tuple[Pipeline, list[str]] = joblib.load(model_file) model, targets = loaded_model
# Run a prediction p = model.predict(["computer cpu memory ram"]) print(targets[p[0]])
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ load_joblib.py
Creating an Efficient Prediction API Endpoint with FastAPI
All we need to do here is call the load function from Joblib and pass it as a valid path to a dump file. The result of this function is the very same Python object we dumped. Here, it’s a tuple composed of the scikit-learn estimator and a list of categories.
Notice that we added some type hints: while not necessary, it helps mypy or whichever IDE you use identify the nature of the objects you loaded and benefit from type-checking and auto-completion.
Finally, we run a prediction on the model: it’s a true scikit-learn estimator, with all the necessary training parameters.
That’s it! As you’ve seen, Joblib is straightforward to use. Nevertheless, it’s an essential tool for exporting your scikit-learn models and being able to use them in external services without repeating the training phase. Now, we can use those dump files in FastAPI projects.
Implementing an efficient prediction endpoint
Now that we have a way to save and load our machine learning models, it’s time to use them in a FastAPI project. As you’ll see, the implementation shouldn’t be too much of a surprise if you’ve followed this book. The main part of the implementation is the class dependency, which will take care of loading the model and making predictions. If you need a refresher on class dependencies, check out Chapter 5, Dependency Injection in FastAPI.
Let’s go! Our example will be based on the newgroups model we dumped in the previous section. We’ll start by showing you how to implement the class dependency, which will take care of loading the model and making predictions:
chapter12_prediction_endpoint.py
class PredictionInput(BaseModel): text: str
class PredictionOutput(BaseModel): category: str
class NewsgroupsModel: model: Pipeline | None = None targets: list[str] | None = None
def load_model(self) -> None: """Loads the model""" model_file = os.path.join(os.path.dirname(__ file__), "newsgroups_model.joblib") loaded_model: tuple[Pipeline, list[str]] = joblib.load(model_file) model, targets = loaded_model
Implementing an efficient prediction endpoint
self.model = model self.targets = targets
async def predict(self, input: PredictionInput) -> PredictionOutput: """Runs a prediction""" if not self.model or not self.targets: raise RuntimeError("Model is not loaded") prediction = self.model.predict([input.text]) category = self.targets[prediction[0]] return PredictionOutput(category=category)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ prediction_endpoint.py
First, we start by defining two Pydantic models: PredictionInput and PredictionOutput. In a pure FastAPI philosophy, they will help us validate the request payload and return a structured JSON response. Here, as input, we simply expect a text property containing the text we want to classify. As output, we expect a category property containing the predicted category.
The most interesting part of this extract is the NewsgroupsModel class. It implements two methods: load_model and predict.
The load_model method loads the model using Joblib, as we saw in the previous section, and stores the model and targets in class properties. Hence, they will be available to use in the predict method.
On the other hand, the predict method will be injected into the path operation function. As you can see, it directly accepts PredictionInput, which will be injected by FastAPI. Inside this method, we are making a prediction, as we usually do with scikit-learn. We return a PredictionOutput object with the category we predicted.
You may have noticed that, first, we check whether the model and its targets have been assigned in the class properties before performing the prediction. Of course, we need to ensure load_model was called at some point before making a prediction. You may be wondering why we are not putting this logic in an initializer, __init__, so that we can ensure the model is loaded at class instantiation. This would work perfectly fine; however, it would cause some issues. As we’ll see, we are instantiating a NewsgroupsModel instance right after FastAPI so that we can use it in our routes. If the loading logic was in __init__, the model would be loaded whenever we imported some variables (such as the app instance) from this file, such as in unit tests. In most cases, this would incur unnecessary I/O operations and memory consumption. As we’ll see, it’s better to use the lifespan handler of FastAPI to load the model when the app is run.
Creating an Efficient Prediction API Endpoint with FastAPI
The following extract shows the rest of the implementation, along with the actual FastAPI route for handling predictions:
chapter12_prediction_endpoint.py
newgroups_model = NewsgroupsModel()
@contextlib.asynccontextmanager async def lifespan(app: FastAPI): newgroups_model.load_model() yield
app = FastAPI(lifespan=lifespan)
@app.post("/prediction") async def prediction( output: PredictionOutput = Depends(newgroups_model. predict), ) -> PredictionOutput: return output
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ prediction_endpoint.py
As we mentioned previously, we are creating an instance of NewsgroupsModel so that we can inject it into our path operation function. Moreover, we are implementing a lifespan handler to call load_model. This way, we are making sure that the model is loaded during application startup and is ready to use.
The prediction endpoint is quite straightforward: as you can see, we directly depend on the predict method, which will take care of injecting the payload and validating it. We only have to return the output.
That’s it! Once again, FastAPI makes our life very easy by allowing us to write very simple and readable code, even for complex tasks. We can run this application using Uvicorn, as usual:
(venv) $ uvicorn chapter12.chapter12_prediction_endpoint:app
Now, we can try to run some predictions with HTTPie:
$ http POST http://localhost:8000/prediction text="computer cpu memory ram" HTTP/1.1 200 OK content-length: 36 content-type: application/json date: Tue, 10 Jan 2023 07:37:22 GMT
Caching results with Joblib
server: uvicorn
{ "category": "comp.sys.mac.hardware" }
Our machine learning classifier is alive! To push this further, let’s see how we can implement a simple caching mechanism using Joblib.
Caching results with Joblib
If your model takes time to make predictions, it may be interesting to cache the results: if the prediction for a particular input has already been done, it makes sense to return the same result we saved on disk rather than running the computations again. In this section, we’ll learn how to do this with the help of Joblib.
Joblib provides us with a very convenient and easy-to-use tool to do this, so the implementation is quite straightforward. The main concern will be about whether we should choose standard or async functions to implement the endpoints and dependencies. This will allow us to explain some of the technical details of FastAPI in more detail.
We’ll build upon the example we provided in the previous section. The first thing we must do is initialize a Joblib Memory class, which is the helper for caching function results. Then, we can add a decorator to the functions we want to cache. You can see this in the following example:
chapter12_caching.py
memory = joblib.Memory(location="cache.joblib")
@memory.cache(ignore=["model"]) def predict(model: Pipeline, text: str) -> int: prediction = model.predict([text]) return prediction[0]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ caching.py
When initializing memory, the main argument is location, which is the directory path where Joblib will store the results. Joblib automatically saves cached results on the hard disk.
Creating an Efficient Prediction API Endpoint with FastAPI
Then, you can see that we implemented a predict function, which accepts our scikit-learn model and some text input and then returns the predicted category index. This is the same prediction operation we’ve seen so far. Here, we extracted it from the NewsgroupsModel dependency class because Joblib caching is primarily designed to work with regular functions. Caching class methods is not recommended. As you can see, we simply have to add a @memory.cache decorator on top of this function to enable Joblib caching.
Whenever this function is called, Joblib will check whether it has the result on disk for the same arguments. If it does, it returns it directly. Otherwise, it proceeds with the regular function call.
As you can see, we added an ignore argument to the decorator, which allows us to tell Joblib to not take into account some arguments in the caching mechanism. Here, we excluded the model argument. Joblib cannot dump complex objects, such as scikit-learn estimators. This isn’t a problem, though: the model doesn’t change between several predictions, so we don’t care about having it cached. If we make improvements to our model and deploy a new one, all we have to do is clear the whole cache so that older predictions are made again with the new model.
Now, we can tweak the NewsgroupsModel dependency class so that it works with this new predict function. You can see this in the following example:
chapter12_caching.py
class NewsgroupsModel: model: Pipeline | None = None targets: list[str] | None = None
def load_model(self) -> None: """Loads the model""" model_file = os.path.join(os.path.dirname(__ file__), "newsgroups_model.joblib") loaded_model: tuple[Pipeline, list[str]] = joblib.load(model_file) model, targets = loaded_model self.model = model self.targets = targets
def predict(self, input: PredictionInput) -> PredictionOutput: """Runs a prediction""" if not self.model or not self.targets: raise RuntimeError("Model is not loaded") prediction = predict(self.model, input.text)
Caching results with Joblib
category = self.targets[prediction] return PredictionOutput(category=category)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ caching.py
In the predict method, we are calling the external predict function instead of doing so directly inside the method, taking care to pass the model and the input text as arguments. All we have to do after that is retrieve the corresponding category name and build a PredictionOutput object.
Finally, we have the REST API endpoints. Here, we added a delete/cache route so that we can clear the whole Joblib cache with an HTTP request. This can be seen in the following example:
chapter12_caching.py
@app.post("/prediction") def prediction( output: PredictionOutput = Depends(newgroups_model. predict), ) -> PredictionOutput: return output
@app.delete("/cache", status_code=status.HTTP_204_NO_CONTENT) def delete_cache(): memory.clear()
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ caching.py
The clear method on the memory object removes all the Joblib cache files on the disk.
Our FastAPI application is now caching prediction results. If you make a request with the same input twice, the second response will show you the cached result. In this example, our model is fast, so you won’t notice a difference in terms of execution time; however, this could be interesting with more complex models.
Choosing between standard or async functions
You may have noticed that we changed the predict method and the prediction and delete_ cache path operation functions so that they’re standard, non-async functions.
Creating an Efficient Prediction API Endpoint with FastAPI
Since the beginning of this book, we’ve shown you how FastAPI completely embraces asynchronous I/O and why it’s good for the performance of your applications. We’ve also recommended libraries that work asynchronously, such as database drivers, to leverage that power.
In some cases, however, that’s not always possible. In this case, Joblib is implemented to work synchronously. Nevertheless, it’s performing long I/O operations: it reads and writes cache files on the hard disk. Hence, it will block the process and won’t be able to answer other requests while this is happening, as we explained in the Asynchronous I/O section of Chapter 2, Python Programming Specificities.
To solve this, FastAPI implements a neat mechanism: if you define a path operation function or a dependency as a standard, non-async function, it’ll run it in a separate thread. This means that blocking operations, such as synchronous file reading, won’t block the main process. In a sense, we could say that it mimics an asynchronous operation.
To understand this, we’ll perform a simple experiment. In the following example, we are building a dummy FastAPI application with three endpoints:
/fast, which directly returns a response
/slow-async, a path operation defined as async, which creates a synchronous blocking operation that takes 10 seconds to run
/slow-sync, a path operation that’s defined as a standard method, which creates a synchronous blocking operation that takes 10 seconds to run
You can read the corresponding code here:
chapter12_async_not_async.py
import time
from fastapi import FastAPI
app = FastAPI()
@app.get("/fast") async def fast(): return {"endpoint": "fast"}
@app.get("/slow-async") async def slow_async(): """Runs in the main process""" time.sleep(10) # Blocking sync operation return {"endpoint": "slow-async"}
@app.get("/slow-sync")
Caching results with Joblib
def slow_sync(): """Runs in a thread""" time.sleep(10) # Blocking sync operation return {"endpoint": "slow-sync"}
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_ async_not_async.py
With this simple application, the goal is to see how those blocking operations block the main process. Let’s run this application with Uvicorn:
(venv) $ uvicorn chapter12.chapter12_async_not_async:app
Next, open two new terminals. In the first one, make a request to the /slow-async endpoint:
$ http GET http://localhost:8000/slow-async
Without waiting for the response, in the second terminal, make a request to the /fast endpoint:
$ http GET http://localhost:8000/fast
You’ll see that you have to wait 10 seconds before you get the response for the /fast endpoint. This means that /slow-async blocked the process and prevented the server from answering the other request while this was happening.
Now, let’s perform the same experiment with the /slow-sync endpoint:
$ http GET http://localhost:8000/slow-sync
And again, run the following command:
$ http GET http://localhost:8000/fast
You’ll immediately get the response of /fast without having to wait for /slow-sync to finish. Since it’s defined as a standard, non-async function, FastAPI will run it in a thread to prevent blocking. However, bear in mind that sending the task to a separate thread implies a small overhead, so it’s important to think about the best approach to your current problem.
So, when developing with FastAPI, how can you choose between standard or async functions for path operations and dependencies? The rules of thumb for this are as follows:
If the functions don’t involve long I/O operations (file reading, network requests, and so on), define them as async.
If they involve I/O operations, see the following:
Creating an Efficient Prediction API Endpoint with FastAPI
 Try to choose libraries that are compatible with asynchronous I/O, as we saw for databases
or HTTP clients. In this case, your functions will be async.

If it’s not possible, which is the case for Joblib caching, define them as standard functions. FastAPI will run them in a separate thread.
Since Joblib is completely synchronous at making I/O operations, we switched the path operations and the dependency method so that they were synchronous, standard methods.
In this example, the difference is not very noticeable because the I/O operations are small and fast. However, it’s good to keep this in mind if you have to implement slower operations, such as for performing file uploads to cloud storage.
Summary
Congratulations! You’re now able to build a fast and efficient REST API to serve your machine learning models. Thanks to Joblib, you learned how to dump a trained scikit-learn estimator into a file that’s easy to load and use inside your application. We also saw an approach to caching prediction results using Joblib. Finally, we discussed how FastAPI handles synchronous operations by sending them to a separate thread to prevent blocking. While this was a bit technical, it’s important to bear this aspect in mind when dealing with blocking I/O operations.
We’re near the end of our FastAPI journey. Before letting you build awesome data science applications by yourself, we will provide three more chapters to push this a bit further and study more complex use cases. We’ll start with an application that can perform real-time object detection, thanks to WebSockets and a computer vision model.
13 Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
In the previous chapter, you learned how to create efficient REST API endpoints to make predictions with trained machine learning models. This approach covers a lot of use cases, given that we have a single observation we want to work on. In some cases, however, we may need to continuously perform predictions on a stream of input – for instance, an object detection system that works in real time with video input. This is exactly what we’ll build in this chapter. How? If you remember, besides HTTP endpoints, FastAPI also has the ability to handle WebSockets endpoints, which allow us to send and receive streams of data. In this case, the browser will send into the WebSocket a stream of images from the webcam, and our application will run an object detection algorithm and send back the coordinates and label of each detected object in the image. For this task, we’ll rely on Hugging Face, which is both a set of tools and a library of pretrained AI models.
In this chapter, we’re going to cover the following main topics:
Using a computer vision model with Hugging Face libraries
Implementing an HTTP endpoint to perform object detection on a single image
Sending a stream of images from the browser in a WebSocket
Showing the object detection results in a browser
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
You’ll find all the code examples for this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter13.
Using a computer vision model with Hugging Face
Computer vision is a field of study and technology that focuses on enabling computers to extract meaningful information from digital images or videos, simulating human vision capabilities. It involves developing algorithms based on statistical methods or machine learning that allow machines to understand, analyze, and interpret visual data. A typical example of computer vision’s application is object detection: a system able to detect and recognize objects in an image. This is the kind of system we’ll build in this chapter.
To help us in this task, we’ll use a set of tools provided by Hugging Face. Hugging Face is a company whose goal is to allow developers to use the most recent and powerful AI models quickly and easily. For this, it has built two things:
A set of open source Python tools built on top of machine learning libraries such as PyTorch and TensorFlow. We’ll use some of them in this chapter.
An online library to share and download pretrained models for various machine learning tasks, such as computer vision or image generation.
You can read more about what it's doing on its official website: https://huggingface.co/.
You’ll see that it’ll greatly help us build a powerful and accurate object detection system in no time! To begin with, we’ll install all the libraries we need for this project:
(venv) $ pip install "transformers[torch]" Pillow
The transformers library from Hugging Face will allow us to download and run pretrained machine learning models. Notice that we install it with the optional torch dependency. Hugging Face tools can be used either with PyTorch or TensorFlow, which are both very powerful ML frameworks. Here, we chose to use PyTorch. Pillow is a widely used Python library for working with images. We’ll see why we need it soon.
Before starting to work with FastAPI, let’s implement a simple script to run an object detection algorithm. It consists of four main steps:
1. Load an image from the disk using Pillow.
2. Load a pretrained object detection model.
3. Run the model on our image.
4. Display the results by drawing rectangles around the detected objects.
Using a computer vision model with Hugging Face
We’ll go step by step through the implementation:
chapter13_object_detection.py
from pathlib import Path
import torch from PIL import Image, ImageDraw, ImageFont from transformers import YolosForObjectDetection, YolosImageProcessor
root_directory = Path(__file__).parent.parent picture_path = root_directory / "assets" / "coffee-shop.jpg" image = Image.open(picture_path)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ object_detection.py
As you can see, the first step is to load our image from the disk. For this example, we use the image named coffee-shop.jpg, which is available in our examples repository at https://github. com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI- Second-Edition/blob/main/assets/coffee-shop.jpg:
chapter13_object_detection.py
image_processor = YolosImageProcessor.from_pretrained("hustvl/yolos- tiny") model = YolosForObjectDetection.from_pretrained("hustvl/yolos-tiny")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ object_detection.py
Next, we load a model from Hugging Face. For this example, we chose the YOLOS model. It’s a cutting- edge approach to object detection that has been trained on 118K annotated images. You can read more about the technical approach in the following Hugging Face article: https://huggingface. co/docs/transformers/model_doc/yolos. To limit the download size and preserve your computer disk space, we chose here to use the tiny version, which is a lighter version of the original model that can be run on an average machine while maintaining good accuracy. This particular version is described here on Hugging Face: https://huggingface.co/hustvl/yolos-tiny.
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
Notice that we instantiate two things: an image processor and a model. If you remember what we said in Chapter 11, Introduction to Data Science in Python, you know that we need to have a set of features that will feed our ML algorithm. Hence, the role of the image processor is to transform a raw image into a set of characteristics that are meaningful to the model.
And that’s exactly what we’re doing in the following lines: we create an inputs variable by calling image_processor on our image. Notice that the return_tensors argument is set to pt for PyTorch since we chose to go with PyTorch as our underlying ML framework. Then, we can feed this inputs variable to our model to get outputs:
chapter13_object_detection.py
inputs = image_processor(images=image, return_tensors="pt") outputs = model(**inputs)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ object_detection.py
You might think that this is it for the prediction phase and that we could now display the results. However, that’s not the case. The result of such algorithms is a set of multi-dimensional matrices, the famous tensors, which don’t really make sense to us as humans. That’s why we need to revert those tensors into something that makes sense for the input image. That’s the purpose of the post_ process_object_detection operation provided by image_processor:
chapter13_object_detection.py
target_sizes = torch.tensor([image.size[::-1]]) results = image_processor.post_process_object_detection( outputs, target_sizes=target_sizes )[0]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ object_detection.py
The result of this operation is a dictionary with the following:
labels: The list of labels of each detected object
boxes: The coordinates of the bounding box of each detected object
scores: The confidence score of the algorithm for each detected object
Using a computer vision model with Hugging Face
All we need to do then is to iterate over them so we can draw the rectangle and the corresponding label thanks to Pillow. We just show the resulting image at the end. Notice that we only consider objects with a score greater than 0.7 to limit the number of false positives:
chapter13_object_detection.py
draw = ImageDraw.Draw(image) font_path = root_directory / "assets" / "OpenSans-ExtraBold.ttf" font = ImageFont.truetype(str(font_path), 24) for score, label, box in zip(results["scores"], results["labels"], results["boxes"]): if score > 0.7: box_values = box.tolist() label = model.config.id2label[label.item()] draw.rectangle(box_values, outline="red", width=5) draw.text(box_values[0:2], label, fill="red", font=font) image.show()
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ object_detection.py
Thanks to Pillow, we’re able to draw rectangles and add a label above the detected objects. Notice that we loaded a custom font, Open Sans, which is an open font available on the web: https://fonts. google.com/specimen/Open+Sans. Let’s try to run this script and see the result:
(venv) $ python chapter13/chapter13_object_detection.py
The first time it’ll run, you’ll see the model being downloaded. The prediction can then take a few seconds to run depending on your computer. When it’s done, the resulting image should automatically open, as shown in Figure 13.1.
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
Figure 13.1 – Object detection result on a sample image
You can see that the model detected several persons in the image, along with various objects such as the couch and a chair. And that’s it! Less than 30 lines of code to have a working object detection script! Hugging Face lets us harness all the power of the latest AI advances very efficiently.
Of course, our goal in this chapter is to put all this intelligence on a remote server so that we can serve this experience to thousands of users. Once again, FastAPI will be our ally here.
Implementing a REST endpoint to perform object detection on a single image
Before working with WebSockets, we’ll start simple and implement, using FastAPI, a classic HTTP endpoint to accept image uploads and perform object detection on them. As you’ll see, the main difference from the previous example is in how we acquire the image: instead of reading it from the disk, we get it from a file upload that we have to convert into a Pillow image object.
Besides, we’ll also use the exact same pattern we saw in Chapter 12, Creating an Efficient Prediction API Endpoint with FastAPI – that is, having a dedicated class for our prediction model, which will be loaded during the lifespan handler.
Implementing a REST endpoint to perform object detection on a single image
The first thing we do in this implementation is to define Pydantic models in order to properly structure the output of our prediction model. You can see this as follows:
chapter13_api.py
class Object(BaseModel): box: tuple[float, float, float, float] label: str
class Objects(BaseModel): objects: list[Object]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ api.py
We have a model for a single detected object, which consists of box, a tuple of four numbers describing the coordinates of the bounding box, and label, which corresponds to the type of detected object. The Objects model is a simple structure bearing a list of objects.
We won’t go through the model prediction class, as it’s very similar to what we saw in the previous chapter and section. Instead, let’s directly focus on the FastAPI endpoint implementation:
chapter13_api.py
object_detection = ObjectDetection()
@contextlib.asynccontextmanager async def lifespan(app: FastAPI): object_detection.load_model() yield
app = FastAPI(lifespan=lifespan)
@app.post("/object-detection", response_model=Objects) async def post_object_detection(image: UploadFile = File(...)) -> Objects: image_object = Image.open(image.file) return object_detection.predict(image_object)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_ api.py
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
Nothing very surprising here! The main point of attention is to correctly use the UploadFile and File dependencies so we get the uploaded file. If you need a refresher on this, you can check the Form data and file uploads section from Chapter 3, Developing a RESTful API with FastAPI. All we need to do then is to instantiate it as a proper Pillow image object and call our prediction model.
As we said, we don’t forget to load the model inside the lifespan handler.
You can run this example using the usual Uvicorn command:
(venv) $ uvicorn chapter13.chapter13_api:app
We’ll use the same coffee shop picture we already saw in the previous section. Let’s upload it on our endpoint with HTTPie:
$ http --form POST http://localhost:8000/object-detection image@./ assets/coffee-shop.jpg { "objects": [ { "box": [659.8709716796875, 592.8882446289062, 792.0460815429688, 840.2132568359375], "label": "person" }, { "box": [873.5499267578125, 875.7918090820312, 1649.1378173828125, 1296.362548828125], "label": "couch" } ] }
We correctly get the list of detected objects, each one with its bounding box and label. Great! Our object detection system is now available as a web server. However, our goal is still to make a real-time system: thanks to WebSockets, we’ll be able to handle a stream of images.
Implementing a WebSocket to perform object detection on a stream of images
One of the main benefits of WebSockets, as we saw in Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI, is that it opens a full-duplex communication channel between the client and the server. Once the connection is established, messages can be passed quickly without having to go through all the steps of the HTTP protocol. Therefore, it’s much more suited to sending a lot of data in real time.
Implementing a WebSocket to perform object detection on a stream of images
The point here will be to implement a WebSocket endpoint that is able to both accept image data and run object detection on it. The main challenge here will be to handle a phenomenon known as backpressure. Put simply, we’ll receive more images from the browser than the server is able to handle because of the time needed to run the detection algorithm. Thus, we’ll have to work with a queue (or buffer) of limited size and drop some images along the way to handle the stream in near real time.
We’ll go step by step through the implementation:
app.py
async def receive(websocket: WebSocket, queue: asyncio.Queue): while True: bytes = await websocket.receive_bytes() try: queue.put_nowait(bytes) except asyncio.QueueFull: pass
async def detect(websocket: WebSocket, queue: asyncio.Queue): while True: bytes = await queue.get() image = Image.open(io.BytesIO(bytes)) objects = object_detection.predict(image) await websocket.send_json(objects.dict())
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/app.py
We defined two tasks: receive and detect. The first one is waiting for raw bytes from the WebSocket, while the second one is performing the detection and sending the result, exactly as we saw in the last section.
The key here is to use the asyncio.Queue object. This is a convenient structure allowing us to queue some data in memory and retrieve it in a first in, first out (FIFO) strategy. We are able to set a limit on the number of elements we store in the queue: this is how we’ll be able to limit the number of images we handle.
The receive function receives data and puts it at the end of the queue. When working with asyncio. Queue, we have two methods to put a new element in the queue: put and put_nowait. If the queue is full, the first one will wait until there is room in the queue. This is not what we want here: we want to drop images that we won’t be able to handle in time. With put_nowait, the QueueFull exception is raised if the queue is full. In this case, we just pass and drop the data.
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
On the other hand, the detect function pulls the first message from the queue and runs its detection before sending the result. Notice that since we get raw image bytes directly, we have to wrap them with io.BytesIO to make it acceptable for Pillow.
The WebSocket implementation in itself is similar to what we saw in Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI. We are scheduling both tasks and waiting until one of them has stopped. Since they both run an infinite loop, this will happen when the WebSocket is disconnected:
app.py
@app.websocket("/object-detection") async def ws_object_detection(websocket: WebSocket): await websocket.accept() queue: asyncio.Queue = asyncio.Queue(maxsize=1) receive_task = asyncio.create_task(receive(websocket, queue)) detect_task = asyncio.create_task(detect(websocket, queue)) try: done, pending = await asyncio.wait( {receive_task, detect_task}, return_when=asyncio.FIRST_COMPLETED, ) for task in pending: task.cancel() for task in done: task.result() except WebSocketDisconnect: pass
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/app.py
Serving static files If you look at the full implementation of the preceding example, you’ll notice that we defined two more things in our server: an index endpoint, which just returns the index.html file, and a StaticFiles app, which is mounted under the /assets path. Both of them are here to allow our FastAPI application to directly serve our HTML and JavaScript code. This way, browsers will be able to query those files on the same server.
The key takeaway of this is that even though FastAPI was designed to build REST APIs, it’s also perfectly able to serve HTML and static files.
Sending a stream of images from the browser in a WebSocket
Our backend is now ready! Let’s now see how to use its power from a browser.
Sending a stream of images from the browser in a WebSocket
In this section, we’ll see how you can capture images from the webcam in the browser and send them through a WebSocket. Since it mainly involves JavaScript code, it’s admittedly a bit beyond the scope of this book, but it’s necessary to make the application work fully.
The first step is to enable a camera input in the browser, open the WebSocket connection, pick a camera image, and send it through the WebSocket. Basically, it’ll work like this: thanks to the MediaDevices browser API, we’ll be able to list all the camera inputs available on the device. With this, we’ll build a selection form so the user can select the camera they want to use. You can see the concrete JavaScript implementation in the following code:
script.js
window.addEventListener('DOMContentLoaded', (event) => { const video = document.getElementById('video'); const canvas = document.getElementById('canvas'); const cameraSelect = document.getElementById('camera-select'); let socket;
// List available cameras and fill select navigator.mediaDevices.getUserMedia({ audio: true, video: true }).then(() => { navigator.mediaDevices.enumerateDevices().then((devices) => { for (const device of devices) { if (device.kind === 'videoinput' && device.deviceId) { const deviceOption = document.createElement('option'); deviceOption.value = device.deviceId; deviceOption.innerText = device.label; cameraSelect.appendChild(deviceOption); } } }); });
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/assets/script.js
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
Once the user submits the form, we call a startObjectDetection function with the selected camera. Most of the actual detection logic is implemented in this function:
script.js
// Start object detection on the selected camera on submit document.getElementById('form-connect').addEventListener('submit', (event) => { event.preventDefault();
// Close previous socket is there is one if (socket) { socket.close(); }
const deviceId = cameraSelect.selectedOptions[0].value; socket = startObjectDetection(video, canvas, deviceId); }); });
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/assets/script.js
Let’s have a look at the startObjectDetection function in the following code block. First, we establish a connection with the WebSocket. Once it’s opened, we can start to get an image stream from the selected camera. For this, we use the MediaDevices API to start capturing video and display the output in an HTML <video> element. You can read all the details about the MediaDevices API in the MDN documentation: https://developer.mozilla.org/en-US/docs/Web/ API/MediaDevices:
script.js
const startObjectDetection = (video, canvas, deviceId) => { const socket = new WebSocket(`ws://${location.host}/object- detection`); let intervalId;
// Connection opened socket.addEventListener('open', function () {
// Start reading video from device navigator.mediaDevices.getUserMedia({ audio: false, video: {
Sending a stream of images from the browser in a WebSocket
deviceId, width: { max: 640 }, height: { max: 480 }, }, }).then(function (stream) { video.srcObject = stream; video.play().then(() => { // Adapt overlay canvas size to the video size canvas.width = video.videoWidth; canvas.height = video.videoHeight;
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/assets/script.js
Then, as shown in the next code block, we launch a repetitive task that captures an image from the video input and sends it to the server. To do this, we have to use a <canvas> element, an HTML tag dedicated to graphics drawing. It comes with a complete JavaScript API so that we can programmatically draw images in it. There, we’ll be able to draw the current video image and convert it into valid JPEG bytes. If you want to know more about this, MDN gives a very detailed tutorial on <canvas>: https:// developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial:
script.js
// Send an image in the WebSocket every 42 ms intervalId = setInterval(() => {
// Create a virtual canvas to draw current video image const canvas = document.createElement('canvas'); const ctx = canvas.getContext('2d'); canvas.width = video.videoWidth; canvas.height = video.videoHeight; ctx.drawImage(video, 0, 0);
// Convert it to JPEG and send it to the WebSocket canvas.toBlob((blob) => socket.send(blob), 'image/jpeg'); }, IMAGE_INTERVAL_MS); });
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
}); });
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/assets/script.js
Notice that we limit the size of the video input to 640 by 480 pixels, so that we don’t blow up the server with images that are too big. Besides, we set the interval to run every 42 milliseconds (the value is set in the IMAGE_INTERVAL_MS constant), which is roughly equivalent to 24 images per second.
Finally, we wire the event listener to handle the messages received from the WebSocket. It calls the drawObjects function, which we’ll detail in the next section:
script.js
// Listen for messages socket.addEventListener('message', function (event) { drawObjects(video, canvas, JSON.parse(event.data)); });
// Stop the interval and video reading on close socket.addEventListener('close', function () { window.clearInterval(intervalId); video.pause(); });
return socket; };
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/assets/script.js
Showing the object detection results in the browser
Now that we are able to send input images to the server, we have to show the result of the detection in the browser. In a similar way to what we showed in the Using a computer vision model with Hugging Face section, we’ll draw a green rectangle around the detected objects, along with their label. Thus, we have to find a way to take the rectangle coordinates sent by the server and draw them in the browser.
Showing the object detection results in the browser
To do this, we’ll once again use a <canvas> element. This time, it’ll be visible to the user and we’ll draw the rectangles using it. The trick here is to use CSS so that this element overlays the video: this way, the rectangles will be shown right on top of the video and the corresponding objects. You can see the HTML code here:
index.html
<body> <div class="container"> <h1 class="my-3">Chapter 13 - Real time object detection</h1> <form id="form-connect"> <div class="mb-3 input-group"> <select id="camera-select"></select> <button class="btn btn-success" type="submit" id="button- start">Start</button> </div> </form> <div class="position-relative" style="width: 640px; height: 480px;"> <video id="video"></video> <canvas id="canvas" class="top-0 position-absolute start-0"></ canvas> </div> </div>
<script src="/assets/script.js"></script> </body>
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/index.html
We are using CSS classes from Bootstrap, a very common CSS library with a lot of helpers like this. Basically, we set the canvas with absolute positioning and put it at the top left so that it covers the video element.
The key now is to use the Canvas API to draw the rectangles according to the received coordinates. This is the purpose of the drawObjects function, which is shown in the next sample code block:
script.js
const drawObjects = (video, canvas, objects) => { const ctx = canvas.getContext('2d');
ctx.width = video.videoWidth; ctx.height = video.videoHeight;
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
ctx.beginPath(); ctx.clearRect(0, 0, ctx.width, ctx.height); for (const object of objects.objects) { const [x1, y1, x2, y2] = object.box; const label = object.label; ctx.strokeStyle = '#49fb35'; ctx.beginPath(); ctx.rect(x1, y1, x2 - x1, y2 - y1); ctx.stroke();
ctx.font = 'bold 16px sans-serif'; ctx.fillStyle = '#ff0000'; ctx.fillText(label, x1 - 5 , y1 - 5); } };
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter13/websock- et_object_detection/assets/script.js
With the <canvas> element, we can use a 2D context to draw things in the object. Notice that we first clean everything to remove the rectangles from the previous detection. Then, we loop through all the detected objects and draw a rectangle with the given coordinates: x1, y1, x2, and y2. Finally, we take care of drawing the label slightly above the rectangle.
Our system is now complete! Figure 13.2 gives you an overview of the file structure we’ve implemented.
Figure 13.2 – Object detection application structure
It’s time to give it a try! We can start it using the usual Uvicorn command:
(venv) $ uvicorn chapter13.websocket_object_detection.app:app
Showing the object detection results in the browser
You can access the application in your browser with the address http://localhost:8000. As we said in the previous section, the index endpoint will be called and will return our index.html file.
You’ll see an interface inviting you to choose the camera you want to use, as shown in Figure 13.3:
Figure 13.3 – Webcam selection for the object detection web application
Select the webcam you wish to use and click on Start. The video output will show up, object detection will start via the WebSocket, and green rectangles will be drawn around the detected objects. We show this in Figure 13.4:
Figure 13.4 – Running the object detection web application
Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
It works! We brought the intelligence of our Python system right to the user’s web browser. This is just an example of what you could achieve using WebSockets and ML algorithms, but this definitely enables you to create near real-time experiences for your users.
Summary
In this chapter, we showed how WebSockets can help us bring a more interactive experience to users. Thanks to the pretrained models provided by the Hugging Face community, we were able to quickly implement an object detection system. Then, we integrated it into a WebSocket endpoint with the help of FastAPI. Finally, by using a modern JavaScript API, we sent video input and displayed algorithm results directly in the browser. All in all, a project like this might sound complex to make at first, but we saw that powerful tools such as FastAPI enable us to get results in a very short time and with very comprehensible source code.
Until now, in our different examples and projects, we assumed the ML model we used was fast enough to be run directly in an API endpoint or a WebSocket task. However, that’s not always the case. In some cases, the algorithm is so complex it takes a couple of minutes to run. If we run this kind of algorithm directly inside an API endpoint, the user would have to wait a long time before getting a response. Not only would this be strange for them but this would also quickly block the whole server, preventing other users from using the API. To solve this, we’ll need a companion for our API server: a worker.
In the next chapter, we’ll study a concrete example of this challenge: we’ll build our very own AI system to generate images from a text prompt!
14 Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Until now, in this book, we’ve built APIs where all the operations were computed inside the request handling. Said another way, before they could get their response, the user had to wait for the server to do everything we had defined: request validation, database queries, ML predictions, and so on. However, this behavior is not always desired or possible.
A typical example is email notifications. It happens quite often in a web application that we need to send an email to the user because they just registered or they performed a specific action. To do this, the server needs to send a request to an email server so the email can be sent. This operation could take a few milliseconds. If we do this inside the request handling, the response will be delayed until we send the email. This is not a very good experience since the user doesn’t really care how and when the email is sent. This example is typical of what we usually call background operations: things that need to be done in our application but don’t require direct user interaction.
Another case is when the user requests an expensive operation that can’t be done in a reasonable time. It’s usually the case for complex data exports or heavy AI models. In this context, the user would like to get the result directly, but doing this in the request handler would block the server process until it’s done. If lots of users were requesting this kind of operation, it would quickly make our server unresponsive. Besides, some network infrastructure such as proxy or web clients, like browsers, have quite strict timeout settings, meaning they will usually cancel an operation if it takes too much time to respond.
To solve this, we’ll introduce a typical architecture for web applications: web-queue-worker. As we’ll see in this chapter, we’ll defer the most expensive, long operations to a background process, a worker. To show you this architecture in action, we’ll build our very own AI system to generate images from text prompts using the Stable Diffusion model.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
In this chapter, we’re going to cover the following main topics:
Using the Stable Diffusion model with Hugging Face Diffusers to generate images from text prompts
Implementing a worker process using Dramatiq and an image-generation task
Storing and serving files in object storage
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
To run the Stable Diffusion model correctly, we recommend you have a recent computer equipped with at least 16 GB of RAM and, ideally, a dedicated GPU with 8 GB of VRAM. For Mac users, recent models equipped with the M1 Pro or M2 Pro chips are also a good fit. If you don’t have that kind of machine, don’t worry: we’ll show you ways to run the system anyway – the only drawback is that image generation will be slow and show poor results.
For running the worker, you’ll need a running Redis server on your local computer. The easiest way is to run it as a Docker container. If you’ve never used Docker before, we recommend you read the Getting started tutorial in the official documentation at https://docs.docker.com/get-started/. Once done, you’ll be able to run a Redis server with this simple command:
$ docker run -d --name worker-redis -p 6379:6379 redis
You’ll find all the code examples of this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter14.
Generating images from text prompts with Stable Diffusion
Recently, a new generation of AI tools has emerged and fascinated the whole world: image-generation models, such as DALL-E or Midjourney. Those models are trained on huge amounts of image data and are able to generate completely new images from a simple text prompt. These AI models are very good use cases for background workers: they take seconds or even minutes to process, and they need lots of resources in the CPU, RAM, and even the GPU.
To build our system, we’ll rely on Stable Diffusion, a very popular image-generation model that was released in 2022. This model is available publicly and can be run on a modern gaming computer. As we did in the previous chapter, we’ll rely on Hugging Face tools for both downloading the model and running it.
Generating images from text prompts with Stable Diffusion
Let’s first install the required tools:
(venv) $ pip install accelerate diffusers
We’re now ready to use diffuser models thanks to Hugging Face.
Implementing the model in a Python script
In the following example, we’ll show you the implementation of a class able to instantiate the model and run an image generation. Once again, we’ll apply our lazy loading pattern with separate load_model and generate methods. Let’s first focus on load_model:
text_to_image.py
class TextToImage: pipe: StableDiffusionPipeline | None = None
def load_model(self) -> None: # Enable CUDA GPU if torch.cuda.is_available(): device = "cuda" # Enable Apple Silicon (M1) GPU elif torch.backends.mps.is_available(): device = "mps" # Fallback to CPU else: device = "cpu"
pipe = StableDiffusionPipeline.from_pretrained("runwayml/ stable-diffusion-v1-5") pipe.to(device) self.pipe = pipe
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter14/basic/ text_to_image.py
The first part of this method aims to find the most efficient way to run the model given your computer. These diffusion models are faster when run on the GPU – that’s why we check first if there are CUDA (NVIDIA GPU) or MPS (Apple Silicon) devices available. If there are none, we fall back to the CPU.
Then, we simply have to create a StableDiffusionPipeline pipeline, as provided by Hugging Face. We simply have to set the model we want to download from the hub. For this example, we chose runwayml/stable-diffusion-v1-5. You can find its details on Hugging Face: https:// huggingface.co/runwayml/stable-diffusion-v1-5.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We can now focus on the generate method:
text_to_image.py
def generate( self, prompt: str, *, negative_prompt: str | None = None, num_steps: int = 50, callback: Callable[[int, int, torch.FloatTensor], None] | None = None, ) Image.Image: if not self.pipe: raise RuntimeError("Pipeline is not loaded") return self.pipe( prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, guidance_scale=9.0, callback=callback, ).images[0]
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter14/basic/ text_to_image.py
You can see it accepts four parameters:
prompt, which is, of course, the text prompt describing the image we want to generate.
negative_prompt, which is an optional prompt to tell the model what we absolutely don’t want.
num_steps, which is the number of inference steps the model should run. More steps lead to a better image, but each iteration delays the inference. The default, 50, should provide a good balance between speed and quality.
callback, which is an optional function that will be called at each iteration step. This is helpful to be informed about the progress of the generation and possibly execute more logic, such as saving the progress in a database.
Generating images from text prompts with Stable Diffusion
What does the asterisk (*) in the method signature mean? You may have noticed the asterisk, *, in the method signature. It tells Python that the arguments coming after this symbol should only be treated as keyword-only arguments. Said another way, you can only call them like this: .generate("PROMPT", negative_prompt="NEGATIVE", num_steps=10).
While not necessary, it’s a way to keep your functions clear and self-explanatory. It’s especially true if you develop classes or functions that are meant to be used by other developers. Another syntax also exists to force arguments to be positional-only, using a slash (/) symbol. You can read more about it here: https://docs.python.org/3/ whatsnew/3.8.html#positional-only-parameters.
All we have to do then is to pass those parameters to pipe. There are a lot more parameters for you to tune if needed, but the default ones should give you quite good results. You can find the whole list of them in the Hugging Face documentation: https://huggingface.co/ docs/diffusers/api/pipelines/stable_diffusion/text2img#diffusers. StableDiffusionPipeline.__call__. This pipe object is able to generate several images per prompt, that’s why the result of this operation is a list of Pillow images. The default here is to generate only one image, so we directly return the first one.
And that’s about it! Once again, Hugging Face makes our lives really easy by allowing us to run cutting-edge models in dozens of lines!
Executing the Python script
We bet that you’re eager to try it yourself – that’s why we added a small main script at the bottom of our example:
text_to_image.py
if __name__ == "__main__": text_to_image = TextToImage() text_to_image.load_model()
def callback(step: int, _timestep, _tensor): Step {step}") print(f"
🚀
image = text_to_image.generate( "A Renaissance castle in the Loire Valley", negative_prompt="low quality, ugly", callback=callback,
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
) image.save("output.png")
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter14/basic/ text_to_image.py
This small script instantiates our TextToImage class, loads the model, and generates an image before saving it to disk. We also define a dummy callback function so you can see how it works.
When you run this script for the first time, you’ll notice that Hugging Face downloads files of several gigabytes to your computer: that’s the Stable Diffusion model, and it’s indeed quite big!
Then, the inference will start. You’ll see a progress bar showing you how many inference steps are left, along with the print statement from our callback, as shown in Figure 14.1.
Figure 14.1 – Stable Diffusion generating an image
How much time does it take to generate a single image? We’ve run several tests on different types of computers. With a modern NVIDIA GPU with 8 GB of RAM or a Mac with an M1 Pro chip, the model is able to generate an image with 50 inference steps in around a minute, with reasonable RAM usage. When run on a CPU, it takes around 5 to 10 minutes and eats up to 16 GB of RAM.
If the inference is really too slow on your computer, you can try to reduce the num_steps parameter.
Creating a Dramatiq worker and defining an image-generation task
When the inference is done, you’ll find your generated image on the disk along with your script. Figure 14.2 shows an example of such a result. Nice, isn’t it?
Figure 14.2 – Result of a Stable Diffusion image generation
We now have the fundamental brick of our AI system. Now, we need to build an API so users can generate their own images. As we’ve just seen, generating a single image takes some time. As we said in the introduction, we’ll need to introduce a web-queue-worker architecture to make this system reliable and scalable.
Creating a Dramatiq worker and defining an image- generation task
As we mentioned in the introduction of this chapter, it’s not conceivable to run our image-generation model directly on our REST API server. As we saw in the previous section, the operation can take several minutes and consumes a massive amount of memory. To solve this, we’ll define another process, apart from the server process, that’ll take care of this image-generation task: the worker. In essence, a worker can be any program whose role is to compute a task in the background.
In web development, this concept usually implies a bit more than this. A worker is a process running continuously in the background, waiting for incoming tasks. The tasks are usually sent by the web server, which asks for specific operations given the user actions.
Therefore, we see that we need a communication channel between the web server and the worker. That’s the role of the queue. It’ll accept and stack messages coming from the web server and make them available to read for the worker. That’s the web-queue-worker architecture. To better understand it, Figure 14.4 shows you the schema of such an architecture.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Figure 14.3 – Schema of web-queue-worker architecture
Does it ring a bell? Yes, it’s very similar to what we saw in Chapter 8, in the Handling multiple WebSocket connections and broadcasting messages section. Actually, this is the same principle: we solve the problem of having separate processes by having a single central data source.
The great feature of this architecture is that it scales very easily. Imagine your application is a huge success and thousands of users want to generate images: a single worker wouldn’t be able to meet the demand. Actually, all we need to do is to start more worker processes. Since there is a single message broker in the architecture, each worker will pull messages as they come, allowing tasks to be processed in parallel. They don’t even need to be on the same physical machine. This is shown in Figure 14.4.
Figure 14.4 – Web-queue-worker architecture with multiple workers
Creating a Dramatiq worker and defining an image-generation task
In Python, there are several libraries to help implement a worker. They provide the required tools to define tasks, schedule them in the queue, and run a process, pulling them and executing them. In this book, we’ll use Dramatiq, a lightweight but powerful and modern background task-processing library. As we did in Chapter 8, we’ll use Redis as a message broker.
Implementing a worker
As usual, we’ll start by installing the required dependency. Run the following command:
(venv) $ pip install "dramatiq[redis]"
This will install Dramatiq with the required dependencies to talk with a Redis broker.
In a minimal example, setting up a Dramatiq worker involves two things:
1. Setting the broker type and URL.
2. Defining tasks by wrapping functions with the @dramatiq.actor decorator.
It works very well for the vast majority of tasks, such as sending emails or generating exports.
In our case, however, we need to load the heavy Stable Diffusion model. As we usually do in the FastAPI server with the startup event, we want to do this only when the process is actually started. To do this with Dramatiq, we implement a middleware. They allow us to plug custom logic at several key events in the lifetime of the worker, including when it’s started.
You can see the implementation of our custom middleware in the following sample:
worker.py
class TextToImageMiddleware(Middleware): def __init__(self) -> None: super().__init__() self.text_to_image = TextToImage()
def after_process_boot(self, broker): self.text_to_image.load_model() return super().after_process_boot(broker)
text_to_image_middleware = TextToImageMiddleware() redis_broker = RedisBroker(host="localhost") redis_broker.add_middleware(text_to_image_middleware) dramatiq.set_broker(redis_broker)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/basic/work- er.py
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We define a TextToImageMiddleware class whose role is to bear an instance of TextToImage, the image generation service we defined in the previous section. It inherits from the Middleware class of Dramatiq. The key thing here is the after_process_boot method. It’s one of the event hooks exposed by Dramatiq, allowing us to plug our own logic. Here, we tell it to load the Stable Diffusion model when the worker process has booted up. You can see the full list of supported hooks in the official documentation: https://dramatiq.io/reference.html#middleware.
The next lines allow us to configure our worker. We first instantiate an instance of our custom middleware. Then, we create a broker class corresponding to the technology we chose; in our case, Redis. We take care of adding our middleware to this broker before telling Dramatiq to use it. Our worker is now completely configured to connect to a Redis broker and load our model at startup.
Now, let’s see how we can define a task to generate images:
worker.py
@dramatiq.actor() def text_to_image_task( prompt: str, *, negative_prompt: str | None = None, num_steps: int = 50 ): image = text_to_image_middleware.text_to_image.generate( prompt, negative_prompt=negative_prompt, num_steps=num_steps ) image.save(f"{uuid.uuid4()}.png")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/basic/work- er.py
The implementation is straightforward: Dramatiq tasks are actually plain functions that we decorated with @dramatiq.actor. We can define arguments as we would for any other function. However, there is an important pitfall to avoid here: when we schedule tasks from our server, the arguments will have to be stored in the queue storage. Thus, Dramatiq will internally serialize the arguments to JSON. It means your task arguments must be serializable data – you can’t have arbitrary Python objects, such as class instances or functions.
The function body calls our TextToImage instance loaded in text_to_image_middleware, before saving the image to the disk. To avoid file overrides, we choose here to generate a UUID, a Universally Unique IDentifier. It’s a big random string that’s guaranteed to be unique in each generation. Thanks to this, we can safely use it as a filename and be sure it won’t already exist on our disk.
That’s it for the worker implementation.
Creating a Dramatiq worker and defining an image-generation task
Starting the worker
We don’t have the web server code to call it yet, but we can already try it manually. First, make sure you have a Redis server started, as explained in the Technical requirements section. Then, we can start the Dramatiq worker using the following command:
(venv) $ dramatiq -p 1 -t 1 chapter14.basic.worker
Dramatiq comes with command-line tools to take care of starting the worker processes. The main positional argument is the dotted path of your worker module. It’s similar to what we do with Uvicorn. We also set two optional parameters, -p and -t. They control the number of processes and threads Dramatiq will start. By default, it starts 10 processes, each one with 8 threads. This means there will be 80 workers able to pull and execute tasks. While this default is good for common needs, it doesn’t work with our Stable Diffusion model for two reasons:
Each thread in a process shares the same memory space. This means that if two (or more) threads try to generate an image, they will read and write on the same objects in memory. For our model here, this causes concurrency problems. We say that it’s not thread-safe. Hence, each process should start only one thread: that’s the point of the -t 1 option.
Each process should load the model in memory. This means that if we start 8 processes, we’ll load the model 8 times. As we saw earlier, it takes quite a huge amount of memory, so doing this would probably blow up your computer’s memory. To be safe here, we start only one process thanks to the -p 1 option. If you want to try parallelization and see that our worker is able to generate two images in parallel, you can try -p 2 to spawn two processes. Make sure your computer can handle it though!
If you run the preceding command, you should see an output like this:
[2023-02-02 08:52:11,479] [PID 44348] [MainThread] [dramatiq. MainProcess] [INFO] Dramatiq '1.13.0' is booting up. Fetching 19 files: 0%| | 0/19 [00:00<?, ?it/s] Fetching 19 files: 100%|██████████| 19/19 [00:00<00:00, 13990.83it/s] [2023-02-02 08:52:11,477] [PID 44350] [MainThread] [dramatiq. WorkerProcess(0)] [INFO] Worker process is ready for action. [2023-02-02 08:52:11,578] [PID 44355] [MainThread] [dramatiq. ForkProcess(0)] [INFO] Fork process 'dramatiq.middleware.prometheus:_ run_exposition_server' is ready for action.
You can see the output of the Stable Diffusion pipeline checking whether the model files are downloaded before the worker is fully started. This means that it has been correctly loaded.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Scheduling tasks in the worker
We can now try to schedule tasks in our worker. For this, we can start a Python interactive shell and import the task function. Open a new command line and run the following commands (make sure you enabled your Python virtual environment):
(venv) $ python >>> from chapter14.basic.worker import text_to_image_task >>> text_to_image_task.send("A Renaissance castle in the Loire Valley") Message(queue_name='default', actor_name='text_to_image_task', args=('A Renaissance castle in the Loire Valley',), kwargs={}, options={'redis_message_id': '663df44a-cfc1-4f13-8457-05d8181290c1'}, message_id='bf57d112-6c20-49bc-a926-682ca43ea7ea', message_ timestamp=1675324585644)
That’s it – we scheduled a task in the worker! Notice how we used the send method on our task function instead of calling it directly: this is how you tell Dramatiq to send it in the queue.
If you go back to your worker terminal, you’ll see the Stable Diffusion output generating the image. After a moment, you’ll have your image saved on disk. You can also try to send two tasks in a row in a short time. You’ll find that Dramatiq processes them one after the other.
Great job! We have our background process ready and are even able to schedule tasks in it. The next step now is to implement a REST API so the users can ask for image generation themselves.
Implementing the REST API
To schedule tasks in our worker, we need a safe interface users can interact with. A REST API is a good choice for this, since it can be easily integrated into any software, such as a website or a mobile app. In this section, we’ll very quickly review a simple API endpoint we implemented to send image- generation tasks into our queue. Here’s the implementation:
api.py
class ImageGenerationInput(BaseModel): prompt: str negative_prompt: str | None num_steps: int = Field(50, gt=0, le=50)
class ImageGenerationOutput(BaseModel): task_id: UUID4
app = FastAPI()
Storing results in a database and object storage
@app.post( "/image-generation", response_model=ImageGenerationOutput, status_code=status.HTTP_202_ACCEPTED, ) async def post_image_generation(input: ImageGenerationInput) -> ImageGenerationOutput: task: Message = text_to_image_task.send( input.prompt, negative_prompt=input.negative_prompt, num_ steps=input.num_steps ) return ImageGenerationOutput(task_id=task.message_id)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/basic/api. py
If you have followed along since the beginning of this book, this shouldn’t surprise you. We took care of defining proper Pydantic models to structure and validate the endpoint payload. This data is then directly used to send a task to Dramatiq, as we saw in the previous section.
In this simple implementation, the output consists only of the message ID, which is automatically assigned to each task by Dramatiq. Notice that we set the HTTP status code to 202, which means Accepted. Semantically, it means the server understood and accepted the request, but the processing has not yet finished or even started. It’s specifically designed for cases where the processing is done in the background, which is exactly our case here.
If you start both the worker and this API, you’ll be able to trigger image generations with an HTTP call.
You’re probably wondering here: That’s nice… But how will the users retrieve the result? How will they know whether the task is done?. You’re right – we didn’t talk at all about this problem! Actually, there are two aspects to solve here: how do we keep track of the pending tasks and their execution? How do we store and serve the resulting images? That’s the subject of the next section.
Storing results in a database and object storage
In the previous section, we showed how to implement a background worker to do the heavy computation and an API to schedule tasks on this worker. However, we are still missing two important aspects: the user doesn’t have any way to know the progress of the task nor to retrieve the final result. Let’s fix this!
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Sharing data between the worker and the API
As we’ve seen, the worker is a program running in the background executing the computations the API has asked it to do. However, the worker doesn’t have any way to talk with the API server. That’s expected: since there could be any number of server processes, and since they could even run on different physical servers, processes cannot communicate directly. It’s always the same problem of having a central data source on which processes can write and read data.
Actually, the first approach to solve the lack of communication between the API and the worker could be to use the same broker we use to schedule tasks: the worker could write results in the broker, and the API could read from it. This is something possible with most background task libraries, including Dramatiq. However, this solution has some limitations, the principal one being the limited time we can retain the data. Brokers, such as Redis, are not really suited to storing data reliably for a long period. At some point, we’ll need to erase the most ancient data to limit memory usage.
Yet, we already know of something able to store structured data efficiently: a database, of course! That’s the approach we’ll show here. By having a central database where we’ll store our image generation requests and results, we’ll be able to share information between the worker and the API. For this, we’ll reuse a lot of techniques we showed in the Communicating with a SQL database with SQLAlchemy ORM section of Chapter 6. Let’s go!
Defining an SQLAlchemy model
The first step is defining an SQLAlchemy model to store a single image-generation task. You can see it as follows:
models.py
class GeneratedImage(Base): __tablename__ = "generated_images"
id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True) created_at: Mapped[datetime] = mapped_column( DateTime, nullable=False, default=datetime.now ) progress: Mapped[int] = mapped_column(Integer, nullable=False, default=0)
prompt: Mapped[str] = mapped_column(Text, nullable=False) negative_prompt: Mapped[str | None] = mapped_column(Text, nullable=True) num_steps: Mapped[int] = mapped_column(Integer, nullable=False)
Storing results in a database and object storage
file_name: Mapped[str | None] = mapped_column(String(255), nullable=True)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ models.py
As usual, we define an auto-incremented ID as the primary key. We also add prompt, negative_ prompt, and num_steps columns, which correspond to the arguments we give to the worker task. This way, we’ll be able to directly give the ID to the worker, and it’ll take the parameter directly from the object. Besides, it’ll allow us to store and remember the parameters we used for a specific generation.
The progress column is an integer where we’ll store the current progress of the generation task.
Finally, file_name will store the actual filename we’ll store on our system. We’ll see how we use it in the next section, about object storage.
Adapting the API to save image-generation tasks in a database
With this model at hand, our approach to scheduling image generation in the API changes a bit. Instead of directly sending the task to the worker, we first create a row in our database and use the ID of this object as input for the worker task. The endpoint implementation is shown here:
api.py
@app.post( "/generated-images", response_model=schemas.GeneratedImageRead, status_code=status.HTTP_201_CREATED, ) async def create_generated_image( generated_image_create: schemas.GeneratedImageCreate, session: AsyncSession = Depends(get_async_session), ) GeneratedImage: image = GeneratedImage(**generated_image_create.dict()) session.add(image) await session.commit()
text_to_image_task.send(image.id)
return image
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ api.py
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We won’t go into the details about how to create an object in a database with SQLAlchemy ORM. If you need a refresher, you can refer to the Communicating with a SQL database with SQLAlchemy ORM section of Chapter 6.
The main thing to notice in this snippet is that we pass the ID of the newly created object as an argument of text_to_image_task. As we’ll see right after, the worker will read it again from the database to retrieve the generation parameters.
The response of this endpoint is simply a representation of our GeneratedImage model, using the Pydantic schema GeneratedImageRead. Thus, the user will get a response like this to their request:
{ "created_at": "2023-02-07T10:17:50.992822", "file_name": null, "id": 6, "negative_prompt": null, "num_steps": 50, "progress": 0, "prompt": "a sunset over a beach" }
It shows the prompt we gave in our request and, most importantly, it gives it an ID. This means that the user will be able to query for this specific request again to retrieve the data and see whether it’s done. That’s the purpose of the get_generated_image endpoint defined below the previous snippet. We won’t show it here, but you can read it in the examples repository.
Adapting the worker to read and update image-generation tasks from a database
You probably have guessed that we need to change the implementation of our task so it can retrieve objects from the database instead of reading the parameters directly. Let’s go through this step by step.
The first thing we do is retrieve a GeneratedImage from the database using the ID we got in the task argument.
worker.py
@dramatiq.actor() def text_to_image_task(image_id: int): image = get_image(image_id)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
Storing results in a database and object storage
To achieve this, you see that we use a helper function called get_image. It’s defined right above the task. Let’s review it:
worker.py
def get_image(id: int) -> GeneratedImage: async def _get_image(id: int) -> GeneratedImage: async with async_session_maker() as session: select_query = select(GeneratedImage). where(GeneratedImage.id == id) result = await session.execute(select_query) image = result.scalar_one_or_none()
if image is None: raise Exception("Image does not exist")
return image
return asyncio.run(_get_image(id))
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
It may look quite strange, but actually, you are already familiar with most of its logic. If you look closely, you’ll see that it defines a nested and private function where we define the actual logic to retrieve and save the object using SQLAlchemy ORM. Notice that it’s async, and that we make great use of async I/O patterns, as we’ve seen throughout this book.
That’s the exact reason why we need a helper function like this. Indeed, Dramatiq is not designed to run async functions natively, so we need to manually schedule their execution using asyncio. run. We already saw this function in Chapter 2, where we presented async I/O. Its role is to run an async function and return its result. That’s how we can call the wrapping function synchronously in our task without any issues.
Other approaches could work to tackle the async I/O problem The approach we show here is the most straightforward and robust one to tackle the problem of asynchronous workers.
Another approach could be to set up a decorator or middleware for Dramatiq so it could natively run async functions, but this is complex and subject to bugs.
We could also consider having another SQLAlchemy engine and session maker that works synchronously. However, this would require us to have a lot of duplicated things in our code. Besides, this wouldn’t help if we had async functions other than SQLAlchemy.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Now, let’s get back to the implementation of text_to_image_task:
worker.py
@dramatiq.actor() def text_to_image_task(image_id: int): image = get_image(image_id)
def callback(step: int, _timestep, _tensor): update_progress(image, step)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
We define a callback function for the Stable Diffusion pipeline. Its role is to save the current progress in a database for the current GeneratedImage. For this, we once again use a helper function, update_progress:
worker.py
def update_progress(image: GeneratedImage, step: int): async def _update_progress(image: GeneratedImage, step: int): async with async_session_maker() as session: image.progress = int((step / image.num_steps) * 100) session.add(image) await session.commit()
asyncio.run(_update_progress(image, step))
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
We use the same approach we explained for get_image, so we can wrap the async function.
Going back to text_to_image_task, we can now call our TextToImage model to generate an image. It’s exactly the same call we showed in the previous section. The only difference is that we take the parameters from the image object. We also generate a random filename using a UUID:
worker.py
image_output = text_to_image_middleware.text_to_image.generate( image.prompt, negative_prompt=image.negative_prompt,
Storing results in a database and object storage
num_steps=image.num_steps, callback=callback, )
file_name = f"{uuid.uuid4()}.png"
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
The following part is designed to upload the image to object storage. We’ll explain this in more detail in the next section:
worker.py
storage = Storage() storage.upload_image(image_output, file_name, settings.storage_ bucket)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
Finally, we call another helper function, update_file_name, to save the random filename in the database. It’ll allow us to retrieve the file for the user:
worker.py
update_file_name(image, file_name)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
As you can see, the main point of attention throughout this implementation is that we read and write information about GeneratedImage from and to the database. This is how we can synchronize between the API server and the worker. That’s it for the worker! With this logic, we are able to schedule an image-generation task from the API, and the worker is able to regularly update the task progress before setting the resulting filename. Thus, from the API, a simple GET request allows us to see the status of our task.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Storing and serving files in object storage
The last challenge we have to tackle concerns the storage of our resulting images. We need a way to store them reliably while letting users retrieve them easily from the internet.
Traditionally, web applications handled this quite simply. They stored the files directly on the server hard disk, in a defined directory, and configured their web server to serve those files when accessed under a certain URL. This is actually what we did in Chapter 13, in the WebSocket example: we used the StaticFiles middleware to statically serve the JavaScript script we had on disk.
While this works well for static files, such as JavaScript or CSS files, for which each server has its own copy, it is not suitable for dynamic files uploaded by the user or generated by the backend, in particular for complex architectures where several processes are run on different physical machines. Once again, this is the problem of having a central source of data that the different processes read from. In the previous sections, we saw that message brokers and databases could solve this issue in several contexts. In the case of arbitrary binary files, whether they are images, videos, or simple text files, we need something else. Let’s introduce object storage.
Object storage is a bit different from the standard file storage we use daily in computers, where the disk is organized in a hierarchy of directories and files. Instead, object storage will store each file as an object, which includes the actual data and all its metadata, such as its name, size, type, and a unique ID. The main benefit of such conceptualization is that it’s easier to spread those files across multiple physical machines: we can store billions of files on the same object storage. From the user’s point of view, we just ask for a specific file, and the storage will take care of loading the file from the actual physical disk.
In the cloud era, this approach has obviously gained a lot of popularity. In 2006, Amazon Web Services (AWS) launched Amazon S3, its own implementation of object storage. It gave developers access to virtually unlimited disk space to store files using a simple API, all at a very cheap price. Amazon S3 gained so much popularity its API became the de facto standard in the industry. Nowadays, most cloud object storage, including storage from competitors such as Microsoft Azure or Google Cloud, is compatible with the S3 API. Open source implementations have also emerged, such as MinIO. The main benefit of this common S3 API is that you can use the same code and libraries in your project to talk with any object storage provider and easily switch if needed.
To sum up, object storage is a very convenient way to store and serve files at scale, no matter the number of processes that need to access this data. At the end of this section, the global architecture of our project will look like the one shown in Figure 14.5.
Storing results in a database and object storage
Figure 14.5 – Web-queue-worker architecture and object storage
It’s worth noting that the object storage will serve the file directly to the user. There won’t be an endpoint where the server would act as a proxy by downloading the file from the object storage before sending it to the user. There isn’t much benefit in doing it that way, even in terms of authentication. We’ll see that S3-compatible storage has built-in mechanisms to protect files from unauthorized access.
Implementing an object storage helper
Let’s get to the code then! We’ll use the MinIO client for Python, a library to interact with any S3-compatible storage. Let’s install it:
(venv) $ pip install minio
We can now implement a class to have all the operations we need at hand. Let’s first go with the initializer:
storage.py
class Storage: def __init__(self) -> None: self.client = Minio( settings.storage_endpoint, access_key=settings.storage_access_key,
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
secret_key=settings.storage_secret_key, )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
In the initializer of this class, we create a Minio client instance. You’ll see that we use a settings object to pull the storage URL and credentials. Thus, it’s very easy to switch them by using environment variables.
We’ll then implement several methods that’ll help us work with object storage. The first one is ensure_ bucket:
storage.py
def ensure_bucket(self, bucket_name: str): bucket_exists = self.client.bucket_exists(bucket_name) if not bucket_exists: self.client.make_bucket(bucket_name)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
The role of this method is to make sure the right bucket is created in our object storage. In S3 implementations, a bucket is like a folder that you own and in which you can store your files. Each file you upload has to be put into an existing bucket.
Then, we define upload_image:
storage.py
def upload_image(self, image: Image, object_name: str, bucket_ name: str): self.ensure_bucket(bucket_name)
image_data = io.BytesIO() image.save(image_data, format="PNG") image_data.seek(0) image_data_length = len(image_data.getvalue())
self.client.put_object( bucket_name, object_name, image_data,
Storing results in a database and object storage
length=image_data_length, content_type="image/png", )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
This is for uploading an image to the storage. To simplify things, this method accepts a Pillow Image, as it’s the result we get at the end of the Stable Diffusion pipeline. We implemented some logic to convert this Image object into a raw stream of bytes suitable for the S3 upload. This method also expects object_name, which will be the actual name of the file in the storage, along with bucket_name. Notice that we first ensure the bucket is correctly created before trying to upload the file.
Finally, we add the get_presigned_url method:
storage.py
def get_presigned_url( self, object_name: str, bucket_name: str, *, expires: timedelta = timedelta(days=7) ) str: return self.client.presigned_get_object( bucket_name, object_name, expires=expires )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
This method will help us to serve the file securely to the user. By default, for security reasons, files in S3 storage are not accessible by any user on the internet. To give access to a file, we can do either of the following:
Set the file as public so anybody with the URL can access it. This is suitable for public files but certainly not for private user files.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Generate a URL with a temporary access key. Thus, we can give access to the file to the user, knowing that even if the URL is stolen, the access will be revoked after a certain time. The huge benefit of this is that this URL generation happens on our API server using the S3 client. Therefore, we could check whether the user is correctly authenticated and has the rights to this specific file following our own logic before generating the file URL. This is the approach we adopt here, and this method generates the pre-signed URL on a specific file in a specific bucket for a certain amount of time.
As you can see, our class is just a thin wrapper around the MinIO client. All we have to do now is to use it to upload the images and get a pre-signed URL from the API.
Using the object storage helper in the worker
In the previous section, we showed the following lines in our task implementation:
worker.py
storage = Storage() storage.upload_image(image_output, file_name, settings.storage_ bucket)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
Now that we’ve talked about the Storage class, you should guess what we’re doing here: we take the generated image and its random name and upload it to a bucket defined in settings. And… That’s it!
Generating a pre-signed URL on the server
On the API’s side, we implement a new endpoint whose role is to return a pre-signed URL for a given GeneratedImage:
server.py
@app.get("/generated-images/{id}/url") async def get_generated_image_url( image: GeneratedImage = Depends(get_generated_image_or_404), storage: Storage = Depends(get_storage), ) schemas.GeneratedImageURL: if image.file_name is None: raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail="Image is not available yet. Please try again later.",
Storing results in a database and object storage
)
url = storage.get_presigned_url(image.file_name, settings.storage_ bucket) return schemas.GeneratedImageURL(url=url)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ server.py
Before generating the URL, we first check whether the file_name property is set on the GeneratedImage object. If it’s not, it means the worker has not completed the task yet. If it is, we can proceed with the call to the get_presigned_url method of our Storage class.
Notice that we took care of defining a dependency injection to get our Storage instance. As we’ve seen throughout this book, using dependencies in FastAPI is a very good practice when dealing with external services.
Well, it seems that we’re all set! Let’s see it in action.
Running the image-generation system
First of all, we need to populate the environment variables for our project with, in particular, a database URL and S3 credentials. To keep things simple, we’ll use a simple SQLite database and the MinIO playground for the S3 storage. It’s a free and open instance of MinIO object storage that’s perfect for examples and toy projects. When going into production, you’ll be able to easily switch to any S3-compatible provider. Let’s create a .env file at the root of the project:
DATABASE_URL=sqlite+aiosqlite:///chapter14.db STORAGE_ENDPOINT=play.min.io STORAGE_ACCESS_KEY=Q3AM3UQ867SPQQA43P2F STORAGE_SECRET_KEY=zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG STORAGE_BUCKET=fastapi-book-text-to-image
The storage endpoint, access key, and secret key are the parameters for the MinIO playground. Make sure to check their official documentation to see whether they have changed since we wrote this book: https://min.io/docs/minio/linux/developers/python/minio-py. html#id5.
Our Settings class will automatically load this file to populate the settings we use throughout the code. Make sure to check the Setting and using environment variables section of Chapter 10 if you need a refresher on this concept.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We can now run our system. Make sure your Redis server is still running, as explained in the Technical requirements section. First of all, let’s run the FastAPI server:
(venv) $ uvicorn chapter14.complete.api:app
Then, start the worker:
(venv) $ dramatiq -p 1 -t 1 chapter14.complete.worker
The stack is now ready to generate images. Let’s make a request with HTTPie to start a new task:
$ http POST http://localhost:8000/generated-images prompt="a sunset over a beach" HTTP/1.1 201 Created content-length: 151 content-type: application/json date: Mon, 13 Feb 2023 07:24:44 GMT server: uvicorn
{ "created_at": "2023-02-13T08:24:45.954240", "file_name": null, "id": 1, "negative_prompt": null, "num_steps": 50, "progress": 0, "prompt": "a sunset over a beach" }
A new GeneratedImage has been created in the database with the assigned ID 1. The progress is at 0%; the processing has not started yet. Let’s try to query it with our API:
http GET http://localhost:8000/generated-images/1 HTTP/1.1 200 OK content-length: 152 content-type: application/json date: Mon, 13 Feb 2023 07:25:04 GMT server: uvicorn
{ "created_at": "2023-02-13T08:24:45.954240", "file_name": null, "id": 1, "negative_prompt": null, "num_steps": 50,
Storing results in a database and object storage
"progress": 36, "prompt": "a sunset over a beach" }
The API returns the same object with all its properties. Notice that the progress has been updated and that it’s now at 36%. After a while, we can try the same request again:
$ http GET http://localhost:8000/generated-images/1 HTTP/1.1 200 OK content-length: 191 content-type: application/json date: Mon, 13 Feb 2023 07:25:34 GMT server: uvicorn
{ "created_at": "2023-02-13T08:24:45.954240", "file_name": "affeec65-5d9b-480e-ac08-000c74e22dc9.png", "id": 1, "negative_prompt": null, "num_steps": 50, "progress": 100, "prompt": "a sunset over a beach" }
This time, the progress is at 100% and the filename has been filled. The image is ready! We can now ask our API to generate a pre-signed URL for this image:
$ http GET http://localhost:8000/generated-images/1/url HTTP/1.1 200 OK content-length: 366 content-type: application/json date: Mon, 13 Feb 2023 07:29:53 GMT server: uvicorn
{ "url": "https://play.min.io/fastapi-book-text-to-image/ affeec65-5d9b-480e-ac08-000c74e22dc9.png?X-Amz-Algorithm=AWS4- HMAC-SHA256&X-Amz-Credential=Q3AM3UQ867SPQQA43P2F%2F20230213%2 Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230213T072954Z&X-Amz- Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=6ffddb81702bed 6aac50786578eb75af3c1f6a3db28e4990467c973cb3b457a9" }
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We get a very long URL on the MinIO server. If you open it in your browser, you’ll see the image that has just been generated by our system, as you can see in Figure 14.6.
Figure 14.6 – Generated image hosted on object storage
Quite nice, isn’t it? We now have a fully featured system where the user is able to do the following:
Request to generate images following their own prompt and parameters
Get information about the progress of the request
Get the resulting image from reliable storage
The architecture we see here is already deployable in a cloud environment with multiple machines. Typically, we may have a standard, cheap server to serve the API and a more expensive one with a dedicated GPU and a good amount of RAM to run the worker. The code doesn’t have to change to handle this kind of deployment since the communication between processes is handled by the central elements – the message broker, the database, and the object storage.
Summary
Awesome! You may not have realized it yet, but in this chapter, you learned how to architect and implement a very complex machine learning system that could rival existing image-generation services you see out there. The concepts we showed here are essential and are at the heart of all the distributed systems you could imagine, whether they are designed to run machine learning models, extraction pipelines, or math computations. By using modern tools such as FastAPI and Dramatiq, you’ll be able to implement this kind of architecture in a short time with a minimum amount of code, leading to a very quick and robust result.
We’re near the end of our journey. Before letting you live your own adventures with FastAPI, we’ll study one last important aspect when building data science applications: logging and monitoring.
15 Monitoring the Health and Performance of a Data Science System
In this chapter, we will cover the extra mile so you are able to build robust, production-ready systems. One of the most important aspects to achieve this is to have all the data we need to ensure the system is operating correctly and detect as soon as possible when something goes wrong so we can take corrective actions. In this chapter, we’ll see how to set up a proper logging facility and how we can monitor the performance and health of our software in real time.
We’re near the end of our journey into FastAPI for data science. Until now, we’ve mainly focused on the functionality of the programs we implemented. However, there is another aspect that is often overlooked by developers but is actually very important: assessing whether the system is functioning correctly and reliably in production and being warned as soon as possible when that’s not the case.
For this, lot of tools and techniques exist so we can gather the maximum amount of data about how our program is performing. That’s what we’ll review in this chapter.
We’re going to cover the following main topics:
Configuring and using a logging facility with Loguru
Configuring Prometheus metrics and monitoring them in Grafana
Configuring Sentry for reporting errors
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
Monitoring the Health and Performance of a Data Science System
To run a Dramatiq worker, you’ll need a running Redis server on your local computer. The easiest way is to run it as a Docker container. If you’ve never used Docker before, we recommend you read the Getting started tutorial in the official documentation at https://docs.docker.com/ get-started/. Once done, you’ll be able to run a Redis server with this simple command:
$ docker run -d --name worker-redis -p 6379:6379 redis
You’ll find all the code examples of this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter15.
A note about the screenshots In the course of this chapter, we’ll present several screenshots, in particular of the Grafana interface. Their goal is to show you the general layout of the UI to help you identify its different parts. Don’t worry if you struggle to read the actual content: the explanations around them will explain where to look at and what to interact with.
Configuring and using a logging facility with Loguru
In software development, logs are probably the simplest but most powerful way to control the behavior of a system. They usually consist of lines of plain text that are printed at specific points of a program. By reading them chronologically, we are able to trace the behavior of the program and check that everything goes well. Actually, we’ve already seen log lines in this book. When you run a FastAPI app with Uvicorn and make some requests, you’ll see these lines in the console output:
INFO: Started server process [94918] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: 127.0.0.1:60736 - "POST /generated-images HTTP/1.1" 201 Created
Those are the logs generated by Uvicorn, which tell us when it has started and when it has handled a request. As you can see, logs can help us to know what happened in our program and what actions it performed. They can also tell us when something goes wrong, which could be a bug that needs to be solved.
Configuring and using a logging facility with Loguru
Understanding log levels
Notice that before each log line, we have the INFO keyword. This is what we call the log level. It’s a way to classify the importance of this log. In general, the following levels are defined:
DEBUG
INFO
WARNING
ERROR
You can consider this the level of importance: DEBUG is really specific information about what the program does, which could help you to debug the code, while ERROR means that something bad happened in your program, which probably requires action on your part. The good thing about those levels is that we can configure the minimum level that should be output by the logger. The actual call to the log function is still there in the code, but it’s ignored by the logger if it doesn’t match the minimum level.
Typically, we can set the DEBUG level in local development so we have all the information to help us develop and fix our program. On the other hand, we can set the level to INFO or WARNING in production so we have only the most important messages.
Adding logs with Loguru
Adding your own logs to a Python program can be fairly easy using the logging module available in the standard library. You could do something like this:
>>> import logging >>> logging.warning("This is my log") WARNING:root:This is my log
As you can see, it’s just a function call with a string in the argument. Typically, logging modules expose the different levels as methods, as you see here with warning.
The standard logging module is really powerful and allows you to finely customize how your logs are handled, printed, and formatted. If you go through the logging tutorials in the official documentation, https://docs.python.org/3/howto/logging.html, you’ll see it can quickly become really complex, even for simple cases.
That’s why Python developers usually use libraries wrapping the logging module and exposing much more friendly functions and interfaces. In this chapter, we’ll review how to use and configure Loguru, a modern yet simple approach to logging.
Monitoring the Health and Performance of a Data Science System
As always, the first thing to do is to install it in our Python environment:
(venv) $ pip install loguru
We can try it right away in a Python shell:
>>> from loguru import logger >>> logger.debug("This is my log!") 2023-02-21 08:44:00.168 | DEBUG | __main__:<module>:1 - This is my log!
You may think that’s not very different from what we did with the standard logging module. However, notice the resulting log already includes the timestamp, the level, and the position of the function call in the code. That’s one of the main benefits of Loguru: it comes with sensible defaults working out of the box.
Let’s see it in action in a more complete script. We’ll define a simple function to check whether an integer, n, is odd or not. We’ll add a debug line to let us know the function starts its logic. Then, before computing the result, we’ll first check whether n truly is an integer and log an error if not. The implementation of this function looks like this:
chapter15_logs_01.py
from loguru import logger
def is_even(n) -> bool: logger.debug("Check if {n} is even", n=n) if not isinstance(n, int): logger.error("{n} is not an integer", n=n) raise TypeError() return n % 2 == 0
if __name__ == "__main__": is_even(2) is_even("hello")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_01.py
Configuring and using a logging facility with Loguru
As you can see, it’s really simple to use: we just have to import logger and call it wherever we need to log something. Notice also how we can add variables to format our string: we just need to add a placeholder around curly braces inside the string and then map each placeholder to its value with keyword arguments. This syntax is actually similar to the standard str.format method. You can read more about it in the official Python documentation: https://docs.python.org/fr/3/ library/stdtypes.html#str.format.
If we run this simple script, we’ll see our log lines in the console output:
(venv) $ python chapter15/chapter15_logs_01.py 2023-03-03 08:16:40.145 | DEBUG | __main__:is_even:5 - Check if 2 is even 2023-03-03 08:16:40.145 | DEBUG | __main__:is_even:5 - Check if hello is even 2023-03-03 08:16:40.145 | ERROR | __main__:is_even:7 - hello is not an integer Traceback (most recent call last): File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_01.py", line 14, in <module> is_even("hello") File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_01.py", line 8, in is_even raise TypeError() TypeError
Our log lines are correctly added to the output before the actual exception is raised. Notice how Loguru is able to precisely tell us where the log call comes from in the code: we have the function’s name and line.
Understanding and configuring sinks
We’ve seen that, by default, logs are added to the console output. By default, Loguru defines a sink targeted at a standard error. A sink is a concept introduced by Loguru to define how log lines should be handled by the logger. We’re not limited to console output: we can also save them to a file, or a database, or even send them to a web service!
The good thing is that you’re not limited to only one sink; you can have as many as you need! Then, each log call will be processed through each sink accordingly. You can see a schematic representation of this approach in Figure 15.1.
Monitoring the Health and Performance of a Data Science System
Figure 15.1 – Schema of Loguru sinks
Each sink is associated with a log level. This means that we could have different log levels depending on the sink. For example, we could choose to output all logs to a file and keep only the most important warning and error logs in the console. Let’s again take our previous example and configure Loguru with this approach:
chapter15_logs_02.py
logger.remove() logger.add(sys.stdout, level="WARNING") logger.add("file.log", level="DEBUG", rotation="1 day")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_02.py
The remove method of logger is helpful for removing a previously defined sink. When calling it like this with no parameter, all the defined sinks are removed. By doing this, we start fresh without the default sink.
Then, we call add to define new sinks. The first parameter, like sys.stdout or file.log here, defines how the log calls should be handled. This parameter can be many things, such as a callable function, but Loguru allows us, for convenience, to directly pass file-like objects, such as sys.stdout, or strings, which will be interpreted as filenames. Several arguments are accepted to customize all the aspects of the sink and, in particular, the level.
As we said, the standard output sink will only log messages with at least a WARNING level, while the file sink will log all messages.
Configuring and using a logging facility with Loguru
Notice also that we added a rotation parameter for the file sink. Since logs will continuously be appended to a file, it can quickly grow in size during the lifetime of your application. That’s why we have access to a couple of options:
“Rotate” the file: This means that the current file will be renamed, and new logs will be added to a new file. This operation can be configured so it happens after a certain amount of time (for example, every day, as in our example) or when it reaches a certain size.
Remove older files: After a certain amount of time, it’s probably not very useful to keep older logs that take up unnecessary space on your disk.
You can read all the details about these features in the official documentation for Loguru: https:// loguru.readthedocs.io/en/stable/api/logger.html#file.
Now, if we run this example, we’ll see this in the console output:
(venv) $ python chapter15/chapter15_logs_02.py 2023-03-03 08:15:16.804 | ERROR | __main__:is_even:12 - hello is not an integer Traceback (most recent call last): File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_02.py", line 19, in <module> is_even("hello") File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_02.py", line 13, in is_even raise TypeError() TypeError
The DEBUG logs don’t appear anymore. However, if we read the file.log file, we’ll have both:
$ cat file.log 2023-03-03 08:15:16.803 | DEBUG | __main__:is_even:10 - Check if 2 is even 2023-03-03 08:15:16.804 | DEBUG | __main__:is_even:10 - Check if hello is even 2023-03-03 08:15:16.804 | ERROR | __main__:is_even:12 - hello is not an integer
That’s it! Sinks are really useful for routing our logs to different places depending on their nature or importance.
Structuring logs and adding context
In their simplest form, logs consist of free-form text. While convenient, we’ve seen that we usually need to log variable values to better understand what’s going on. With only strings, this usually ends up in a messy string consisting of multiple concatenated values.
Monitoring the Health and Performance of a Data Science System
A better approach to handle this is to adopt structured logging. The goal is to have a clear and proper structure for each log line, so we can embed all the information we need without sacrificing readability. Loguru supports this approach natively, thanks to contexts. The next example shows you how to use it:
chapter15_logs_03.py
def is_even(n) -> bool: logger_context = logger.bind(n=n) logger_context.debug("Check if even") if not isinstance(n, int): logger_context.error("Not an integer") raise TypeError() return n % 2 == 0
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_03.py
We once again took the same example as before. As you can see, we use the bind method of logger to retain extra information. Here, we set the n variable. This method returns a new instance of our logger with those attributes attached. Then, we can use this instance normally to log things. We don’t need to add n in the formatted string anymore.
However, if you try this example directly, you won’t see the value of n in the logs. That’s normal: by default, Loguru doesn’t add context information to the formatted log line. We need to customize it! Let’s see how:
chapter15_logs_04.py
logger.add( sys.stdout, level="DEBUG", format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | " "<level>{level: <8}</level> | " "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>" " - {extra}", )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_04.py
Configuring and using a logging facility with Loguru
To format log output, we have to use the format parameter when configuring a sink. It expects a template string. Here, we copied and pasted the default Loguru format and added a part with the extra variable. extra is a dictionary where Loguru stores all the values you added in context. Here, we just output it directly so we can see all variables.
Format syntax and available variables You can find all the available variables you can output in the format string, such as extra or level, in the Loguru documentation: https://loguru.readthedocs.io/en/ stable/api/logger.html#record.
The format string supports standard formatting directives, which are useful for retrieving values, format numbers, pad strings, and so on. You can read more about it in the Python documentation: https://docs.python.org/3/library/string.html#format- string-syntax.
Also, Loguru adds special markup so you can color the output. You can read more about it here: https://loguru.readthedocs.io/en/stable/api/logger.html#color.
This time, if you run this example, you’ll see the extra context added to the log lines:
(venv) $ python chapter15/chapter15_logs_04.py 2023-03-03 08:30:10.905 | DEBUG | __main__:is_even:18 - Check if even - {'n': 2} 2023-03-03 08:30:10.905 | DEBUG | __main__:is_even:18 - Check if even - {'n': 'hello'} 2023-03-03 08:30:10.905 | ERROR | __main__:is_even:20 - Not an integer - {'n': 'hello'}
This approach is very convenient and powerful: if you want to keep track of a value you care about across logs, you just have to add it once.
Logs as JSON objects Another approach to structured logging is to serialize all the data of a log into a JSON object. This can be enabled easily with Loguru by setting serialize=True when configuring the sink. This approach can be interesting if you plan to use a log ingestion service such as Logstash or Datadog: they will be able to parse the JSON data and make it available for querying.
You now have the basics of adding and configuring logs with Loguru. Let’s now see how we can leverage them in a FastAPI application.
Monitoring the Health and Performance of a Data Science System
Configuring Loguru as the central logger
Adding logs to your FastAPI application can be really useful to know what’s happening in your different routes and dependencies.
Let’s take an example from Chapter 5, where we added a global dependency to check for a secret value that should be set in the header. In this new version, we’ll add a debug log to trace when the secret_ header dependency is called and a warning log to inform us when this secret is missing or invalid:
chapter15_logs_05.py
from loguru import logger
def secret_header(secret_header: str | None = Header(None)) -> None: logger.debug("Check secret header") if not secret_header or secret_header != "SECRET_VALUE": logger.warning("Invalid or missing secret header") raise HTTPException(status.HTTP_403_FORBIDDEN)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_05.py
That’s nothing really surprising if you have followed us so far! Now, let’s run this application with Uvicorn and make a request with an invalid header:
INFO: Started server process [47073] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) 2023-03-03 09:00:47.324 | DEBUG | chapter15.chapter15_ logs_05:secret_header:6 - Check secret header 2023-03-03 09:00:47.324 | WARNING | chapter15.chapter15_ logs_05:secret_header:8 - Invalid or missing secret header INFO: 127.0.0.1:58190 - "GET /route1 HTTP/1.1" 403 Forbidden
Our own logs are here, but there is a problem: Uvicorn also adds its own logs, but it doesn’t follow our format! Actually, that’s expected: other libraries, such as Uvicorn, may have their own logs with their own settings. As such, they won’t follow what we defined with Loguru. It’s a bit annoying because if we have a complex, well-thought-out setup, we would like every log to follow it. Fortunately, there are ways to configure this.
Configuring and using a logging facility with Loguru
First of all, we’ll create a module named logger.py, where we’ll put all our logger configurations. It’s a good practice in your project to have this module so your configuration is centralized in one place. The first thing we do in this file is to configure Loguru:
logger.py
LOG_LEVEL = "DEBUG"
logger.remove() logger.add( sys.stdout, level=LOG_LEVEL, format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | " "<level>{level: <8}</level> | " "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>" " - {extra}", )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
As we did in the previous section, we removed the default handler and defined our own. Notice that we set the level thanks to a constant named LOG_LEVEL. We hardcoded it here, but a better way would be to take the value from a Settings object, as we showed in Chapter 10. This way, we could directly set the level from environment variables!
After that, we have a quite complex piece of code in the class named InterceptHandler. It’s a custom handler for the standard logging module that will forward every standard log call to Loguru. This code is directly taken from the Loguru documentation. We won’t go into much detail about its functioning but just know that it’ll retrieve the log level and go through the call stack to retrieve the original caller and forward this information to Loguru.
The most important part, however, is how we use this class. Let’s see this here:
logger.py
logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)
for uvicorn_logger_name in ["uvicorn.error", "uvicorn.access"]: uvicorn_logger = logging.getLogger(uvicorn_logger_name)
Monitoring the Health and Performance of a Data Science System
uvicorn_logger.propagate = False uvicorn_logger.handlers = [InterceptHandler()]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
The trick here is to call the basicConfig method from the standard logging module to set our custom interception handler. This way, every log call made with the root logger, even ones from external libraries, will go through it and be handled by Loguru.
In some cases, however, this configuration is not sufficient. Some libraries define their own loggers with their own handlers, so they won’t use the root configuration. That’s the case for Uvicorn, which defines two main loggers: uvicorn.error and uvicorn.access. By retrieving those loggers and changing their handler, we force them to go through Loguru as well.
If you use other libraries that define their own loggers like Uvicorn does, you’ll probably need to apply the same technique. All you need to determine is the name of their logger, which should be quite easy to find in the library’s source code.
It works out of the box with Dramatiq If you implement a worker with Dramatiq, as we showed in Chapter 14, you’ll see that, if you use the logger module, the default logs of Dramatiq will be correctly handled by Loguru.
Finally, we take care of setting the __all__ variable at the end of the module:
logger.py
__all__ = ["logger"]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
__all__ is a special variable telling Python which variables should be made publicly available when importing this module. Here, we’ll expose logger from Loguru, so we can easily import it everywhere we need in our project.
Bear in mind that it’s not strictly necessary to use __all__: we could very well import logger without it, but it’s a clean way to hide other things we want to keep private, such as InterceptHandler, for example.
Configuring and using a logging facility with Loguru
Finally, we can use it as we saw previously in our code:
logger.py
from chapter15.logger import logger
def secret_header(secret_header: str | None = Header(None)) None: logger.debug("Check secret header") if not secret_header or secret_header != "SECRET_VALUE": logger.warning("Invalid or missing secret header") raise HTTPException(status.HTTP_403_FORBIDDEN)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
If we run it with Uvicorn, you’ll now see that all our logs are formatted the same way:
2023-03-03 09:06:16.196 | INFO | uvicorn.server:serve:75 - Started server process [47534] - {} 2023-03-03 09:06:16.196 | INFO | uvicorn.lifespan.on:startup:47 - Waiting for application startup. - {} 2023-03-03 09:06:16.196 | INFO | uvicorn.lifespan.on:startup:61 - Application startup complete. - {} 2023-03-03 09:06:16.196 | INFO | uvicorn.server:_log_started_ message:209 - Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) - {} 2023-03-03 09:06:18.500 | DEBUG | chapter15.chapter15_ logs_06:secret_header:7 - Check secret header - {} 2023-03-03 09:06:18.500 | WARNING | chapter15.chapter15_ logs_06:secret_header:9 - Invalid or missing secret header - {} 2023-03-03 09:06:18.500 | INFO | uvicorn.protocols.http.httptools_ impl:send:489 - 127.0.0.1:59542 - "GET /route1 HTTP/1.1" 403 - {}
Great! Now, whenever you need to add logs in your app, all you need to do is to import logger from your logger module.
You now have the basics to add logs to your application, with plenty of options to fine-tune how and where you output them. Logs are very useful for monitoring what your application is doing at a micro-level, operation per operation. Another important aspect of monitoring is to have information at a more general level in order to have big figures and quickly detect if something goes wrong. That’s what we’ll see now with metrics.
Monitoring the Health and Performance of a Data Science System
Adding Prometheus metrics
In the previous section, we saw how logs can help us understand what our program is doing by finely tracing the operations it does over time. However, most of the time, you can’t afford to keep an eye on the logs all day: they are useful for understanding and debugging a particular situation but way less useful for getting global insights to alert you when something goes wrong.
To solve this, we’ll see in this section how to add metrics to our application. Their role is to measure things that matter in the execution of our program: the number of requests made, the time taken to give a response, the number of pending tasks in the worker queue, the accuracy of our ML predictions… Anything that we could easily monitor over time – usually, with charts and graphs – so we can easily monitor the health of our system. We say that we instrument our application.
To achieve this task, we’ll use two widely used technologies in the industry: Prometheus and Grafana.
Understanding Prometheus and the different metrics
Prometheus is a technology to help you instrument your application. It consists of three things:
Libraries for a wide range of programming languages, including Python, to add metrics to an application
A server to aggregate and store those metrics over time
A query language, PromQL, so we can pull data from those metrics into visualization tools
Prometheus has very precise guidelines and conventions about how to define metrics. Actually, it defines four different types of metrics.
The counter metric
The counter metric is a way to measure a value that goes up over time. For example, this could be the number of requests answered or the number of predictions done. This will not be used for values that can go down. For that, there is the gauge metric.
Figure 15.2 – Possible representation of a counter
Adding Prometheus metrics
The gauge metric
The gauge metric is a way to measure a value that can go up or down over time. For example, this could be the current memory usage or the number of pending tasks in a worker queue.
Figure 15.3 – Possible representation of a gauge
The histogram metric
Contrary to counters and gauges, a histogram will measure values and count them in buckets. Typically, if we want to measure the response time of our API, we can count the number of requests that have been processed in less than 10 milliseconds, less than 100 milliseconds, and less than 1 second. Doing this is much more insightful than getting a simple average or median, for example.
When using a histogram, it’s our responsibility to define the buckets we want with their value threshold.
Figure 15.4 – Possible representation of a histogram
Monitoring the Health and Performance of a Data Science System
Prometheus defines a fourth type of metric, a summary. It’s quite similar to the histogram metric, but it works with sliding quantiles instead of defined buckets. We won’t go through it since it has quite limited support in Python. Besides, we’ll see in the Grafana section of this chapter that we’ll be able to compute quantiles with the histogram metric.
You can read more details about those metrics in the official Prometheus documentation:
https://prometheus.io/docs/concepts/metric_types/
Measuring and exposing metrics
Once the metrics have been defined, we can start to measure things during the lifetime of our program. Similar to what we do with logs, metrics expose methods so we can store values during the execution of the application. Prometheus will then retain those values in memory to build the metrics.
But then, how can we access those metrics so we can actually analyze and monitor them? Quite simply, apps using Prometheus usually expose an HTTP endpoint called /metrics, which will return the current values of all metrics in a specific format. You can see what it looks like in Figure 15.5.
Figure 15.5 – Output of a Prometheus metrics endpoint
This endpoint can then be polled at regular intervals by a Prometheus server, which will store those metrics over time and make them available through PromQL.
Adding Prometheus metrics
Metrics are reset when your application restarts It’s worth noting that every time you restart your application, like your FastAPI server, metric values are lost, and you start from zero. It may be a bit surprising, but it’s key to understand that metric values are only stored in memory in your app. The responsibility for properly storing them permanently belongs to the Prometheus server.
Now that we have a good idea of how they work, let’s see how to add metrics to FastAPI and Dramatiq applications.
Adding Prometheus metrics to FastAPI
As we said, Prometheus maintains official libraries for various languages, including Python.
We could very well use it on its own and manually define various metrics to monitor our FastAPI app. We would also need to come up with some logic to hook into a FastAPI request handler so we could measure things such as the requests count, response time, payload size, and so on.
While definitely doable, we’ll take a shortcut and rely once again on the open source community, which proposes a ready-to-use library for integrating Prometheus into a FastAPI project: Prometheus FastAPI Instrumentator. It comes with useful metrics by default, such as the total number of requests or the response size in bytes. It also takes care of exposing the /metrics endpoint.
The first thing is, of course, to install it with pip. Run the following command:
(venv) $ pip install prometheus_fastapi_instrumentator
In the following example, we’ve implemented a very simple FastAPI app and enabled the instrumentator:
chapter15_metrics_01.py
from fastapi import FastAPI from prometheus_fastapi_instrumentator import Instrumentator, metrics
app = FastAPI()
@app.get("/") async def hello(): return {"hello": "world"}
instrumentator = Instrumentator() instrumentator.add(metrics.default()) instrumentator.instrument(app).expose(app)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_01.py
Monitoring the Health and Performance of a Data Science System
Enabling the instrumentator consists of three lines:
1.
Instantiate the Instrumentator class.
2. Enable the default metrics proposed by the library.
3. Wire it to our FastAPI app and expose the /metrics endpoint.
That’s it! FastAPI is instrumented with Prometheus!
Let’s run this app with Uvicorn and access the hello endpoint. Internally, Prometheus will measure things about this request. Let’s now access /metrics to see the result. If you scroll down this big list of metrics, you should come across these lines:
# HELP http_requests_total Total number of requests by method, status and handler. # TYPE http_requests_total counter http_requests_total{handler="/",method="GET",status="2xx"} 1.0
This is the metrics counting the number of requests. We see that we have one request in total, which corresponds to our call to hello. Notice that the instrumentator is smart enough to label the metrics by path, method, and even status code. This is very convenient, as it’ll enable us to pull interesting figures depending on the characteristics of the request.
Adding custom metrics
The built-in metrics are a good start, but we’ll likely need to come up with our own to measure things specific to our application.
Let’s say we want to implement a function that rolls a dice with six faces and exposes it via a REST API. We want to define a metric allowing us to count the number of times each face has appeared. For this task, a counter is a good match. Let’s see how to declare it in the code:
chapter15_metrics_02.py
DICE_COUNTER = Counter( "app_dice_rolls_total", "Total number of dice rolls labelled per face", labelnames=["face"], )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_02.py
Adding Prometheus metrics
We have to instantiate a Counter object. The two first arguments are, respectively, the name and description of the metric. The name will be used by Prometheus to uniquely identify this metric. Since we want to count the rolls per face, we also add a single label named face. Every time we count a roll of the dice, we’ll have to set this label to the corresponding result face.
Conventions for metric names Prometheus defines very precise conventions for naming your metrics. In particular, it should start with the domain the metrics belong to, such as http_ or app_, and should end with the unit, such as _seconds, _bytes, or _total if this is just a value count. We strongly recommend you read the Prometheus guidelines: https://prometheus.io/docs/ practices/naming/.
We can now use this metric in our code. In the following snippet, you’ll see the implementation of the roll_dice function:
chapter15_metrics_02.py
def roll_dice() -> int: result = random.randint(1, 6) DICE_COUNTER.labels(result).inc() return result
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_02.py
You can see that we directly use the metrics instance, DICE_COUNTER, and first call the labels method to set the face, and then inc to actually increment the counter.
That’s all we need to do: our metric is automatically registered in the Prometheus client and will start to be exposed by the /metrics endpoint. In Figure 15.6, you can see a possible visualization of this metric in Grafana.
Monitoring the Health and Performance of a Data Science System
Figure 15.6 – Representation of the dice roll metric in Grafana
As you can see, declaring and using a new metric is quite straightforward: we can just call it directly in the code we want to monitor.
Handling multiple processes
In Chapter 10, we mentioned in the Adding Gunicorn as a server process for deployment section that, in a production deployment, FastAPI apps are usually run with several workers. Basically, it spawns several processes of the same application and balances the incoming requests between them. This allows us to serve more requests concurrently and avoid blocks if one of the operations is blocking the process.
Do not confuse Gunicorn workers and Dramatiq workers When we talk about workers in the context of a Gunicorn deployment for FastAPI, we are referring to the fact that we are spawning multiple processes that’ll be able to serve our API requests concurrently. We are not talking about workers in the context of Dramatiq that are processing tasks in the background.
Having multiple processes for the same application is a bit problematic for Prometheus metrics. Indeed, as we mentioned before, those metrics are only stored in memory and exposed through a / metrics endpoint.
Adding Prometheus metrics
If we have several processes answering requests, each one will have its own set of metrics values. Then, when the Prometheus server asks for /metrics, we’ll get the values of the process that answered our request but not the ones of the others. And it may change in the next poll! Obviously, this will totally defeat our initial goal.
To circumvent this, the Prometheus client has a special multiprocess mode. Basically, instead of storing the values in memory, it’ll store them in files in a dedicated folder. When calling /metrics, it’ll take care of loading all the files and reconciling the values of all processes together.
Enabling this mode requires us to set the environment variable called PROMETHEUS_MULTIPROC_ DIR. It should point to a valid folder in your filesystem where the metrics files will be stored. Here is a command example of how to set this variable and start Gunicorn with four workers:
(venv) $ PROMETHEUS_MULTIPROC_DIR=./prometheus-tmp gunicorn -w 4 -k uvicorn.workers.UvicornWorker chapter15.chapter15_metrics_01:app
Of course, in a production deployment, you would set the environment variable globally on your platform, as we explained in Chapter 10.
If you try this command, you’ll see that Prometheus will start to store some .db files inside the folder, each one corresponding to a metric and a process. The side effect is that metrics won’t be cleared when restarting the process. It can lead to unexpected behaviors if you change your metrics definition or if you run a completely different application. Make sure to choose a dedicated folder for each of your apps and clean it up when you run a new version.
We are now able to precisely instrument a FastAPI app. However, we saw in the previous chapter that data science applications can be constituted of a separate worker process, where a lot of logic and intelligence is run. Thus, it’s also crucial to instrument this part of the application.
Adding Prometheus metrics to Dramatiq
In Chapter 14, we implemented a complex application with a distinct worker process that was in charge of loading and executing the Stable Diffusion model to generate images. Hence, this part of the architecture is critical and needs to be monitored to be sure everything is going well.
In this section, we’ll see how to add Prometheus metrics to a Dramatiq worker. The good news is that Dramatiq already comes with built-in metrics and exposes the /metrics endpoint by default. Really, there is nothing much to do!
Let’s take a very basic example of a Dramatiq worker with a dummy task:
chapter15_metrics_03.py
import time
import dramatiq
Monitoring the Health and Performance of a Data Science System
from dramatiq.brokers.redis import RedisBroker
redis_broker = RedisBroker(host="localhost") dramatiq.set_broker(redis_broker)
@dramatiq.actor() def addition_task(a: int, b: int): time.sleep(2) print(a + b)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_03.py
As you probably understand by now, Dramatiq is by nature a multiprocessing program: it spawns several workers to handle tasks concurrently. As such, we need to make sure Prometheus is in multiprocessing mode, as we mentioned in the Handling multiple processes section. Thus, we’ll need to set the PROMETHEUS_MULTIPROC_DIR environment variable, as we explained earlier, but also dramatiq_prom_db. Indeed, Dramatiq implements its own mechanism to enable Prometheus’s multiprocessing mode, which should work out of the box, but it turns out, in our experience, that it’s better to be explicit about it.
The following command shows you how to start our worker with PROMETHEUS_MULTIPROC_DIR and dramatiq_prom_db set:
(venv) $ PROMETHEUS_MULTIPROC_DIR=./prometheus-tmp-dramatiq dramatiq_ prom_db=./prometheus-tmp-dramatiq dramatiq chapter15.chapter15_ metrics_03
To allow you to schedule a task easily in this worker, we’ve added a small __name__ == "__main__" instruction. In another terminal, run the following command:
(venv) $ python -m chapter15.chapter15_metrics_03
It’ll schedule a task in the worker. You’ll probably see it being executed in the worker logs.
Now, try to open the following URL in your browser: http://localhost:9191/metrics. You’ll see a result similar to what we show in Figure 15.7.
Adding Prometheus metrics
Figure 15.7 – Output of a Dramatiq Prometheus metrics endpoint
We already see several metrics, including a counter for the total number of messages processed by Dramatiq, a histogram to measure the execution time of our tasks, and a gauge to measure the number of tasks currently in progress. You can review the complete list of metrics included by Dramatiq in its official documentation: https://dramatiq.io/advanced.html#prometheus-metrics.
Adding custom metrics
Of course, as for FastAPI, we would probably like to add our own metrics to the Dramatiq worker. Actually, this is very similar to what we saw in the previous section. Let’s again take the dice roll example:
chapter15_metrics_04.py
DICE_COUNTER = Counter( "worker_dice_rolls_total", "Total number of dice rolls labelled per face", labelnames=["face"], )
@dramatiq.actor() def roll_dice_task(): result = random.randint(1, 6) time.sleep(2)
Monitoring the Health and Performance of a Data Science System
DICE_COUNTER.labels(result).inc() print(result)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_04.py
All we needed to do was to create our Counter object, as we did before, and use it in our task. If you try to run the worker and request the /metrics endpoint, you’ll see this new metric appear.
We are now able to instrument our FastAPI and Dramatiq apps. As we have already mentioned several times, we now need to aggregate those metrics in a Prometheus server and visualize them in Grafana. That’s what we’ll look at in the next section.
Monitoring metrics in Grafana
Having metrics is nice, but being able to visualize them is better! In this section, we’ll see how we can collect Prometheus metrics, send them to Grafana, and create dashboards to monitor them.
Grafana is an open source web application for data visualization and analytics. It’s able to connect to various data sources, such as timeseries databases and, of course, Prometheus. Its powerful query and graph builder allows us to create detailed dashboards where we can monitor our data in real time.
Configuring Grafana to collect metrics
Since it’s open source, you can run it from your own machine or server. Detailed instructions are available in the official documentation: https://grafana.com/docs/grafana/latest/ setup-grafana/installation/. However, to speed things up and get you started quickly, we’ll rely here on Grafana Cloud, an official hosting platform. It offers a free plan, which should be enough for you to get started. You can create your account here: https://grafana.com/auth/ sign-up/create-user. Once done, you’ll be asked to create your own instance, a “Grafana Stack,” by choosing a subdomain and a data center region, as you can see in Figure 15.8. Choose a region close to your geographic location.
Monitoring metrics in Grafana
Figure 15.8 – Instance creation on Grafana Cloud
You’ll then be presented with a set of common actions to get started with Grafana. The first thing we’ll do is add Prometheus metrics. Click on Scale and centralize existing data, then Hosted Prometheus metrics. You’ll be taken to a page to configure a Prometheus metrics collection. Click on the tab named Configuration Details at the top. The page will look like the one shown in Figure 15.9.
Figure 15.9 – Hosted Prometheus metrics configuration on Grafana
Monitoring the Health and Performance of a Data Science System
You see that we have two ways to forward metrics: via Grafana Agent or via a Prometheus server.
As we mentioned earlier, a Prometheus server is responsible for collecting metrics for all our apps and storing the data in a database. It’s the standard way to do it. You can find instructions on how to install it in the official documentation: https://prometheus.io/docs/prometheus/ latest/installation/. Bear in mind, though, that it’s a dedicated application server that’ll need proper backups, as it’ll store all your metrics data.
The most straightforward way is to use Grafana Agent. It consists of a small command-line program with a single configuration file. When it runs, it’ll poll the metrics of each of your apps and send the data to Grafana Cloud. All the data is stored on Grafana Cloud, so nothing is lost, even if you stop or delete the agent. This is what we’ll use here.
Grafana shows you commands on the page to download, unzip, and execute the Grafana Agent program. Execute those commands so you have it at the root of your project.
Then, in the last step, you’ll have to create an API token so Grafana Agent can send data to your instance. Give it a name and click on Create API Token. A new text area will appear with a new command to create the agent’s configuration file, as you can see in Figure 15.10.
Figure 15.10 – Command to create Grafana Agent configuration
Monitoring metrics in Grafana
Execute the ./grafana-agent-linux-amd64 –config.file=agent-config.yaml command. A file named agent-config.yaml will be created in your project. We now have to edit it so we can configure our actual FastAPI and Dramatiq applications. You can see the result in the following snippet:
agent-config.yaml
metrics: global: scrape_interval: 60s configs: - name: hosted-prometheus scrape_configs: - job_name: app static_configs: - targets: ['localhost:8000'] - job_name: worker static_configs: - targets: ['localhost:9191'] remote_write: - url: https://prometheus-prod-01-eu-west-0.grafana.net/api/ prom/push basic_auth: username: 811873 password: __YOUR_API_TOKEN__
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/agent-con- fig.yaml
It’s a YAML configuration file where we can set the various options for Grafana Agent. The most important part is the scrape_configs key. As you can see, we can define the list of all the apps we want to gather the metrics for and specify their hostname, the “target”: localhost:8000 for the FastAPI app and localhost:9191 for the Dramatiq worker. Of course, this configuration is valid for local development, but you’ll have to adapt it with the proper hostnames of your apps in a production deployment.
We are now ready to start Grafana Agent and collect the metrics! Make sure your FastAPI and Dramatiq apps are running, and then run Grafana Agent. Depending on your system, the name of the executable will vary, but it’ll look similar to this:
$ ./grafana-agent-linux-amd64 --config.file=agent-config.yaml
Grafana Agent will start and will collect the metrics at regular intervals before sending them to Grafana. We’re now ready to plot some data!
Monitoring the Health and Performance of a Data Science System
Visualizing metrics in Grafana
Our metrics data is now sent to Grafana. We’re ready to query it and build some graphs. The first step is to create a new dashboard, a place where you’ll be able to create and organize multiple graphs. Click on the plus button at the top right and then New dashboard.
A new blank dashboard will appear, as you can see in Figure 15.11.
Figure 15.11 – Create a new dashboard in Grafana
Click on Add a new panel. The interface to build a new graph will appear. There are three main parts:
The graph preview at the top left. When starting, it’s empty.
The query builder at the bottom left. This is where we’ll query the metrics data.
The graph settings on the right. This is where we’ll choose the type of graph and finely configure its look and feel, similar to what we have in spreadsheet software.
Let’s try to create a graph for the duration of HTTP requests in our FastAPI app. In the select menu called Metric, you’ll have access to all the Prometheus metrics that have been reported by our apps. Select http_request_duration_seconds_bucket. This is the histogram metric defined by default by Prometheus FastAPI Instrumentator to measure the response time of our endpoints.
Then, click on Run queries. Under the hood, Grafana will build and execute PromQL queries to retrieve the data.
At the top right of the graph, let’s select a shorter time span, such as Last 15 minutes. Since we do not have much data yet, we’ll have a clearer view if we look at only a few minutes of data instead of hours. You should see a graph similar to the one in Figure 15.12.
Monitoring metrics in Grafana
Figure 15.12 – Basic plot of a histogram metric in Grafana
Grafana has plotted several series: for each handler (which corresponds to the endpoint pattern), we have several buckets, le. Each line roughly represents the number of times we answered “handler” in less than “le” seconds.
This is the raw representation of the metric. However, you probably see that it’s not very convenient to read and analyze. It would be better if we could look at this data another way, in terms of response time, arranged by quantiles.
Fortunately, PromQL includes some math operations so we can arrange the raw data. The part below the Metric menu allows us to add those operations. We can even see that Grafana suggests we use add histogram_quantile. If you click on this blue button, Grafana will automatically add three operations: a Rate, a Sum by le, and finally, a Histogram quantile, set by default to 0.95.
By doing this, we’ll now have a view of the evolution of our response time: 95% of the time, we answer in less than x seconds.
Monitoring the Health and Performance of a Data Science System
The default y axis unit is not very convenient. Since we know we work with seconds, let’s select this unit in the graph options. On the right, look for the Standard options part and, in the Unit menu, look for seconds (s) under the Time group. Your graph will now look like Figure 15.13.
Figure 15.13 – Quantile representation of a histogram metric in Grafana
Now it’s much more insightful: we can see that we answer nearly all our requests (95%) in under 100 milliseconds. If our server starts to slow down, we’ll immediately see an increase in our graph, which could alert us that something has gone wrong.
If we want to have other quantiles on the same graph, we can duplicate this query by clicking on the Duplicate button right above Run queries. Then, all we have to do is to select another quantile. We show the result with quantiles 0.95, 0.90, and 0.50 in Figure 15.14.
Monitoring metrics in Grafana
Figure 15.14 – Several quantiles on the same graph in Grafana
The legend can be customized Notice that the name of the series in the legend can be customized. Under the Options part of each query, you can customize it at will. You can even include dynamic values coming from the query, such as metrics labels.
Finally, we can give a name to our graph by setting Panel title, in the right column. Now that we’re happy with our graph, we can click on Apply at the top right to add it to our dashboard, as we see in Figure 15.15.
Figure 15.15 – Grafana dashboard
Monitoring the Health and Performance of a Data Science System
That’s it! We can start to monitor our application. You can resize and position each panel at will. You can set the query time span you want to look at and even enable auto-refresh so the data gets updated in real time! Don’t forget to click on the Save button to save your dashboard.
We can build a similar graph with the exact same configuration to monitor the time needed to execute tasks in Dramatiq, thanks to the metric named dramatiq_message_duration_ milliseconds_bucket. Notice that this one is expressed in milliseconds instead of seconds, so you should be careful when selecting the unit of your graph. We see here one of the benefits of the Prometheus naming convention for metrics!
Adding a bar chart graph
There are a lot of different types of graphs available in Grafana. For example, we could plot our dice roll metric in the form of a bar chart, where each bar represents the number of times a face has been seen. Let’s try it: add a new panel and select the app_dice_rolls_total metric. You’ll see something similar to what is shown in Figure 15.6.
Figure 15.16 – Default representation of a counter metric with a bar chart in Grafana
We do have a bar for each face, but there is something strange: there are bars for each point in time. That’s a key thing to understand with Prometheus metrics and PromQL: all metrics are stored as time series. This allows us to go back in time and see the evolution of the metrics over time.
Monitoring metrics in Grafana
However, for some representations, like the one shown here, it’s not really insightful. For this case, it would be better to show us the latest values for the time span we selected. We can do this by setting Type to Instant under the Options part of the metric panel. We’ll see that we now have a single graph with a single point in time, as you can see in Figure 15.17.
Figure 15.17 – Counter metric configured as Instant in Grafana
It’s better, but we can go further. Typically, we would like the x axis to show the face labels instead of the point in time. First, let’s customize the legend with a Custom label and type {{face}}. The legend will now only show the face label.
Now, we’ll transform the data so the x axis is the face label. Click on the Transform tab. You’ll see a list of functions that can be applied by Grafana to your data before visualizing it. For our case here, we’ll choose Reduce. The effect of this function is to take each series, take a specific value from it, and plot it on the x axis. By default, Grafana will take the maximum value, Max, but there are other choices, such as Last, Mean, or StdDev. In this context, they won’t make a difference since we already queried the instant value.
That’s it! Our graph now shows the number of times we’ve seen a face. This is the one we showed in Figure 15.6 earlier in the chapter.
Monitoring the Health and Performance of a Data Science System
Summary
Congratulations! You are now able to report metrics and build your own dashboards in Grafana to monitor your data science applications. Over time, don’t hesitate to add new metrics or complete your dashboards if you notice some blind spots: the goal is to be able to watch over every important part at a glance so you can quickly take corrective actions. Those metrics can also be used to drive the evolution of your work: by monitoring the performance and accuracy of your ML models, you can track the effects of your changes and see whether you are going in the right direction.
This is the end of this book and our FastAPI journey. We sincerely hope that you liked it and that you learned a lot along the way. We’ve covered many subjects, sometimes just by scratching the surface, but you should now be ready to build your own projects with FastAPI and serve smart data science algorithms. Be sure to check all the external resources we proposed along the way, as they will give you all the insights you need to master them.
In recent years, Python has gained a lot of popularity, especially in data science communities, and the FastAPI framework, even though still very young, is already a game-changer and has seen an unprecedented adoption rate. It’ll likely be at the heart of many data science systems in the coming years... And as you read this book, you’ll probably be one of the developers behind them. Cheers!
Symbols
args syntax
using 27, 28 **kwargs syntax using 27, 28
== None 22
A
access token
endpoints, securing with 189 generating 184
aggregating operations, NumPy
reference link 279
Amazon ECR
reference link 260
Amazon Elastic Container Service
reference link 261
Amazon RDS
reference link 256
Amazon Web Services (AWS) 214, 344 Any annotation
using 49
apt 4
Index
arrays
adding 278 aggregating 279 comparing 279 creating, with NumPy 272-274 manipulating 270, 271 manipulating, with NumPy 276, 277 multiplying 278
asynchronous generator 170 asynchronous I/O
working with 51-54
Asynchronous Server Gateway
Interface (ASGI) 52, 57
automatic interactive documentation 57 Azure Database for PostgreSQL
reference link 256
B
background operations 325 backpressure 315 Boolean logic
membership operators, reviewing 22, 23 performing 21 variable similarity, checking 22
brew 4
broadcasting 278
reference link 279
bucket 346 built-in types, Python 16, 17
reference link 21
conditional statements elif statement 23, 24 else statement 23, 24 for loop statement 24, 25 if statement 23, 24 while loop statement 25, 26
C
containers 256 context
caching
implementing, with Joblib 301-303 standard or async functions,
selecting between 304-306
Callable class
adding, to logs 361 context manager 151 cookies 76, 78, 178, 190 coroutines 52 CORS
type signature, using with 48, 49
camel case 35 cast function using 50 central logger
Loguru, configuring as 362-365
configuring 190 configuring, in FastAPI 191-196
counter metric 366 cross-origin HTTP requests 191 Cross-Site Request Forgery (CSRF) 191 cross-validation 269
classification problems 268 class inheritance
used, for creating model variations 110-112
models, validating with 292, 293 cryptographic hash functions 181 CSRF attacks
class methods
preventing, with double-submit
using, as dependencies 131, 132
cookies 196-200
class properties 35 cloud providers, documentation pages
protecting against 190
CSV data
references 254, 255
clustering 268 collections 144 collections.abc module 48 columns 142 computer vision 308 computer vision model
exporting 285 importing 284, 285
cURL 9 custom data validation with Pydantic
validation, applying at field level 112, 113 validation, applying at object level 113, 114 validation, applying before
using, with Hugging Face 308-312
Pydantic parsing 114, 115
concurrency
custom response 94, 95
handling, in WebSocket 209, 210
Conda 8
building 90 file, serving 93, 94
redirection, making 92 response_class argument, using 91, 92
dimensionality reduction 268 Docker 256
FastAPI application, deploying with 256
D
Dockerfile 256
writing 257, 258
DALL-E 326 dashboard 380 database access token
implementing 184, 185
Docker image building 259 deploying 260 running, locally 259
databases
selecting, factors 144, 145 testing with 235-241
document 143 document-oriented databases 143-145 double-submit cookies
dataset loading utilities, scikit-learn
implementing, to prevent CSRF
reference link 286
attacks 196-200
data, sharing between worker and API 338 API, adapting to save image-generation
tasks in database 339, 340
SQLAlchemy model, defining 338, 339 worker, adapting to read and update image- generation tasks in database 340-343
Dramatiq worker creating 333, 334 starting 335 tasks, scheduling 336
dumped model
loading 297, 298
data structures, Python
dictionary 20 lists 17, 18 sets 20, 21 tuples 18-20 decorator 56 default values 105, 106 dependencies 125
dumping 296 dynamic default values 107
E
eager loading 161 efficient prediction endpoint
implementing 298-301
404 error, raising 129 class methods, using as 131, 132 object, obtaining 128 using, in WebSocket 211-213 using, on path decorator 133, 134 using, on whole application 136 using, on whole router 134, 135 dependency injection 50, 123-125 dictionaries 20
ellipsis syntax 63 email addresses
validating, with Pydantic types 108-110
email notifications 325 endpoint
creating 56 running, locally 56 securing, with access tokens 189
Pydantic objects, converting into 115-117
environment variables
file uploads
setting 246-249 setting, with .env file 249, 250 using 246-249 estimators 286
handling 73-76
first in, first out (FIFO) strategy 315 five-fold cross-validation 269 fixtures
chaining, with pipelines 288-292
creating, for reusing test logic 226-228
event loop 51
F
FastAPI 9, 55, 56
CORS, configuring 191-196 security dependencies 178-180 WebSockets, creating with 205-208
foreign key 143 for loop statement 24 form data 72, 73 forward reference 158 f-strings 15 function dependency
creating 125 using 126, 127
FastAPI application
functions
database servers, adding 256 deploying, on serverless platform 253-255 deploying, on traditional server 261, 262
defining 26, 27 defining, with *args and **kwargs 27, 28
FastAPI application, deploying
with Docker 256
G
Dockerfile, writing 257, 258 Docker image, building 259 Docker image, deploying 260 Docker image, running locally 259, 260 prestart script, writing 258, 259
gauge metric 367 generator functions 33 generators 33, 34 generic CamelCase types
reference link 147
FastAPI, with HTTPX
testing tools, setting up 228-232
generics 45 Google Artifact Registry
features 268 Field customization, Pydantic
reference link 107 file-like interface 74 files, storing and serving in object
storage 344, 345
reference link 260
Google Cloud Platform (GCP) 214 Google Cloud Run reference link 261 Google Cloud SQL reference link 256
image-generation system, running 349-352 object storage helper, implementing 345-348 object storage helper, using in worker 348 pre-signed URL, generating
on server 348, 349
Grafana 376
bar chart graph, adding 384, 385 configuring, to collect metrics 376-379 metrics, monitoring in 376 metrics, visualizing in 380-384
Gunicorn
integrated development
adding, as server process for deployment 252, 253
environment (IDE) 7
is None 22 iterator 24
H
hashing passwords 182, 183 headers 76, 77 Heroku Postgres
reference link 256 histogram metric 367 holdout set 269 Homebrew package
URL 3
J
Joblib 296
results, caching with 301-303 trained model, persisting with 296
join query 143
K
HTTP authentication 178 HTTP errors
keyword arguments 27
raising 88-90
HTTPie command-line utility
L
installing 9-12 Hugging Face 307
computer vision model, using with 308-312 URL 308
label 268 lazy loading 161 list comprehensions 31, 32 lists 17
I
idiomatic constructions
immutable 18 mutable 18
logging module 355 reference link 355
generators 33, 34 list comprehensions 31, 32
login endpoint
implementing 186-188
image-generation system
logs 354, 359
running 349-352 image-generation task
REST API, implementing 336, 337
image processor 310 indexing, pandas
adding, with Loguru 355-357 as JSON objects 361 context, adding to 361 levels 355 Loguru 355
reference link 282
inheritance
used, for avoiding repetition 40, 41
configuring, as central logger 362-365 logs, adding with 355-357 reference link 359
M
mixins 42 models 310
machine learning (ML) 267, 268
model validation 268, 269 supervised learning 268 unsupervised learning 268
magic methods
creating 181, 182 training, with scikit-learn 285-288 validating, with cross-validation 292, 293
model validation 268, 269 model variations
__call__ method 39, 40 __eq__ method 38, 39 __gt__ method 38, 39 implementing 36 __lt__ method 38, 39 operators 39 __repr__ method 37 __str__ method 37
creating, with class inheritance 110-112
modules
using 28, 29
MongoDB
reference link 173
Motor, used for communication with
MongoDB database 166
mapped_column arguments
reference link 147
marker 225 masking 284 message brokers 214 Method Resolution Order (MRO) 43 metric names
database connection 167, 168 documents, deleting 172, 173 documents, inserting 168, 169 documents, nesting 173, 174 documents, obtaining 169-172 documents, updating 172, 173 models compatible with MongoDB,
creating 166, 167
conventions 371
multi-dimensional data
metrics 366
pandas DataFrames, using for 282-284
exposing 368 measuring 368 monitoring, in Grafana 376 visualizing, in Grafana 380-384
multiple WebSocket connections
handling 213-219 messages, broadcasting 213-219
mypy
metrics, Prometheus counter metric 366 gauge metric 367 histogram metric 367 reference link 368
Microsoft Azure Container Instances
reference link 45 used, for type checking 45 used, for type hinting 43, 45
N
reference link 261
Microsoft Azure Container Registry
reference link 260
Midjourney 326
namespace package reference link 30 negative indexing 17 NoSQL databases 142-144
NumPy
pandas Series
arrays, manipulating with 276, 277 working 271 NumPy arrays adding 278 creating 272-274 elements, accessing 274-276 multiplying 278 sub-arrays 274-276
NumPy user guide reference link 280
using, for one-dimensional data 280-282
parameterized dependency
creating 129 using 130, 131
parametrize
tests, generating with 224-226
pass statement 41 path 56 path operation function 56 path operation parameters 79
O
response model 81-83 status code 79-81 path parameters 59-61
object detection results
displaying, in browser 320-324 object-oriented programming 35
advanced validation 63-65 allowed values, limiting 62, 63
pip
class, defining 35, 36 inheritance, used for avoiding
Python packages, installing with 8
pipelines 288
repetition 40, 41
magic methods, implementing 36 multiple inheritance 42, 43
object storage 344 one-dimensional data
estimators, chaining with 288-292 preprocessors, chaining with 288-292
Pipenv 8 Poetry 8 POST endpoints
pandas Series, using for 280-282
tests, writing for 233, 234
operations, pandas reference link 284
optional fields 105, 106
Postman 9 preflight requests 194 preprocessors
chaining, with pipelines 288-292
P
package managers 4 packages
using 28-30
pandas 280 pandas DataFrames
primary key 143 private methods 36 Prometheus 366 Prometheus FastAPI Instrumentator 369 Prometheus metrics 366 Prometheus metrics, adding to Dramatiq 373-375
using, for multi-dimensional data 282-284
custom metrics, adding 375, 376
Prometheus metrics, adding to
FastAPI 369, 370
custom metrics, adding 370, 371 multiple processes, handling 372, 373 publish-subscribe (pub-sub) pattern 214 Pydantic 99 URL 59 used, for adding custom data validation 112
indentation 15, 16 key aspects 14 modules, using 29 packages, structuring 29, 30 packages, using 28 scripts, running 14
Python scripts
running 14, 15
Pydantic models 68, 217 Pydantic objects
Python virtual environment
creating 7, 8
converting, into dictionary 115-117 instance, creating from sub-class
object 117-119
Q
instance, updating partially 119, 120 working with 115
query parameters 65-67 queue 331
Pydantic types
reference link 109 used, for validating email addresses 108-110 used, for validating URLs 108-110
pyenv
R
Redis 215 URL 215
reference link 4 used, for installing Python distribution 4-6
pytest
used, for unit testing 222
Python 3.10 6 Python 3.11 6 Python dependencies managing 250-252 Python distribution
registration routes
implementing 183, 184 regression problems 268 relational databases 142-144 relationships 142 representational state transfer (REST) API 9, 59, 336
request body 67-69
multiple objects 69, 70
installing, with pyenv 4, 5, 6 Python Package Index (PyPi)
request object 78, 79 Request object from Starlette
URL 7
Python programming
reference link 79 request parameters
Boolean logic, performing 21 built-in types 16, 17 data structures 17 flow, controlling 23 functions, defining 26, 27
handling 59 path parameters 59-61 query parameters 65-67
response
customizing 79
response parameter 84 cookies, setting 85, 86 headers, setting 84, 85 status code, setting dynamically 87, 88
REST API endpoints
tests, writing for 232, 233
REST endpoint
implementing, to perform object
detection on single image 312-314
database migration system, setting up with Alembic 161-165
objects, deleting 155, 156 objects, filtering 153-155 objects, gathering 153-155 objects, inserting into database 152, 153 objects, updating 155, 156 ORM models, defining 146-148 Pydantic models, defining 148, 149 relationships, adding 157-161
routers 95
S
project, structuring 95-97
square brackets 17 Stable Diffusion 325, 326 Stable Diffusion, used for generating images from text prompts
same-origin policy 191 schemas 148 scikit-learn 285
model implementation, in Python script 327-329
Python script, executing 329-331
models, training with 285-288
security dependencies, FastAPI 178-180 serverless platform
standard field types 100-104 Starlette
URL 59
FastAPI application, deploying on 253-255
sessions 150 sets 20, 21 shell 5 singular body values 70 sinks
configuring 357-359 sized aliases, NumPy reference link 273
startup event 152 static type checkers 43 status code 79, 80 stop words 290 structured logging 360 supervised learning 268 Swagger
URL 58
snake case 27 sockets 204 SQLAlchemy Core 145 SQLAlchemy ORM model
creating 181
SQLAlchemy ORM, used for
T
tables 142 tensors 310 Term Frequency-Inverse Document Frequency (TF-IDF) 290
communication with SQL database
reference link 290
database connection 149-152
tests
URLs
and global fixtures, organizing 231 generating, with parametrize 224-226 logic, reusing by creating fixtures 226-228 writing, for POST endpoints 233, 234 writing, for REST API endpoints 232, 233 writing, for WebSocket endpoints 241-243
validating, with Pydantic types 108-110
user
retrieving 184
user agent 78 user and password
storing, securely in database 181
tokens 178 traditional server
Uvicorn 57 Uvicorn, as process manager
FastAPI application, deploying on 261, 262
reference link 253
trained model
dumping 296, 297 persisting, with Joblib 296
V
transformers library 308 tuples 18-20 two-fold cross-validation 269 two-way communication principles
virtual environments 7
W
with WebSockets 204
web-queue-worker
type annotations 43 working 44, 45
type checking
architecture 325, 331, 332
Web Server Gateway Interface (WSGI) 51 WebSocket endpoints
with mypy 45
tests, writing for 241-243
type data structures 45-47 type hinting
with mypy 43-45
type signature
using, with Callable 48, 49
WebSockets
concurrency, handling 209, 210 creating, with FastAPI 205-208 dependencies, using 211-213 implementing, to perform object detection
on stream of images 314-317
U
multiple WebSocket connections,
handling 213-219
unit testing, with pytest 222-224 test logic, reusing by creating
stream of images, sending from
browser 317-320
fixtures 226-228
two-way communication principles 204
tests, generating with parametrize 224-226 Universally Unique IDentifier (UUID) 334 unpacking syntax 20 unsupervised learning 268
whitespace indentation 15 Windows Subsystem for Linux (WSL)
reference link 3 worker 325, 331
Packtpub.com
Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.
Why subscribe?
Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals
Improve your learning with Skill Plans built especially for you
Get a free eBook or video every month
Fully searchable for easy access to vital information
Copy and paste, print, and bookmark content
Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub. com for more details.
At www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.
Other Books You May Enjoy
If you enjoyed this book, you may be interested in these other books by Packt:
Applied Geospatial Data Science with Python
David S. Jordan
ISBN: 978-1-80323-812-8
Understand the fundamentals needed to work with geospatial data
Transition from tabular to geo-enabled data in your workflows
Develop an introductory portfolio of spatial data science work using Python
Gain hands-on skills with case studies relevant to different industries
Discover best practices focusing on geospatial data to bring a positive change in your environment
Explore solving use cases, such as traveling salesperson and vehicle routing problems
Building Data Science Solutions with Anaconda
Dan Meador
ISBN: 978-1-80056-878-5
Install packages and create virtual environments using conda
Understand the landscape of open source software and assess new tools
Use scikit-learn to train and evaluate model approaches
Detect bias types in your data and what you can do to prevent it
Grow your skillset with tools such as NumPy, pandas, and Jupyter Notebooks
Solve common dataset issues, such as imbalanced and missing data
Use LIME and SHAP to interpret and explain black-box models
Packt is searching for authors like you
If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.
Share Your Thoughts
Now you’ve finished Building Data Science Applications with FastAPI, Second Edition, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.
Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.
Download a free PDF copy of this book
Thanks for purchasing this book!
Do you like to read on the go but are unable to carry your print books everywhere?
Is your eBook purchase not compatible with the device of your choice?
Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.
Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.
The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily
Follow these simple steps to get the benefits:
1. Scan the QR code or visit the link below
https://packt.link/free-ebook/9781837632749
2. Submit your proof of purchase
3. That’s it! We’ll send your free PDF and other benefits to your email directly