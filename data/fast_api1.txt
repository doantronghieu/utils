

14 Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Until now, in this book, we’ve built APIs where all the operations were computed inside the request handling. Said another way, before they could get their response, the user had to wait for the server to do everything we had defined: request validation, database queries, ML predictions, and so on. However, this behavior is not always desired or possible.
A typical example is email notifications. It happens quite often in a web application that we need to send an email to the user because they just registered or they performed a specific action. To do this, the server needs to send a request to an email server so the email can be sent. This operation could take a few milliseconds. If we do this inside the request handling, the response will be delayed until we send the email. This is not a very good experience since the user doesn’t really care how and when the email is sent. This example is typical of what we usually call background operations: things that need to be done in our application but don’t require direct user interaction.
Another case is when the user requests an expensive operation that can’t be done in a reasonable time. It’s usually the case for complex data exports or heavy AI models. In this context, the user would like to get the result directly, but doing this in the request handler would block the server process until it’s done. If lots of users were requesting this kind of operation, it would quickly make our server unresponsive. Besides, some network infrastructure such as proxy or web clients, like browsers, have quite strict timeout settings, meaning they will usually cancel an operation if it takes too much time to respond.
To solve this, we’ll introduce a typical architecture for web applications: web-queue-worker. As we’ll see in this chapter, we’ll defer the most expensive, long operations to a background process, a worker. To show you this architecture in action, we’ll build our very own AI system to generate images from text prompts using the Stable Diffusion model.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
In this chapter, we’re going to cover the following main topics:
Using the Stable Diffusion model with Hugging Face Diffusers to generate images from text prompts
Implementing a worker process using Dramatiq and an image-generation task
Storing and serving files in object storage
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
To run the Stable Diffusion model correctly, we recommend you have a recent computer equipped with at least 16 GB of RAM and, ideally, a dedicated GPU with 8 GB of VRAM. For Mac users, recent models equipped with the M1 Pro or M2 Pro chips are also a good fit. If you don’t have that kind of machine, don’t worry: we’ll show you ways to run the system anyway – the only drawback is that image generation will be slow and show poor results.
For running the worker, you’ll need a running Redis server on your local computer. The easiest way is to run it as a Docker container. If you’ve never used Docker before, we recommend you read the Getting started tutorial in the official documentation at https://docs.docker.com/get-started/. Once done, you’ll be able to run a Redis server with this simple command:
$ docker run -d --name worker-redis -p 6379:6379 redis
You’ll find all the code examples of this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter14.
Generating images from text prompts with Stable Diffusion
Recently, a new generation of AI tools has emerged and fascinated the whole world: image-generation models, such as DALL-E or Midjourney. Those models are trained on huge amounts of image data and are able to generate completely new images from a simple text prompt. These AI models are very good use cases for background workers: they take seconds or even minutes to process, and they need lots of resources in the CPU, RAM, and even the GPU.
To build our system, we’ll rely on Stable Diffusion, a very popular image-generation model that was released in 2022. This model is available publicly and can be run on a modern gaming computer. As we did in the previous chapter, we’ll rely on Hugging Face tools for both downloading the model and running it.
Generating images from text prompts with Stable Diffusion
Let’s first install the required tools:
(venv) $ pip install accelerate diffusers
We’re now ready to use diffuser models thanks to Hugging Face.
Implementing the model in a Python script
In the following example, we’ll show you the implementation of a class able to instantiate the model and run an image generation. Once again, we’ll apply our lazy loading pattern with separate load_model and generate methods. Let’s first focus on load_model:
text_to_image.py
class TextToImage: pipe: StableDiffusionPipeline | None = None
def load_model(self) -> None: # Enable CUDA GPU if torch.cuda.is_available(): device = "cuda" # Enable Apple Silicon (M1) GPU elif torch.backends.mps.is_available(): device = "mps" # Fallback to CPU else: device = "cpu"
pipe = StableDiffusionPipeline.from_pretrained("runwayml/ stable-diffusion-v1-5") pipe.to(device) self.pipe = pipe
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter14/basic/ text_to_image.py
The first part of this method aims to find the most efficient way to run the model given your computer. These diffusion models are faster when run on the GPU – that’s why we check first if there are CUDA (NVIDIA GPU) or MPS (Apple Silicon) devices available. If there are none, we fall back to the CPU.
Then, we simply have to create a StableDiffusionPipeline pipeline, as provided by Hugging Face. We simply have to set the model we want to download from the hub. For this example, we chose runwayml/stable-diffusion-v1-5. You can find its details on Hugging Face: https:// huggingface.co/runwayml/stable-diffusion-v1-5.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We can now focus on the generate method:
text_to_image.py
def generate( self, prompt: str, *, negative_prompt: str | None = None, num_steps: int = 50, callback: Callable[[int, int, torch.FloatTensor], None] | None = None, ) Image.Image: if not self.pipe: raise RuntimeError("Pipeline is not loaded") return self.pipe( prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, guidance_scale=9.0, callback=callback, ).images[0]
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter14/basic/ text_to_image.py
You can see it accepts four parameters:
prompt, which is, of course, the text prompt describing the image we want to generate.
negative_prompt, which is an optional prompt to tell the model what we absolutely don’t want.
num_steps, which is the number of inference steps the model should run. More steps lead to a better image, but each iteration delays the inference. The default, 50, should provide a good balance between speed and quality.
callback, which is an optional function that will be called at each iteration step. This is helpful to be informed about the progress of the generation and possibly execute more logic, such as saving the progress in a database.
Generating images from text prompts with Stable Diffusion
What does the asterisk (*) in the method signature mean? You may have noticed the asterisk, *, in the method signature. It tells Python that the arguments coming after this symbol should only be treated as keyword-only arguments. Said another way, you can only call them like this: .generate("PROMPT", negative_prompt="NEGATIVE", num_steps=10).
While not necessary, it’s a way to keep your functions clear and self-explanatory. It’s especially true if you develop classes or functions that are meant to be used by other developers. Another syntax also exists to force arguments to be positional-only, using a slash (/) symbol. You can read more about it here: https://docs.python.org/3/ whatsnew/3.8.html#positional-only-parameters.
All we have to do then is to pass those parameters to pipe. There are a lot more parameters for you to tune if needed, but the default ones should give you quite good results. You can find the whole list of them in the Hugging Face documentation: https://huggingface.co/ docs/diffusers/api/pipelines/stable_diffusion/text2img#diffusers. StableDiffusionPipeline.__call__. This pipe object is able to generate several images per prompt, that’s why the result of this operation is a list of Pillow images. The default here is to generate only one image, so we directly return the first one.
And that’s about it! Once again, Hugging Face makes our lives really easy by allowing us to run cutting-edge models in dozens of lines!
Executing the Python script
We bet that you’re eager to try it yourself – that’s why we added a small main script at the bottom of our example:
text_to_image.py
if __name__ == "__main__": text_to_image = TextToImage() text_to_image.load_model()
def callback(step: int, _timestep, _tensor): Step {step}") print(f"
🚀
image = text_to_image.generate( "A Renaissance castle in the Loire Valley", negative_prompt="low quality, ugly", callback=callback,
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
) image.save("output.png")
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter14/basic/ text_to_image.py
This small script instantiates our TextToImage class, loads the model, and generates an image before saving it to disk. We also define a dummy callback function so you can see how it works.
When you run this script for the first time, you’ll notice that Hugging Face downloads files of several gigabytes to your computer: that’s the Stable Diffusion model, and it’s indeed quite big!
Then, the inference will start. You’ll see a progress bar showing you how many inference steps are left, along with the print statement from our callback, as shown in Figure 14.1.
Figure 14.1 – Stable Diffusion generating an image
How much time does it take to generate a single image? We’ve run several tests on different types of computers. With a modern NVIDIA GPU with 8 GB of RAM or a Mac with an M1 Pro chip, the model is able to generate an image with 50 inference steps in around a minute, with reasonable RAM usage. When run on a CPU, it takes around 5 to 10 minutes and eats up to 16 GB of RAM.
If the inference is really too slow on your computer, you can try to reduce the num_steps parameter.
Creating a Dramatiq worker and defining an image-generation task
When the inference is done, you’ll find your generated image on the disk along with your script. Figure 14.2 shows an example of such a result. Nice, isn’t it?
Figure 14.2 – Result of a Stable Diffusion image generation
We now have the fundamental brick of our AI system. Now, we need to build an API so users can generate their own images. As we’ve just seen, generating a single image takes some time. As we said in the introduction, we’ll need to introduce a web-queue-worker architecture to make this system reliable and scalable.
Creating a Dramatiq worker and defining an image- generation task
As we mentioned in the introduction of this chapter, it’s not conceivable to run our image-generation model directly on our REST API server. As we saw in the previous section, the operation can take several minutes and consumes a massive amount of memory. To solve this, we’ll define another process, apart from the server process, that’ll take care of this image-generation task: the worker. In essence, a worker can be any program whose role is to compute a task in the background.
In web development, this concept usually implies a bit more than this. A worker is a process running continuously in the background, waiting for incoming tasks. The tasks are usually sent by the web server, which asks for specific operations given the user actions.
Therefore, we see that we need a communication channel between the web server and the worker. That’s the role of the queue. It’ll accept and stack messages coming from the web server and make them available to read for the worker. That’s the web-queue-worker architecture. To better understand it, Figure 14.4 shows you the schema of such an architecture.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Figure 14.3 – Schema of web-queue-worker architecture
Does it ring a bell? Yes, it’s very similar to what we saw in Chapter 8, in the Handling multiple WebSocket connections and broadcasting messages section. Actually, this is the same principle: we solve the problem of having separate processes by having a single central data source.
The great feature of this architecture is that it scales very easily. Imagine your application is a huge success and thousands of users want to generate images: a single worker wouldn’t be able to meet the demand. Actually, all we need to do is to start more worker processes. Since there is a single message broker in the architecture, each worker will pull messages as they come, allowing tasks to be processed in parallel. They don’t even need to be on the same physical machine. This is shown in Figure 14.4.
Figure 14.4 – Web-queue-worker architecture with multiple workers
Creating a Dramatiq worker and defining an image-generation task
In Python, there are several libraries to help implement a worker. They provide the required tools to define tasks, schedule them in the queue, and run a process, pulling them and executing them. In this book, we’ll use Dramatiq, a lightweight but powerful and modern background task-processing library. As we did in Chapter 8, we’ll use Redis as a message broker.
Implementing a worker
As usual, we’ll start by installing the required dependency. Run the following command:
(venv) $ pip install "dramatiq[redis]"
This will install Dramatiq with the required dependencies to talk with a Redis broker.
In a minimal example, setting up a Dramatiq worker involves two things:
1. Setting the broker type and URL.
2. Defining tasks by wrapping functions with the @dramatiq.actor decorator.
It works very well for the vast majority of tasks, such as sending emails or generating exports.
In our case, however, we need to load the heavy Stable Diffusion model. As we usually do in the FastAPI server with the startup event, we want to do this only when the process is actually started. To do this with Dramatiq, we implement a middleware. They allow us to plug custom logic at several key events in the lifetime of the worker, including when it’s started.
You can see the implementation of our custom middleware in the following sample:
worker.py
class TextToImageMiddleware(Middleware): def __init__(self) -> None: super().__init__() self.text_to_image = TextToImage()
def after_process_boot(self, broker): self.text_to_image.load_model() return super().after_process_boot(broker)
text_to_image_middleware = TextToImageMiddleware() redis_broker = RedisBroker(host="localhost") redis_broker.add_middleware(text_to_image_middleware) dramatiq.set_broker(redis_broker)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/basic/work- er.py
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We define a TextToImageMiddleware class whose role is to bear an instance of TextToImage, the image generation service we defined in the previous section. It inherits from the Middleware class of Dramatiq. The key thing here is the after_process_boot method. It’s one of the event hooks exposed by Dramatiq, allowing us to plug our own logic. Here, we tell it to load the Stable Diffusion model when the worker process has booted up. You can see the full list of supported hooks in the official documentation: https://dramatiq.io/reference.html#middleware.
The next lines allow us to configure our worker. We first instantiate an instance of our custom middleware. Then, we create a broker class corresponding to the technology we chose; in our case, Redis. We take care of adding our middleware to this broker before telling Dramatiq to use it. Our worker is now completely configured to connect to a Redis broker and load our model at startup.
Now, let’s see how we can define a task to generate images:
worker.py
@dramatiq.actor() def text_to_image_task( prompt: str, *, negative_prompt: str | None = None, num_steps: int = 50 ): image = text_to_image_middleware.text_to_image.generate( prompt, negative_prompt=negative_prompt, num_steps=num_steps ) image.save(f"{uuid.uuid4()}.png")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/basic/work- er.py
The implementation is straightforward: Dramatiq tasks are actually plain functions that we decorated with @dramatiq.actor. We can define arguments as we would for any other function. However, there is an important pitfall to avoid here: when we schedule tasks from our server, the arguments will have to be stored in the queue storage. Thus, Dramatiq will internally serialize the arguments to JSON. It means your task arguments must be serializable data – you can’t have arbitrary Python objects, such as class instances or functions.
The function body calls our TextToImage instance loaded in text_to_image_middleware, before saving the image to the disk. To avoid file overrides, we choose here to generate a UUID, a Universally Unique IDentifier. It’s a big random string that’s guaranteed to be unique in each generation. Thanks to this, we can safely use it as a filename and be sure it won’t already exist on our disk.
That’s it for the worker implementation.
Creating a Dramatiq worker and defining an image-generation task
Starting the worker
We don’t have the web server code to call it yet, but we can already try it manually. First, make sure you have a Redis server started, as explained in the Technical requirements section. Then, we can start the Dramatiq worker using the following command:
(venv) $ dramatiq -p 1 -t 1 chapter14.basic.worker
Dramatiq comes with command-line tools to take care of starting the worker processes. The main positional argument is the dotted path of your worker module. It’s similar to what we do with Uvicorn. We also set two optional parameters, -p and -t. They control the number of processes and threads Dramatiq will start. By default, it starts 10 processes, each one with 8 threads. This means there will be 80 workers able to pull and execute tasks. While this default is good for common needs, it doesn’t work with our Stable Diffusion model for two reasons:
Each thread in a process shares the same memory space. This means that if two (or more) threads try to generate an image, they will read and write on the same objects in memory. For our model here, this causes concurrency problems. We say that it’s not thread-safe. Hence, each process should start only one thread: that’s the point of the -t 1 option.
Each process should load the model in memory. This means that if we start 8 processes, we’ll load the model 8 times. As we saw earlier, it takes quite a huge amount of memory, so doing this would probably blow up your computer’s memory. To be safe here, we start only one process thanks to the -p 1 option. If you want to try parallelization and see that our worker is able to generate two images in parallel, you can try -p 2 to spawn two processes. Make sure your computer can handle it though!
If you run the preceding command, you should see an output like this:
[2023-02-02 08:52:11,479] [PID 44348] [MainThread] [dramatiq. MainProcess] [INFO] Dramatiq '1.13.0' is booting up. Fetching 19 files: 0%| | 0/19 [00:00<?, ?it/s] Fetching 19 files: 100%|██████████| 19/19 [00:00<00:00, 13990.83it/s] [2023-02-02 08:52:11,477] [PID 44350] [MainThread] [dramatiq. WorkerProcess(0)] [INFO] Worker process is ready for action. [2023-02-02 08:52:11,578] [PID 44355] [MainThread] [dramatiq. ForkProcess(0)] [INFO] Fork process 'dramatiq.middleware.prometheus:_ run_exposition_server' is ready for action.
You can see the output of the Stable Diffusion pipeline checking whether the model files are downloaded before the worker is fully started. This means that it has been correctly loaded.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Scheduling tasks in the worker
We can now try to schedule tasks in our worker. For this, we can start a Python interactive shell and import the task function. Open a new command line and run the following commands (make sure you enabled your Python virtual environment):
(venv) $ python >>> from chapter14.basic.worker import text_to_image_task >>> text_to_image_task.send("A Renaissance castle in the Loire Valley") Message(queue_name='default', actor_name='text_to_image_task', args=('A Renaissance castle in the Loire Valley',), kwargs={}, options={'redis_message_id': '663df44a-cfc1-4f13-8457-05d8181290c1'}, message_id='bf57d112-6c20-49bc-a926-682ca43ea7ea', message_ timestamp=1675324585644)
That’s it – we scheduled a task in the worker! Notice how we used the send method on our task function instead of calling it directly: this is how you tell Dramatiq to send it in the queue.
If you go back to your worker terminal, you’ll see the Stable Diffusion output generating the image. After a moment, you’ll have your image saved on disk. You can also try to send two tasks in a row in a short time. You’ll find that Dramatiq processes them one after the other.
Great job! We have our background process ready and are even able to schedule tasks in it. The next step now is to implement a REST API so the users can ask for image generation themselves.
Implementing the REST API
To schedule tasks in our worker, we need a safe interface users can interact with. A REST API is a good choice for this, since it can be easily integrated into any software, such as a website or a mobile app. In this section, we’ll very quickly review a simple API endpoint we implemented to send image- generation tasks into our queue. Here’s the implementation:
api.py
class ImageGenerationInput(BaseModel): prompt: str negative_prompt: str | None num_steps: int = Field(50, gt=0, le=50)
class ImageGenerationOutput(BaseModel): task_id: UUID4
app = FastAPI()
Storing results in a database and object storage
@app.post( "/image-generation", response_model=ImageGenerationOutput, status_code=status.HTTP_202_ACCEPTED, ) async def post_image_generation(input: ImageGenerationInput) -> ImageGenerationOutput: task: Message = text_to_image_task.send( input.prompt, negative_prompt=input.negative_prompt, num_ steps=input.num_steps ) return ImageGenerationOutput(task_id=task.message_id)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/basic/api. py
If you have followed along since the beginning of this book, this shouldn’t surprise you. We took care of defining proper Pydantic models to structure and validate the endpoint payload. This data is then directly used to send a task to Dramatiq, as we saw in the previous section.
In this simple implementation, the output consists only of the message ID, which is automatically assigned to each task by Dramatiq. Notice that we set the HTTP status code to 202, which means Accepted. Semantically, it means the server understood and accepted the request, but the processing has not yet finished or even started. It’s specifically designed for cases where the processing is done in the background, which is exactly our case here.
If you start both the worker and this API, you’ll be able to trigger image generations with an HTTP call.
You’re probably wondering here: That’s nice… But how will the users retrieve the result? How will they know whether the task is done?. You’re right – we didn’t talk at all about this problem! Actually, there are two aspects to solve here: how do we keep track of the pending tasks and their execution? How do we store and serve the resulting images? That’s the subject of the next section.
Storing results in a database and object storage
In the previous section, we showed how to implement a background worker to do the heavy computation and an API to schedule tasks on this worker. However, we are still missing two important aspects: the user doesn’t have any way to know the progress of the task nor to retrieve the final result. Let’s fix this!
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Sharing data between the worker and the API
As we’ve seen, the worker is a program running in the background executing the computations the API has asked it to do. However, the worker doesn’t have any way to talk with the API server. That’s expected: since there could be any number of server processes, and since they could even run on different physical servers, processes cannot communicate directly. It’s always the same problem of having a central data source on which processes can write and read data.
Actually, the first approach to solve the lack of communication between the API and the worker could be to use the same broker we use to schedule tasks: the worker could write results in the broker, and the API could read from it. This is something possible with most background task libraries, including Dramatiq. However, this solution has some limitations, the principal one being the limited time we can retain the data. Brokers, such as Redis, are not really suited to storing data reliably for a long period. At some point, we’ll need to erase the most ancient data to limit memory usage.
Yet, we already know of something able to store structured data efficiently: a database, of course! That’s the approach we’ll show here. By having a central database where we’ll store our image generation requests and results, we’ll be able to share information between the worker and the API. For this, we’ll reuse a lot of techniques we showed in the Communicating with a SQL database with SQLAlchemy ORM section of Chapter 6. Let’s go!
Defining an SQLAlchemy model
The first step is defining an SQLAlchemy model to store a single image-generation task. You can see it as follows:
models.py
class GeneratedImage(Base): __tablename__ = "generated_images"
id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True) created_at: Mapped[datetime] = mapped_column( DateTime, nullable=False, default=datetime.now ) progress: Mapped[int] = mapped_column(Integer, nullable=False, default=0)
prompt: Mapped[str] = mapped_column(Text, nullable=False) negative_prompt: Mapped[str | None] = mapped_column(Text, nullable=True) num_steps: Mapped[int] = mapped_column(Integer, nullable=False)
Storing results in a database and object storage
file_name: Mapped[str | None] = mapped_column(String(255), nullable=True)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ models.py
As usual, we define an auto-incremented ID as the primary key. We also add prompt, negative_ prompt, and num_steps columns, which correspond to the arguments we give to the worker task. This way, we’ll be able to directly give the ID to the worker, and it’ll take the parameter directly from the object. Besides, it’ll allow us to store and remember the parameters we used for a specific generation.
The progress column is an integer where we’ll store the current progress of the generation task.
Finally, file_name will store the actual filename we’ll store on our system. We’ll see how we use it in the next section, about object storage.
Adapting the API to save image-generation tasks in a database
With this model at hand, our approach to scheduling image generation in the API changes a bit. Instead of directly sending the task to the worker, we first create a row in our database and use the ID of this object as input for the worker task. The endpoint implementation is shown here:
api.py
@app.post( "/generated-images", response_model=schemas.GeneratedImageRead, status_code=status.HTTP_201_CREATED, ) async def create_generated_image( generated_image_create: schemas.GeneratedImageCreate, session: AsyncSession = Depends(get_async_session), ) GeneratedImage: image = GeneratedImage(**generated_image_create.dict()) session.add(image) await session.commit()
text_to_image_task.send(image.id)
return image
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ api.py
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We won’t go into the details about how to create an object in a database with SQLAlchemy ORM. If you need a refresher, you can refer to the Communicating with a SQL database with SQLAlchemy ORM section of Chapter 6.
The main thing to notice in this snippet is that we pass the ID of the newly created object as an argument of text_to_image_task. As we’ll see right after, the worker will read it again from the database to retrieve the generation parameters.
The response of this endpoint is simply a representation of our GeneratedImage model, using the Pydantic schema GeneratedImageRead. Thus, the user will get a response like this to their request:
{ "created_at": "2023-02-07T10:17:50.992822", "file_name": null, "id": 6, "negative_prompt": null, "num_steps": 50, "progress": 0, "prompt": "a sunset over a beach" }
It shows the prompt we gave in our request and, most importantly, it gives it an ID. This means that the user will be able to query for this specific request again to retrieve the data and see whether it’s done. That’s the purpose of the get_generated_image endpoint defined below the previous snippet. We won’t show it here, but you can read it in the examples repository.
Adapting the worker to read and update image-generation tasks from a database
You probably have guessed that we need to change the implementation of our task so it can retrieve objects from the database instead of reading the parameters directly. Let’s go through this step by step.
The first thing we do is retrieve a GeneratedImage from the database using the ID we got in the task argument.
worker.py
@dramatiq.actor() def text_to_image_task(image_id: int): image = get_image(image_id)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
Storing results in a database and object storage
To achieve this, you see that we use a helper function called get_image. It’s defined right above the task. Let’s review it:
worker.py
def get_image(id: int) -> GeneratedImage: async def _get_image(id: int) -> GeneratedImage: async with async_session_maker() as session: select_query = select(GeneratedImage). where(GeneratedImage.id == id) result = await session.execute(select_query) image = result.scalar_one_or_none()
if image is None: raise Exception("Image does not exist")
return image
return asyncio.run(_get_image(id))
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
It may look quite strange, but actually, you are already familiar with most of its logic. If you look closely, you’ll see that it defines a nested and private function where we define the actual logic to retrieve and save the object using SQLAlchemy ORM. Notice that it’s async, and that we make great use of async I/O patterns, as we’ve seen throughout this book.
That’s the exact reason why we need a helper function like this. Indeed, Dramatiq is not designed to run async functions natively, so we need to manually schedule their execution using asyncio. run. We already saw this function in Chapter 2, where we presented async I/O. Its role is to run an async function and return its result. That’s how we can call the wrapping function synchronously in our task without any issues.
Other approaches could work to tackle the async I/O problem The approach we show here is the most straightforward and robust one to tackle the problem of asynchronous workers.
Another approach could be to set up a decorator or middleware for Dramatiq so it could natively run async functions, but this is complex and subject to bugs.
We could also consider having another SQLAlchemy engine and session maker that works synchronously. However, this would require us to have a lot of duplicated things in our code. Besides, this wouldn’t help if we had async functions other than SQLAlchemy.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Now, let’s get back to the implementation of text_to_image_task:
worker.py
@dramatiq.actor() def text_to_image_task(image_id: int): image = get_image(image_id)
def callback(step: int, _timestep, _tensor): update_progress(image, step)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
We define a callback function for the Stable Diffusion pipeline. Its role is to save the current progress in a database for the current GeneratedImage. For this, we once again use a helper function, update_progress:
worker.py
def update_progress(image: GeneratedImage, step: int): async def _update_progress(image: GeneratedImage, step: int): async with async_session_maker() as session: image.progress = int((step / image.num_steps) * 100) session.add(image) await session.commit()
asyncio.run(_update_progress(image, step))
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
We use the same approach we explained for get_image, so we can wrap the async function.
Going back to text_to_image_task, we can now call our TextToImage model to generate an image. It’s exactly the same call we showed in the previous section. The only difference is that we take the parameters from the image object. We also generate a random filename using a UUID:
worker.py
image_output = text_to_image_middleware.text_to_image.generate( image.prompt, negative_prompt=image.negative_prompt,
Storing results in a database and object storage
num_steps=image.num_steps, callback=callback, )
file_name = f"{uuid.uuid4()}.png"
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
The following part is designed to upload the image to object storage. We’ll explain this in more detail in the next section:
worker.py
storage = Storage() storage.upload_image(image_output, file_name, settings.storage_ bucket)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
Finally, we call another helper function, update_file_name, to save the random filename in the database. It’ll allow us to retrieve the file for the user:
worker.py
update_file_name(image, file_name)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
As you can see, the main point of attention throughout this implementation is that we read and write information about GeneratedImage from and to the database. This is how we can synchronize between the API server and the worker. That’s it for the worker! With this logic, we are able to schedule an image-generation task from the API, and the worker is able to regularly update the task progress before setting the resulting filename. Thus, from the API, a simple GET request allows us to see the status of our task.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Storing and serving files in object storage
The last challenge we have to tackle concerns the storage of our resulting images. We need a way to store them reliably while letting users retrieve them easily from the internet.
Traditionally, web applications handled this quite simply. They stored the files directly on the server hard disk, in a defined directory, and configured their web server to serve those files when accessed under a certain URL. This is actually what we did in Chapter 13, in the WebSocket example: we used the StaticFiles middleware to statically serve the JavaScript script we had on disk.
While this works well for static files, such as JavaScript or CSS files, for which each server has its own copy, it is not suitable for dynamic files uploaded by the user or generated by the backend, in particular for complex architectures where several processes are run on different physical machines. Once again, this is the problem of having a central source of data that the different processes read from. In the previous sections, we saw that message brokers and databases could solve this issue in several contexts. In the case of arbitrary binary files, whether they are images, videos, or simple text files, we need something else. Let’s introduce object storage.
Object storage is a bit different from the standard file storage we use daily in computers, where the disk is organized in a hierarchy of directories and files. Instead, object storage will store each file as an object, which includes the actual data and all its metadata, such as its name, size, type, and a unique ID. The main benefit of such conceptualization is that it’s easier to spread those files across multiple physical machines: we can store billions of files on the same object storage. From the user’s point of view, we just ask for a specific file, and the storage will take care of loading the file from the actual physical disk.
In the cloud era, this approach has obviously gained a lot of popularity. In 2006, Amazon Web Services (AWS) launched Amazon S3, its own implementation of object storage. It gave developers access to virtually unlimited disk space to store files using a simple API, all at a very cheap price. Amazon S3 gained so much popularity its API became the de facto standard in the industry. Nowadays, most cloud object storage, including storage from competitors such as Microsoft Azure or Google Cloud, is compatible with the S3 API. Open source implementations have also emerged, such as MinIO. The main benefit of this common S3 API is that you can use the same code and libraries in your project to talk with any object storage provider and easily switch if needed.
To sum up, object storage is a very convenient way to store and serve files at scale, no matter the number of processes that need to access this data. At the end of this section, the global architecture of our project will look like the one shown in Figure 14.5.
Storing results in a database and object storage
Figure 14.5 – Web-queue-worker architecture and object storage
It’s worth noting that the object storage will serve the file directly to the user. There won’t be an endpoint where the server would act as a proxy by downloading the file from the object storage before sending it to the user. There isn’t much benefit in doing it that way, even in terms of authentication. We’ll see that S3-compatible storage has built-in mechanisms to protect files from unauthorized access.
Implementing an object storage helper
Let’s get to the code then! We’ll use the MinIO client for Python, a library to interact with any S3-compatible storage. Let’s install it:
(venv) $ pip install minio
We can now implement a class to have all the operations we need at hand. Let’s first go with the initializer:
storage.py
class Storage: def __init__(self) -> None: self.client = Minio( settings.storage_endpoint, access_key=settings.storage_access_key,
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
secret_key=settings.storage_secret_key, )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
In the initializer of this class, we create a Minio client instance. You’ll see that we use a settings object to pull the storage URL and credentials. Thus, it’s very easy to switch them by using environment variables.
We’ll then implement several methods that’ll help us work with object storage. The first one is ensure_ bucket:
storage.py
def ensure_bucket(self, bucket_name: str): bucket_exists = self.client.bucket_exists(bucket_name) if not bucket_exists: self.client.make_bucket(bucket_name)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
The role of this method is to make sure the right bucket is created in our object storage. In S3 implementations, a bucket is like a folder that you own and in which you can store your files. Each file you upload has to be put into an existing bucket.
Then, we define upload_image:
storage.py
def upload_image(self, image: Image, object_name: str, bucket_ name: str): self.ensure_bucket(bucket_name)
image_data = io.BytesIO() image.save(image_data, format="PNG") image_data.seek(0) image_data_length = len(image_data.getvalue())
self.client.put_object( bucket_name, object_name, image_data,
Storing results in a database and object storage
length=image_data_length, content_type="image/png", )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
This is for uploading an image to the storage. To simplify things, this method accepts a Pillow Image, as it’s the result we get at the end of the Stable Diffusion pipeline. We implemented some logic to convert this Image object into a raw stream of bytes suitable for the S3 upload. This method also expects object_name, which will be the actual name of the file in the storage, along with bucket_name. Notice that we first ensure the bucket is correctly created before trying to upload the file.
Finally, we add the get_presigned_url method:
storage.py
def get_presigned_url( self, object_name: str, bucket_name: str, *, expires: timedelta = timedelta(days=7) ) str: return self.client.presigned_get_object( bucket_name, object_name, expires=expires )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ storage.py
This method will help us to serve the file securely to the user. By default, for security reasons, files in S3 storage are not accessible by any user on the internet. To give access to a file, we can do either of the following:
Set the file as public so anybody with the URL can access it. This is suitable for public files but certainly not for private user files.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
Generate a URL with a temporary access key. Thus, we can give access to the file to the user, knowing that even if the URL is stolen, the access will be revoked after a certain time. The huge benefit of this is that this URL generation happens on our API server using the S3 client. Therefore, we could check whether the user is correctly authenticated and has the rights to this specific file following our own logic before generating the file URL. This is the approach we adopt here, and this method generates the pre-signed URL on a specific file in a specific bucket for a certain amount of time.
As you can see, our class is just a thin wrapper around the MinIO client. All we have to do now is to use it to upload the images and get a pre-signed URL from the API.
Using the object storage helper in the worker
In the previous section, we showed the following lines in our task implementation:
worker.py
storage = Storage() storage.upload_image(image_output, file_name, settings.storage_ bucket)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ worker.py
Now that we’ve talked about the Storage class, you should guess what we’re doing here: we take the generated image and its random name and upload it to a bucket defined in settings. And… That’s it!
Generating a pre-signed URL on the server
On the API’s side, we implement a new endpoint whose role is to return a pre-signed URL for a given GeneratedImage:
server.py
@app.get("/generated-images/{id}/url") async def get_generated_image_url( image: GeneratedImage = Depends(get_generated_image_or_404), storage: Storage = Depends(get_storage), ) schemas.GeneratedImageURL: if image.file_name is None: raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail="Image is not available yet. Please try again later.",
Storing results in a database and object storage
)
url = storage.get_presigned_url(image.file_name, settings.storage_ bucket) return schemas.GeneratedImageURL(url=url)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter14/complete/ server.py
Before generating the URL, we first check whether the file_name property is set on the GeneratedImage object. If it’s not, it means the worker has not completed the task yet. If it is, we can proceed with the call to the get_presigned_url method of our Storage class.
Notice that we took care of defining a dependency injection to get our Storage instance. As we’ve seen throughout this book, using dependencies in FastAPI is a very good practice when dealing with external services.
Well, it seems that we’re all set! Let’s see it in action.
Running the image-generation system
First of all, we need to populate the environment variables for our project with, in particular, a database URL and S3 credentials. To keep things simple, we’ll use a simple SQLite database and the MinIO playground for the S3 storage. It’s a free and open instance of MinIO object storage that’s perfect for examples and toy projects. When going into production, you’ll be able to easily switch to any S3-compatible provider. Let’s create a .env file at the root of the project:
DATABASE_URL=sqlite+aiosqlite:///chapter14.db STORAGE_ENDPOINT=play.min.io STORAGE_ACCESS_KEY=Q3AM3UQ867SPQQA43P2F STORAGE_SECRET_KEY=zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG STORAGE_BUCKET=fastapi-book-text-to-image
The storage endpoint, access key, and secret key are the parameters for the MinIO playground. Make sure to check their official documentation to see whether they have changed since we wrote this book: https://min.io/docs/minio/linux/developers/python/minio-py. html#id5.
Our Settings class will automatically load this file to populate the settings we use throughout the code. Make sure to check the Setting and using environment variables section of Chapter 10 if you need a refresher on this concept.
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We can now run our system. Make sure your Redis server is still running, as explained in the Technical requirements section. First of all, let’s run the FastAPI server:
(venv) $ uvicorn chapter14.complete.api:app
Then, start the worker:
(venv) $ dramatiq -p 1 -t 1 chapter14.complete.worker
The stack is now ready to generate images. Let’s make a request with HTTPie to start a new task:
$ http POST http://localhost:8000/generated-images prompt="a sunset over a beach" HTTP/1.1 201 Created content-length: 151 content-type: application/json date: Mon, 13 Feb 2023 07:24:44 GMT server: uvicorn
{ "created_at": "2023-02-13T08:24:45.954240", "file_name": null, "id": 1, "negative_prompt": null, "num_steps": 50, "progress": 0, "prompt": "a sunset over a beach" }
A new GeneratedImage has been created in the database with the assigned ID 1. The progress is at 0%; the processing has not started yet. Let’s try to query it with our API:
http GET http://localhost:8000/generated-images/1 HTTP/1.1 200 OK content-length: 152 content-type: application/json date: Mon, 13 Feb 2023 07:25:04 GMT server: uvicorn
{ "created_at": "2023-02-13T08:24:45.954240", "file_name": null, "id": 1, "negative_prompt": null, "num_steps": 50,
Storing results in a database and object storage
"progress": 36, "prompt": "a sunset over a beach" }
The API returns the same object with all its properties. Notice that the progress has been updated and that it’s now at 36%. After a while, we can try the same request again:
$ http GET http://localhost:8000/generated-images/1 HTTP/1.1 200 OK content-length: 191 content-type: application/json date: Mon, 13 Feb 2023 07:25:34 GMT server: uvicorn
{ "created_at": "2023-02-13T08:24:45.954240", "file_name": "affeec65-5d9b-480e-ac08-000c74e22dc9.png", "id": 1, "negative_prompt": null, "num_steps": 50, "progress": 100, "prompt": "a sunset over a beach" }
This time, the progress is at 100% and the filename has been filled. The image is ready! We can now ask our API to generate a pre-signed URL for this image:
$ http GET http://localhost:8000/generated-images/1/url HTTP/1.1 200 OK content-length: 366 content-type: application/json date: Mon, 13 Feb 2023 07:29:53 GMT server: uvicorn
{ "url": "https://play.min.io/fastapi-book-text-to-image/ affeec65-5d9b-480e-ac08-000c74e22dc9.png?X-Amz-Algorithm=AWS4- HMAC-SHA256&X-Amz-Credential=Q3AM3UQ867SPQQA43P2F%2F20230213%2 Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230213T072954Z&X-Amz- Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=6ffddb81702bed 6aac50786578eb75af3c1f6a3db28e4990467c973cb3b457a9" }
Creating a Distributed Text-to-Image AI System Using the Stable Diffusion Model
We get a very long URL on the MinIO server. If you open it in your browser, you’ll see the image that has just been generated by our system, as you can see in Figure 14.6.
Figure 14.6 – Generated image hosted on object storage
Quite nice, isn’t it? We now have a fully featured system where the user is able to do the following:
Request to generate images following their own prompt and parameters
Get information about the progress of the request
Get the resulting image from reliable storage
The architecture we see here is already deployable in a cloud environment with multiple machines. Typically, we may have a standard, cheap server to serve the API and a more expensive one with a dedicated GPU and a good amount of RAM to run the worker. The code doesn’t have to change to handle this kind of deployment since the communication between processes is handled by the central elements – the message broker, the database, and the object storage.
Summary
Awesome! You may not have realized it yet, but in this chapter, you learned how to architect and implement a very complex machine learning system that could rival existing image-generation services you see out there. The concepts we showed here are essential and are at the heart of all the distributed systems you could imagine, whether they are designed to run machine learning models, extraction pipelines, or math computations. By using modern tools such as FastAPI and Dramatiq, you’ll be able to implement this kind of architecture in a short time with a minimum amount of code, leading to a very quick and robust result.
We’re near the end of our journey. Before letting you live your own adventures with FastAPI, we’ll study one last important aspect when building data science applications: logging and monitoring.
15 Monitoring the Health and Performance of a Data Science System
In this chapter, we will cover the extra mile so you are able to build robust, production-ready systems. One of the most important aspects to achieve this is to have all the data we need to ensure the system is operating correctly and detect as soon as possible when something goes wrong so we can take corrective actions. In this chapter, we’ll see how to set up a proper logging facility and how we can monitor the performance and health of our software in real time.
We’re near the end of our journey into FastAPI for data science. Until now, we’ve mainly focused on the functionality of the programs we implemented. However, there is another aspect that is often overlooked by developers but is actually very important: assessing whether the system is functioning correctly and reliably in production and being warned as soon as possible when that’s not the case.
For this, lot of tools and techniques exist so we can gather the maximum amount of data about how our program is performing. That’s what we’ll review in this chapter.
We’re going to cover the following main topics:
Configuring and using a logging facility with Loguru
Configuring Prometheus metrics and monitoring them in Grafana
Configuring Sentry for reporting errors
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
Monitoring the Health and Performance of a Data Science System
To run a Dramatiq worker, you’ll need a running Redis server on your local computer. The easiest way is to run it as a Docker container. If you’ve never used Docker before, we recommend you read the Getting started tutorial in the official documentation at https://docs.docker.com/ get-started/. Once done, you’ll be able to run a Redis server with this simple command:
$ docker run -d --name worker-redis -p 6379:6379 redis
You’ll find all the code examples of this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter15.
A note about the screenshots In the course of this chapter, we’ll present several screenshots, in particular of the Grafana interface. Their goal is to show you the general layout of the UI to help you identify its different parts. Don’t worry if you struggle to read the actual content: the explanations around them will explain where to look at and what to interact with.
Configuring and using a logging facility with Loguru
In software development, logs are probably the simplest but most powerful way to control the behavior of a system. They usually consist of lines of plain text that are printed at specific points of a program. By reading them chronologically, we are able to trace the behavior of the program and check that everything goes well. Actually, we’ve already seen log lines in this book. When you run a FastAPI app with Uvicorn and make some requests, you’ll see these lines in the console output:
INFO: Started server process [94918] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: 127.0.0.1:60736 - "POST /generated-images HTTP/1.1" 201 Created
Those are the logs generated by Uvicorn, which tell us when it has started and when it has handled a request. As you can see, logs can help us to know what happened in our program and what actions it performed. They can also tell us when something goes wrong, which could be a bug that needs to be solved.
Configuring and using a logging facility with Loguru
Understanding log levels
Notice that before each log line, we have the INFO keyword. This is what we call the log level. It’s a way to classify the importance of this log. In general, the following levels are defined:
DEBUG
INFO
WARNING
ERROR
You can consider this the level of importance: DEBUG is really specific information about what the program does, which could help you to debug the code, while ERROR means that something bad happened in your program, which probably requires action on your part. The good thing about those levels is that we can configure the minimum level that should be output by the logger. The actual call to the log function is still there in the code, but it’s ignored by the logger if it doesn’t match the minimum level.
Typically, we can set the DEBUG level in local development so we have all the information to help us develop and fix our program. On the other hand, we can set the level to INFO or WARNING in production so we have only the most important messages.
Adding logs with Loguru
Adding your own logs to a Python program can be fairly easy using the logging module available in the standard library. You could do something like this:
>>> import logging >>> logging.warning("This is my log") WARNING:root:This is my log
As you can see, it’s just a function call with a string in the argument. Typically, logging modules expose the different levels as methods, as you see here with warning.
The standard logging module is really powerful and allows you to finely customize how your logs are handled, printed, and formatted. If you go through the logging tutorials in the official documentation, https://docs.python.org/3/howto/logging.html, you’ll see it can quickly become really complex, even for simple cases.
That’s why Python developers usually use libraries wrapping the logging module and exposing much more friendly functions and interfaces. In this chapter, we’ll review how to use and configure Loguru, a modern yet simple approach to logging.
Monitoring the Health and Performance of a Data Science System
As always, the first thing to do is to install it in our Python environment:
(venv) $ pip install loguru
We can try it right away in a Python shell:
>>> from loguru import logger >>> logger.debug("This is my log!") 2023-02-21 08:44:00.168 | DEBUG | __main__:<module>:1 - This is my log!
You may think that’s not very different from what we did with the standard logging module. However, notice the resulting log already includes the timestamp, the level, and the position of the function call in the code. That’s one of the main benefits of Loguru: it comes with sensible defaults working out of the box.
Let’s see it in action in a more complete script. We’ll define a simple function to check whether an integer, n, is odd or not. We’ll add a debug line to let us know the function starts its logic. Then, before computing the result, we’ll first check whether n truly is an integer and log an error if not. The implementation of this function looks like this:
chapter15_logs_01.py
from loguru import logger
def is_even(n) -> bool: logger.debug("Check if {n} is even", n=n) if not isinstance(n, int): logger.error("{n} is not an integer", n=n) raise TypeError() return n % 2 == 0
if __name__ == "__main__": is_even(2) is_even("hello")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_01.py
Configuring and using a logging facility with Loguru
As you can see, it’s really simple to use: we just have to import logger and call it wherever we need to log something. Notice also how we can add variables to format our string: we just need to add a placeholder around curly braces inside the string and then map each placeholder to its value with keyword arguments. This syntax is actually similar to the standard str.format method. You can read more about it in the official Python documentation: https://docs.python.org/fr/3/ library/stdtypes.html#str.format.
If we run this simple script, we’ll see our log lines in the console output:
(venv) $ python chapter15/chapter15_logs_01.py 2023-03-03 08:16:40.145 | DEBUG | __main__:is_even:5 - Check if 2 is even 2023-03-03 08:16:40.145 | DEBUG | __main__:is_even:5 - Check if hello is even 2023-03-03 08:16:40.145 | ERROR | __main__:is_even:7 - hello is not an integer Traceback (most recent call last): File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_01.py", line 14, in <module> is_even("hello") File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_01.py", line 8, in is_even raise TypeError() TypeError
Our log lines are correctly added to the output before the actual exception is raised. Notice how Loguru is able to precisely tell us where the log call comes from in the code: we have the function’s name and line.
Understanding and configuring sinks
We’ve seen that, by default, logs are added to the console output. By default, Loguru defines a sink targeted at a standard error. A sink is a concept introduced by Loguru to define how log lines should be handled by the logger. We’re not limited to console output: we can also save them to a file, or a database, or even send them to a web service!
The good thing is that you’re not limited to only one sink; you can have as many as you need! Then, each log call will be processed through each sink accordingly. You can see a schematic representation of this approach in Figure 15.1.
Monitoring the Health and Performance of a Data Science System
Figure 15.1 – Schema of Loguru sinks
Each sink is associated with a log level. This means that we could have different log levels depending on the sink. For example, we could choose to output all logs to a file and keep only the most important warning and error logs in the console. Let’s again take our previous example and configure Loguru with this approach:
chapter15_logs_02.py
logger.remove() logger.add(sys.stdout, level="WARNING") logger.add("file.log", level="DEBUG", rotation="1 day")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_02.py
The remove method of logger is helpful for removing a previously defined sink. When calling it like this with no parameter, all the defined sinks are removed. By doing this, we start fresh without the default sink.
Then, we call add to define new sinks. The first parameter, like sys.stdout or file.log here, defines how the log calls should be handled. This parameter can be many things, such as a callable function, but Loguru allows us, for convenience, to directly pass file-like objects, such as sys.stdout, or strings, which will be interpreted as filenames. Several arguments are accepted to customize all the aspects of the sink and, in particular, the level.
As we said, the standard output sink will only log messages with at least a WARNING level, while the file sink will log all messages.
Configuring and using a logging facility with Loguru
Notice also that we added a rotation parameter for the file sink. Since logs will continuously be appended to a file, it can quickly grow in size during the lifetime of your application. That’s why we have access to a couple of options:
“Rotate” the file: This means that the current file will be renamed, and new logs will be added to a new file. This operation can be configured so it happens after a certain amount of time (for example, every day, as in our example) or when it reaches a certain size.
Remove older files: After a certain amount of time, it’s probably not very useful to keep older logs that take up unnecessary space on your disk.
You can read all the details about these features in the official documentation for Loguru: https:// loguru.readthedocs.io/en/stable/api/logger.html#file.
Now, if we run this example, we’ll see this in the console output:
(venv) $ python chapter15/chapter15_logs_02.py 2023-03-03 08:15:16.804 | ERROR | __main__:is_even:12 - hello is not an integer Traceback (most recent call last): File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_02.py", line 19, in <module> is_even("hello") File "/Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition/chapter15/chapter15_logs_02.py", line 13, in is_even raise TypeError() TypeError
The DEBUG logs don’t appear anymore. However, if we read the file.log file, we’ll have both:
$ cat file.log 2023-03-03 08:15:16.803 | DEBUG | __main__:is_even:10 - Check if 2 is even 2023-03-03 08:15:16.804 | DEBUG | __main__:is_even:10 - Check if hello is even 2023-03-03 08:15:16.804 | ERROR | __main__:is_even:12 - hello is not an integer
That’s it! Sinks are really useful for routing our logs to different places depending on their nature or importance.
Structuring logs and adding context
In their simplest form, logs consist of free-form text. While convenient, we’ve seen that we usually need to log variable values to better understand what’s going on. With only strings, this usually ends up in a messy string consisting of multiple concatenated values.
Monitoring the Health and Performance of a Data Science System
A better approach to handle this is to adopt structured logging. The goal is to have a clear and proper structure for each log line, so we can embed all the information we need without sacrificing readability. Loguru supports this approach natively, thanks to contexts. The next example shows you how to use it:
chapter15_logs_03.py
def is_even(n) -> bool: logger_context = logger.bind(n=n) logger_context.debug("Check if even") if not isinstance(n, int): logger_context.error("Not an integer") raise TypeError() return n % 2 == 0
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_03.py
We once again took the same example as before. As you can see, we use the bind method of logger to retain extra information. Here, we set the n variable. This method returns a new instance of our logger with those attributes attached. Then, we can use this instance normally to log things. We don’t need to add n in the formatted string anymore.
However, if you try this example directly, you won’t see the value of n in the logs. That’s normal: by default, Loguru doesn’t add context information to the formatted log line. We need to customize it! Let’s see how:
chapter15_logs_04.py
logger.add( sys.stdout, level="DEBUG", format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | " "<level>{level: <8}</level> | " "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>" " - {extra}", )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_04.py
Configuring and using a logging facility with Loguru
To format log output, we have to use the format parameter when configuring a sink. It expects a template string. Here, we copied and pasted the default Loguru format and added a part with the extra variable. extra is a dictionary where Loguru stores all the values you added in context. Here, we just output it directly so we can see all variables.
Format syntax and available variables You can find all the available variables you can output in the format string, such as extra or level, in the Loguru documentation: https://loguru.readthedocs.io/en/ stable/api/logger.html#record.
The format string supports standard formatting directives, which are useful for retrieving values, format numbers, pad strings, and so on. You can read more about it in the Python documentation: https://docs.python.org/3/library/string.html#format- string-syntax.
Also, Loguru adds special markup so you can color the output. You can read more about it here: https://loguru.readthedocs.io/en/stable/api/logger.html#color.
This time, if you run this example, you’ll see the extra context added to the log lines:
(venv) $ python chapter15/chapter15_logs_04.py 2023-03-03 08:30:10.905 | DEBUG | __main__:is_even:18 - Check if even - {'n': 2} 2023-03-03 08:30:10.905 | DEBUG | __main__:is_even:18 - Check if even - {'n': 'hello'} 2023-03-03 08:30:10.905 | ERROR | __main__:is_even:20 - Not an integer - {'n': 'hello'}
This approach is very convenient and powerful: if you want to keep track of a value you care about across logs, you just have to add it once.
Logs as JSON objects Another approach to structured logging is to serialize all the data of a log into a JSON object. This can be enabled easily with Loguru by setting serialize=True when configuring the sink. This approach can be interesting if you plan to use a log ingestion service such as Logstash or Datadog: they will be able to parse the JSON data and make it available for querying.
You now have the basics of adding and configuring logs with Loguru. Let’s now see how we can leverage them in a FastAPI application.
Monitoring the Health and Performance of a Data Science System
Configuring Loguru as the central logger
Adding logs to your FastAPI application can be really useful to know what’s happening in your different routes and dependencies.
Let’s take an example from Chapter 5, where we added a global dependency to check for a secret value that should be set in the header. In this new version, we’ll add a debug log to trace when the secret_ header dependency is called and a warning log to inform us when this secret is missing or invalid:
chapter15_logs_05.py
from loguru import logger
def secret_header(secret_header: str | None = Header(None)) -> None: logger.debug("Check secret header") if not secret_header or secret_header != "SECRET_VALUE": logger.warning("Invalid or missing secret header") raise HTTPException(status.HTTP_403_FORBIDDEN)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ logs_05.py
That’s nothing really surprising if you have followed us so far! Now, let’s run this application with Uvicorn and make a request with an invalid header:
INFO: Started server process [47073] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) 2023-03-03 09:00:47.324 | DEBUG | chapter15.chapter15_ logs_05:secret_header:6 - Check secret header 2023-03-03 09:00:47.324 | WARNING | chapter15.chapter15_ logs_05:secret_header:8 - Invalid or missing secret header INFO: 127.0.0.1:58190 - "GET /route1 HTTP/1.1" 403 Forbidden
Our own logs are here, but there is a problem: Uvicorn also adds its own logs, but it doesn’t follow our format! Actually, that’s expected: other libraries, such as Uvicorn, may have their own logs with their own settings. As such, they won’t follow what we defined with Loguru. It’s a bit annoying because if we have a complex, well-thought-out setup, we would like every log to follow it. Fortunately, there are ways to configure this.
Configuring and using a logging facility with Loguru
First of all, we’ll create a module named logger.py, where we’ll put all our logger configurations. It’s a good practice in your project to have this module so your configuration is centralized in one place. The first thing we do in this file is to configure Loguru:
logger.py
LOG_LEVEL = "DEBUG"
logger.remove() logger.add( sys.stdout, level=LOG_LEVEL, format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | " "<level>{level: <8}</level> | " "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>" " - {extra}", )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
As we did in the previous section, we removed the default handler and defined our own. Notice that we set the level thanks to a constant named LOG_LEVEL. We hardcoded it here, but a better way would be to take the value from a Settings object, as we showed in Chapter 10. This way, we could directly set the level from environment variables!
After that, we have a quite complex piece of code in the class named InterceptHandler. It’s a custom handler for the standard logging module that will forward every standard log call to Loguru. This code is directly taken from the Loguru documentation. We won’t go into much detail about its functioning but just know that it’ll retrieve the log level and go through the call stack to retrieve the original caller and forward this information to Loguru.
The most important part, however, is how we use this class. Let’s see this here:
logger.py
logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)
for uvicorn_logger_name in ["uvicorn.error", "uvicorn.access"]: uvicorn_logger = logging.getLogger(uvicorn_logger_name)
Monitoring the Health and Performance of a Data Science System
uvicorn_logger.propagate = False uvicorn_logger.handlers = [InterceptHandler()]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
The trick here is to call the basicConfig method from the standard logging module to set our custom interception handler. This way, every log call made with the root logger, even ones from external libraries, will go through it and be handled by Loguru.
In some cases, however, this configuration is not sufficient. Some libraries define their own loggers with their own handlers, so they won’t use the root configuration. That’s the case for Uvicorn, which defines two main loggers: uvicorn.error and uvicorn.access. By retrieving those loggers and changing their handler, we force them to go through Loguru as well.
If you use other libraries that define their own loggers like Uvicorn does, you’ll probably need to apply the same technique. All you need to determine is the name of their logger, which should be quite easy to find in the library’s source code.
It works out of the box with Dramatiq If you implement a worker with Dramatiq, as we showed in Chapter 14, you’ll see that, if you use the logger module, the default logs of Dramatiq will be correctly handled by Loguru.
Finally, we take care of setting the __all__ variable at the end of the module:
logger.py
__all__ = ["logger"]
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
__all__ is a special variable telling Python which variables should be made publicly available when importing this module. Here, we’ll expose logger from Loguru, so we can easily import it everywhere we need in our project.
Bear in mind that it’s not strictly necessary to use __all__: we could very well import logger without it, but it’s a clean way to hide other things we want to keep private, such as InterceptHandler, for example.
Configuring and using a logging facility with Loguru
Finally, we can use it as we saw previously in our code:
logger.py
from chapter15.logger import logger
def secret_header(secret_header: str | None = Header(None)) None: logger.debug("Check secret header") if not secret_header or secret_header != "SECRET_VALUE": logger.warning("Invalid or missing secret header") raise HTTPException(status.HTTP_403_FORBIDDEN)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py
If we run it with Uvicorn, you’ll now see that all our logs are formatted the same way:
2023-03-03 09:06:16.196 | INFO | uvicorn.server:serve:75 - Started server process [47534] - {} 2023-03-03 09:06:16.196 | INFO | uvicorn.lifespan.on:startup:47 - Waiting for application startup. - {} 2023-03-03 09:06:16.196 | INFO | uvicorn.lifespan.on:startup:61 - Application startup complete. - {} 2023-03-03 09:06:16.196 | INFO | uvicorn.server:_log_started_ message:209 - Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) - {} 2023-03-03 09:06:18.500 | DEBUG | chapter15.chapter15_ logs_06:secret_header:7 - Check secret header - {} 2023-03-03 09:06:18.500 | WARNING | chapter15.chapter15_ logs_06:secret_header:9 - Invalid or missing secret header - {} 2023-03-03 09:06:18.500 | INFO | uvicorn.protocols.http.httptools_ impl:send:489 - 127.0.0.1:59542 - "GET /route1 HTTP/1.1" 403 - {}
Great! Now, whenever you need to add logs in your app, all you need to do is to import logger from your logger module.
You now have the basics to add logs to your application, with plenty of options to fine-tune how and where you output them. Logs are very useful for monitoring what your application is doing at a micro-level, operation per operation. Another important aspect of monitoring is to have information at a more general level in order to have big figures and quickly detect if something goes wrong. That’s what we’ll see now with metrics.
Monitoring the Health and Performance of a Data Science System
Adding Prometheus metrics
In the previous section, we saw how logs can help us understand what our program is doing by finely tracing the operations it does over time. However, most of the time, you can’t afford to keep an eye on the logs all day: they are useful for understanding and debugging a particular situation but way less useful for getting global insights to alert you when something goes wrong.
To solve this, we’ll see in this section how to add metrics to our application. Their role is to measure things that matter in the execution of our program: the number of requests made, the time taken to give a response, the number of pending tasks in the worker queue, the accuracy of our ML predictions… Anything that we could easily monitor over time – usually, with charts and graphs – so we can easily monitor the health of our system. We say that we instrument our application.
To achieve this task, we’ll use two widely used technologies in the industry: Prometheus and Grafana.
Understanding Prometheus and the different metrics
Prometheus is a technology to help you instrument your application. It consists of three things:
Libraries for a wide range of programming languages, including Python, to add metrics to an application
A server to aggregate and store those metrics over time
A query language, PromQL, so we can pull data from those metrics into visualization tools
Prometheus has very precise guidelines and conventions about how to define metrics. Actually, it defines four different types of metrics.
The counter metric
The counter metric is a way to measure a value that goes up over time. For example, this could be the number of requests answered or the number of predictions done. This will not be used for values that can go down. For that, there is the gauge metric.
Figure 15.2 – Possible representation of a counter
Adding Prometheus metrics
The gauge metric
The gauge metric is a way to measure a value that can go up or down over time. For example, this could be the current memory usage or the number of pending tasks in a worker queue.
Figure 15.3 – Possible representation of a gauge
The histogram metric
Contrary to counters and gauges, a histogram will measure values and count them in buckets. Typically, if we want to measure the response time of our API, we can count the number of requests that have been processed in less than 10 milliseconds, less than 100 milliseconds, and less than 1 second. Doing this is much more insightful than getting a simple average or median, for example.
When using a histogram, it’s our responsibility to define the buckets we want with their value threshold.
Figure 15.4 – Possible representation of a histogram
Monitoring the Health and Performance of a Data Science System
Prometheus defines a fourth type of metric, a summary. It’s quite similar to the histogram metric, but it works with sliding quantiles instead of defined buckets. We won’t go through it since it has quite limited support in Python. Besides, we’ll see in the Grafana section of this chapter that we’ll be able to compute quantiles with the histogram metric.
You can read more details about those metrics in the official Prometheus documentation:
https://prometheus.io/docs/concepts/metric_types/
Measuring and exposing metrics
Once the metrics have been defined, we can start to measure things during the lifetime of our program. Similar to what we do with logs, metrics expose methods so we can store values during the execution of the application. Prometheus will then retain those values in memory to build the metrics.
But then, how can we access those metrics so we can actually analyze and monitor them? Quite simply, apps using Prometheus usually expose an HTTP endpoint called /metrics, which will return the current values of all metrics in a specific format. You can see what it looks like in Figure 15.5.
Figure 15.5 – Output of a Prometheus metrics endpoint
This endpoint can then be polled at regular intervals by a Prometheus server, which will store those metrics over time and make them available through PromQL.
Adding Prometheus metrics
Metrics are reset when your application restarts It’s worth noting that every time you restart your application, like your FastAPI server, metric values are lost, and you start from zero. It may be a bit surprising, but it’s key to understand that metric values are only stored in memory in your app. The responsibility for properly storing them permanently belongs to the Prometheus server.
Now that we have a good idea of how they work, let’s see how to add metrics to FastAPI and Dramatiq applications.
Adding Prometheus metrics to FastAPI
As we said, Prometheus maintains official libraries for various languages, including Python.
We could very well use it on its own and manually define various metrics to monitor our FastAPI app. We would also need to come up with some logic to hook into a FastAPI request handler so we could measure things such as the requests count, response time, payload size, and so on.
While definitely doable, we’ll take a shortcut and rely once again on the open source community, which proposes a ready-to-use library for integrating Prometheus into a FastAPI project: Prometheus FastAPI Instrumentator. It comes with useful metrics by default, such as the total number of requests or the response size in bytes. It also takes care of exposing the /metrics endpoint.
The first thing is, of course, to install it with pip. Run the following command:
(venv) $ pip install prometheus_fastapi_instrumentator
In the following example, we’ve implemented a very simple FastAPI app and enabled the instrumentator:
chapter15_metrics_01.py
from fastapi import FastAPI from prometheus_fastapi_instrumentator import Instrumentator, metrics
app = FastAPI()
@app.get("/") async def hello(): return {"hello": "world"}
instrumentator = Instrumentator() instrumentator.add(metrics.default()) instrumentator.instrument(app).expose(app)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_01.py
Monitoring the Health and Performance of a Data Science System
Enabling the instrumentator consists of three lines:
1.
Instantiate the Instrumentator class.
2. Enable the default metrics proposed by the library.
3. Wire it to our FastAPI app and expose the /metrics endpoint.
That’s it! FastAPI is instrumented with Prometheus!
Let’s run this app with Uvicorn and access the hello endpoint. Internally, Prometheus will measure things about this request. Let’s now access /metrics to see the result. If you scroll down this big list of metrics, you should come across these lines:
# HELP http_requests_total Total number of requests by method, status and handler. # TYPE http_requests_total counter http_requests_total{handler="/",method="GET",status="2xx"} 1.0
This is the metrics counting the number of requests. We see that we have one request in total, which corresponds to our call to hello. Notice that the instrumentator is smart enough to label the metrics by path, method, and even status code. This is very convenient, as it’ll enable us to pull interesting figures depending on the characteristics of the request.
Adding custom metrics
The built-in metrics are a good start, but we’ll likely need to come up with our own to measure things specific to our application.
Let’s say we want to implement a function that rolls a dice with six faces and exposes it via a REST API. We want to define a metric allowing us to count the number of times each face has appeared. For this task, a counter is a good match. Let’s see how to declare it in the code:
chapter15_metrics_02.py
DICE_COUNTER = Counter( "app_dice_rolls_total", "Total number of dice rolls labelled per face", labelnames=["face"], )
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_02.py
Adding Prometheus metrics
We have to instantiate a Counter object. The two first arguments are, respectively, the name and description of the metric. The name will be used by Prometheus to uniquely identify this metric. Since we want to count the rolls per face, we also add a single label named face. Every time we count a roll of the dice, we’ll have to set this label to the corresponding result face.
Conventions for metric names Prometheus defines very precise conventions for naming your metrics. In particular, it should start with the domain the metrics belong to, such as http_ or app_, and should end with the unit, such as _seconds, _bytes, or _total if this is just a value count. We strongly recommend you read the Prometheus guidelines: https://prometheus.io/docs/ practices/naming/.
We can now use this metric in our code. In the following snippet, you’ll see the implementation of the roll_dice function:
chapter15_metrics_02.py
def roll_dice() -> int: result = random.randint(1, 6) DICE_COUNTER.labels(result).inc() return result
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_02.py
You can see that we directly use the metrics instance, DICE_COUNTER, and first call the labels method to set the face, and then inc to actually increment the counter.
That’s all we need to do: our metric is automatically registered in the Prometheus client and will start to be exposed by the /metrics endpoint. In Figure 15.6, you can see a possible visualization of this metric in Grafana.
Monitoring the Health and Performance of a Data Science System
Figure 15.6 – Representation of the dice roll metric in Grafana
As you can see, declaring and using a new metric is quite straightforward: we can just call it directly in the code we want to monitor.
Handling multiple processes
In Chapter 10, we mentioned in the Adding Gunicorn as a server process for deployment section that, in a production deployment, FastAPI apps are usually run with several workers. Basically, it spawns several processes of the same application and balances the incoming requests between them. This allows us to serve more requests concurrently and avoid blocks if one of the operations is blocking the process.
Do not confuse Gunicorn workers and Dramatiq workers When we talk about workers in the context of a Gunicorn deployment for FastAPI, we are referring to the fact that we are spawning multiple processes that’ll be able to serve our API requests concurrently. We are not talking about workers in the context of Dramatiq that are processing tasks in the background.
Having multiple processes for the same application is a bit problematic for Prometheus metrics. Indeed, as we mentioned before, those metrics are only stored in memory and exposed through a / metrics endpoint.
Adding Prometheus metrics
If we have several processes answering requests, each one will have its own set of metrics values. Then, when the Prometheus server asks for /metrics, we’ll get the values of the process that answered our request but not the ones of the others. And it may change in the next poll! Obviously, this will totally defeat our initial goal.
To circumvent this, the Prometheus client has a special multiprocess mode. Basically, instead of storing the values in memory, it’ll store them in files in a dedicated folder. When calling /metrics, it’ll take care of loading all the files and reconciling the values of all processes together.
Enabling this mode requires us to set the environment variable called PROMETHEUS_MULTIPROC_ DIR. It should point to a valid folder in your filesystem where the metrics files will be stored. Here is a command example of how to set this variable and start Gunicorn with four workers:
(venv) $ PROMETHEUS_MULTIPROC_DIR=./prometheus-tmp gunicorn -w 4 -k uvicorn.workers.UvicornWorker chapter15.chapter15_metrics_01:app
Of course, in a production deployment, you would set the environment variable globally on your platform, as we explained in Chapter 10.
If you try this command, you’ll see that Prometheus will start to store some .db files inside the folder, each one corresponding to a metric and a process. The side effect is that metrics won’t be cleared when restarting the process. It can lead to unexpected behaviors if you change your metrics definition or if you run a completely different application. Make sure to choose a dedicated folder for each of your apps and clean it up when you run a new version.
We are now able to precisely instrument a FastAPI app. However, we saw in the previous chapter that data science applications can be constituted of a separate worker process, where a lot of logic and intelligence is run. Thus, it’s also crucial to instrument this part of the application.
Adding Prometheus metrics to Dramatiq
In Chapter 14, we implemented a complex application with a distinct worker process that was in charge of loading and executing the Stable Diffusion model to generate images. Hence, this part of the architecture is critical and needs to be monitored to be sure everything is going well.
In this section, we’ll see how to add Prometheus metrics to a Dramatiq worker. The good news is that Dramatiq already comes with built-in metrics and exposes the /metrics endpoint by default. Really, there is nothing much to do!
Let’s take a very basic example of a Dramatiq worker with a dummy task:
chapter15_metrics_03.py
import time
import dramatiq
Monitoring the Health and Performance of a Data Science System
from dramatiq.brokers.redis import RedisBroker
redis_broker = RedisBroker(host="localhost") dramatiq.set_broker(redis_broker)
@dramatiq.actor() def addition_task(a: int, b: int): time.sleep(2) print(a + b)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_03.py
As you probably understand by now, Dramatiq is by nature a multiprocessing program: it spawns several workers to handle tasks concurrently. As such, we need to make sure Prometheus is in multiprocessing mode, as we mentioned in the Handling multiple processes section. Thus, we’ll need to set the PROMETHEUS_MULTIPROC_DIR environment variable, as we explained earlier, but also dramatiq_prom_db. Indeed, Dramatiq implements its own mechanism to enable Prometheus’s multiprocessing mode, which should work out of the box, but it turns out, in our experience, that it’s better to be explicit about it.
The following command shows you how to start our worker with PROMETHEUS_MULTIPROC_DIR and dramatiq_prom_db set:
(venv) $ PROMETHEUS_MULTIPROC_DIR=./prometheus-tmp-dramatiq dramatiq_ prom_db=./prometheus-tmp-dramatiq dramatiq chapter15.chapter15_ metrics_03
To allow you to schedule a task easily in this worker, we’ve added a small __name__ == "__main__" instruction. In another terminal, run the following command:
(venv) $ python -m chapter15.chapter15_metrics_03
It’ll schedule a task in the worker. You’ll probably see it being executed in the worker logs.
Now, try to open the following URL in your browser: http://localhost:9191/metrics. You’ll see a result similar to what we show in Figure 15.7.
Adding Prometheus metrics
Figure 15.7 – Output of a Dramatiq Prometheus metrics endpoint
We already see several metrics, including a counter for the total number of messages processed by Dramatiq, a histogram to measure the execution time of our tasks, and a gauge to measure the number of tasks currently in progress. You can review the complete list of metrics included by Dramatiq in its official documentation: https://dramatiq.io/advanced.html#prometheus-metrics.
Adding custom metrics
Of course, as for FastAPI, we would probably like to add our own metrics to the Dramatiq worker. Actually, this is very similar to what we saw in the previous section. Let’s again take the dice roll example:
chapter15_metrics_04.py
DICE_COUNTER = Counter( "worker_dice_rolls_total", "Total number of dice rolls labelled per face", labelnames=["face"], )
@dramatiq.actor() def roll_dice_task(): result = random.randint(1, 6) time.sleep(2)
Monitoring the Health and Performance of a Data Science System
DICE_COUNTER.labels(result).inc() print(result)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_ metrics_04.py
All we needed to do was to create our Counter object, as we did before, and use it in our task. If you try to run the worker and request the /metrics endpoint, you’ll see this new metric appear.
We are now able to instrument our FastAPI and Dramatiq apps. As we have already mentioned several times, we now need to aggregate those metrics in a Prometheus server and visualize them in Grafana. That’s what we’ll look at in the next section.
Monitoring metrics in Grafana
Having metrics is nice, but being able to visualize them is better! In this section, we’ll see how we can collect Prometheus metrics, send them to Grafana, and create dashboards to monitor them.
Grafana is an open source web application for data visualization and analytics. It’s able to connect to various data sources, such as timeseries databases and, of course, Prometheus. Its powerful query and graph builder allows us to create detailed dashboards where we can monitor our data in real time.
Configuring Grafana to collect metrics
Since it’s open source, you can run it from your own machine or server. Detailed instructions are available in the official documentation: https://grafana.com/docs/grafana/latest/ setup-grafana/installation/. However, to speed things up and get you started quickly, we’ll rely here on Grafana Cloud, an official hosting platform. It offers a free plan, which should be enough for you to get started. You can create your account here: https://grafana.com/auth/ sign-up/create-user. Once done, you’ll be asked to create your own instance, a “Grafana Stack,” by choosing a subdomain and a data center region, as you can see in Figure 15.8. Choose a region close to your geographic location.
Monitoring metrics in Grafana
Figure 15.8 – Instance creation on Grafana Cloud
You’ll then be presented with a set of common actions to get started with Grafana. The first thing we’ll do is add Prometheus metrics. Click on Scale and centralize existing data, then Hosted Prometheus metrics. You’ll be taken to a page to configure a Prometheus metrics collection. Click on the tab named Configuration Details at the top. The page will look like the one shown in Figure 15.9.
Figure 15.9 – Hosted Prometheus metrics configuration on Grafana
Monitoring the Health and Performance of a Data Science System
You see that we have two ways to forward metrics: via Grafana Agent or via a Prometheus server.
As we mentioned earlier, a Prometheus server is responsible for collecting metrics for all our apps and storing the data in a database. It’s the standard way to do it. You can find instructions on how to install it in the official documentation: https://prometheus.io/docs/prometheus/ latest/installation/. Bear in mind, though, that it’s a dedicated application server that’ll need proper backups, as it’ll store all your metrics data.
The most straightforward way is to use Grafana Agent. It consists of a small command-line program with a single configuration file. When it runs, it’ll poll the metrics of each of your apps and send the data to Grafana Cloud. All the data is stored on Grafana Cloud, so nothing is lost, even if you stop or delete the agent. This is what we’ll use here.
Grafana shows you commands on the page to download, unzip, and execute the Grafana Agent program. Execute those commands so you have it at the root of your project.
Then, in the last step, you’ll have to create an API token so Grafana Agent can send data to your instance. Give it a name and click on Create API Token. A new text area will appear with a new command to create the agent’s configuration file, as you can see in Figure 15.10.
Figure 15.10 – Command to create Grafana Agent configuration
Monitoring metrics in Grafana
Execute the ./grafana-agent-linux-amd64 –config.file=agent-config.yaml command. A file named agent-config.yaml will be created in your project. We now have to edit it so we can configure our actual FastAPI and Dramatiq applications. You can see the result in the following snippet:
agent-config.yaml
metrics: global: scrape_interval: 60s configs: - name: hosted-prometheus scrape_configs: - job_name: app static_configs: - targets: ['localhost:8000'] - job_name: worker static_configs: - targets: ['localhost:9191'] remote_write: - url: https://prometheus-prod-01-eu-west-0.grafana.net/api/ prom/push basic_auth: username: 811873 password: __YOUR_API_TOKEN__
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter15/agent-con- fig.yaml
It’s a YAML configuration file where we can set the various options for Grafana Agent. The most important part is the scrape_configs key. As you can see, we can define the list of all the apps we want to gather the metrics for and specify their hostname, the “target”: localhost:8000 for the FastAPI app and localhost:9191 for the Dramatiq worker. Of course, this configuration is valid for local development, but you’ll have to adapt it with the proper hostnames of your apps in a production deployment.
We are now ready to start Grafana Agent and collect the metrics! Make sure your FastAPI and Dramatiq apps are running, and then run Grafana Agent. Depending on your system, the name of the executable will vary, but it’ll look similar to this:
$ ./grafana-agent-linux-amd64 --config.file=agent-config.yaml
Grafana Agent will start and will collect the metrics at regular intervals before sending them to Grafana. We’re now ready to plot some data!
Monitoring the Health and Performance of a Data Science System
Visualizing metrics in Grafana
Our metrics data is now sent to Grafana. We’re ready to query it and build some graphs. The first step is to create a new dashboard, a place where you’ll be able to create and organize multiple graphs. Click on the plus button at the top right and then New dashboard.
A new blank dashboard will appear, as you can see in Figure 15.11.
Figure 15.11 – Create a new dashboard in Grafana
Click on Add a new panel. The interface to build a new graph will appear. There are three main parts:
The graph preview at the top left. When starting, it’s empty.
The query builder at the bottom left. This is where we’ll query the metrics data.
The graph settings on the right. This is where we’ll choose the type of graph and finely configure its look and feel, similar to what we have in spreadsheet software.
Let’s try to create a graph for the duration of HTTP requests in our FastAPI app. In the select menu called Metric, you’ll have access to all the Prometheus metrics that have been reported by our apps. Select http_request_duration_seconds_bucket. This is the histogram metric defined by default by Prometheus FastAPI Instrumentator to measure the response time of our endpoints.
Then, click on Run queries. Under the hood, Grafana will build and execute PromQL queries to retrieve the data.
At the top right of the graph, let’s select a shorter time span, such as Last 15 minutes. Since we do not have much data yet, we’ll have a clearer view if we look at only a few minutes of data instead of hours. You should see a graph similar to the one in Figure 15.12.
Monitoring metrics in Grafana
Figure 15.12 – Basic plot of a histogram metric in Grafana
Grafana has plotted several series: for each handler (which corresponds to the endpoint pattern), we have several buckets, le. Each line roughly represents the number of times we answered “handler” in less than “le” seconds.
This is the raw representation of the metric. However, you probably see that it’s not very convenient to read and analyze. It would be better if we could look at this data another way, in terms of response time, arranged by quantiles.
Fortunately, PromQL includes some math operations so we can arrange the raw data. The part below the Metric menu allows us to add those operations. We can even see that Grafana suggests we use add histogram_quantile. If you click on this blue button, Grafana will automatically add three operations: a Rate, a Sum by le, and finally, a Histogram quantile, set by default to 0.95.
By doing this, we’ll now have a view of the evolution of our response time: 95% of the time, we answer in less than x seconds.
Monitoring the Health and Performance of a Data Science System
The default y axis unit is not very convenient. Since we know we work with seconds, let’s select this unit in the graph options. On the right, look for the Standard options part and, in the Unit menu, look for seconds (s) under the Time group. Your graph will now look like Figure 15.13.
Figure 15.13 – Quantile representation of a histogram metric in Grafana
Now it’s much more insightful: we can see that we answer nearly all our requests (95%) in under 100 milliseconds. If our server starts to slow down, we’ll immediately see an increase in our graph, which could alert us that something has gone wrong.
If we want to have other quantiles on the same graph, we can duplicate this query by clicking on the Duplicate button right above Run queries. Then, all we have to do is to select another quantile. We show the result with quantiles 0.95, 0.90, and 0.50 in Figure 15.14.
Monitoring metrics in Grafana
Figure 15.14 – Several quantiles on the same graph in Grafana
The legend can be customized Notice that the name of the series in the legend can be customized. Under the Options part of each query, you can customize it at will. You can even include dynamic values coming from the query, such as metrics labels.
Finally, we can give a name to our graph by setting Panel title, in the right column. Now that we’re happy with our graph, we can click on Apply at the top right to add it to our dashboard, as we see in Figure 15.15.
Figure 15.15 – Grafana dashboard
Monitoring the Health and Performance of a Data Science System
That’s it! We can start to monitor our application. You can resize and position each panel at will. You can set the query time span you want to look at and even enable auto-refresh so the data gets updated in real time! Don’t forget to click on the Save button to save your dashboard.
We can build a similar graph with the exact same configuration to monitor the time needed to execute tasks in Dramatiq, thanks to the metric named dramatiq_message_duration_ milliseconds_bucket. Notice that this one is expressed in milliseconds instead of seconds, so you should be careful when selecting the unit of your graph. We see here one of the benefits of the Prometheus naming convention for metrics!
Adding a bar chart graph
There are a lot of different types of graphs available in Grafana. For example, we could plot our dice roll metric in the form of a bar chart, where each bar represents the number of times a face has been seen. Let’s try it: add a new panel and select the app_dice_rolls_total metric. You’ll see something similar to what is shown in Figure 15.6.
Figure 15.16 – Default representation of a counter metric with a bar chart in Grafana
We do have a bar for each face, but there is something strange: there are bars for each point in time. That’s a key thing to understand with Prometheus metrics and PromQL: all metrics are stored as time series. This allows us to go back in time and see the evolution of the metrics over time.
Monitoring metrics in Grafana
However, for some representations, like the one shown here, it’s not really insightful. For this case, it would be better to show us the latest values for the time span we selected. We can do this by setting Type to Instant under the Options part of the metric panel. We’ll see that we now have a single graph with a single point in time, as you can see in Figure 15.17.
Figure 15.17 – Counter metric configured as Instant in Grafana
It’s better, but we can go further. Typically, we would like the x axis to show the face labels instead of the point in time. First, let’s customize the legend with a Custom label and type {{face}}. The legend will now only show the face label.
Now, we’ll transform the data so the x axis is the face label. Click on the Transform tab. You’ll see a list of functions that can be applied by Grafana to your data before visualizing it. For our case here, we’ll choose Reduce. The effect of this function is to take each series, take a specific value from it, and plot it on the x axis. By default, Grafana will take the maximum value, Max, but there are other choices, such as Last, Mean, or StdDev. In this context, they won’t make a difference since we already queried the instant value.
That’s it! Our graph now shows the number of times we’ve seen a face. This is the one we showed in Figure 15.6 earlier in the chapter.
Monitoring the Health and Performance of a Data Science System
Summary
Congratulations! You are now able to report metrics and build your own dashboards in Grafana to monitor your data science applications. Over time, don’t hesitate to add new metrics or complete your dashboards if you notice some blind spots: the goal is to be able to watch over every important part at a glance so you can quickly take corrective actions. Those metrics can also be used to drive the evolution of your work: by monitoring the performance and accuracy of your ML models, you can track the effects of your changes and see whether you are going in the right direction.
This is the end of this book and our FastAPI journey. We sincerely hope that you liked it and that you learned a lot along the way. We’ve covered many subjects, sometimes just by scratching the surface, but you should now be ready to build your own projects with FastAPI and serve smart data science algorithms. Be sure to check all the external resources we proposed along the way, as they will give you all the insights you need to master them.
In recent years, Python has gained a lot of popularity, especially in data science communities, and the FastAPI framework, even though still very young, is already a game-changer and has seen an unprecedented adoption rate. It’ll likely be at the heart of many data science systems in the coming years... And as you read this book, you’ll probably be one of the developers behind them. Cheers!
Symbols
args syntax
using 27, 28 **kwargs syntax using 27, 28
== None 22
A
access token
endpoints, securing with 189 generating 184
aggregating operations, NumPy
reference link 279
Amazon ECR
reference link 260
Amazon Elastic Container Service
reference link 261
Amazon RDS
reference link 256
Amazon Web Services (AWS) 214, 344 Any annotation
using 49
apt 4
Index
arrays
adding 278 aggregating 279 comparing 279 creating, with NumPy 272-274 manipulating 270, 271 manipulating, with NumPy 276, 277 multiplying 278
asynchronous generator 170 asynchronous I/O
working with 51-54
Asynchronous Server Gateway
Interface (ASGI) 52, 57
automatic interactive documentation 57 Azure Database for PostgreSQL
reference link 256
B
background operations 325 backpressure 315 Boolean logic
membership operators, reviewing 22, 23 performing 21 variable similarity, checking 22
brew 4
broadcasting 278
reference link 279
bucket 346 built-in types, Python 16, 17
reference link 21
conditional statements elif statement 23, 24 else statement 23, 24 for loop statement 24, 25 if statement 23, 24 while loop statement 25, 26
C
containers 256 context
caching
implementing, with Joblib 301-303 standard or async functions,
selecting between 304-306
Callable class
adding, to logs 361 context manager 151 cookies 76, 78, 178, 190 coroutines 52 CORS
type signature, using with 48, 49
camel case 35 cast function using 50 central logger
Loguru, configuring as 362-365
configuring 190 configuring, in FastAPI 191-196
counter metric 366 cross-origin HTTP requests 191 Cross-Site Request Forgery (CSRF) 191 cross-validation 269
classification problems 268 class inheritance
used, for creating model variations 110-112
models, validating with 292, 293 cryptographic hash functions 181 CSRF attacks
class methods
preventing, with double-submit
using, as dependencies 131, 132
cookies 196-200
class properties 35 cloud providers, documentation pages
protecting against 190
CSV data
references 254, 255
clustering 268 collections 144 collections.abc module 48 columns 142 computer vision 308 computer vision model
exporting 285 importing 284, 285
cURL 9 custom data validation with Pydantic
validation, applying at field level 112, 113 validation, applying at object level 113, 114 validation, applying before
using, with Hugging Face 308-312
Pydantic parsing 114, 115
concurrency
custom response 94, 95
handling, in WebSocket 209, 210
Conda 8
building 90 file, serving 93, 94
redirection, making 92 response_class argument, using 91, 92
dimensionality reduction 268 Docker 256
FastAPI application, deploying with 256
D
Dockerfile 256
writing 257, 258
DALL-E 326 dashboard 380 database access token
implementing 184, 185
Docker image building 259 deploying 260 running, locally 259
databases
selecting, factors 144, 145 testing with 235-241
document 143 document-oriented databases 143-145 double-submit cookies
dataset loading utilities, scikit-learn
implementing, to prevent CSRF
reference link 286
attacks 196-200
data, sharing between worker and API 338 API, adapting to save image-generation
tasks in database 339, 340
SQLAlchemy model, defining 338, 339 worker, adapting to read and update image- generation tasks in database 340-343
Dramatiq worker creating 333, 334 starting 335 tasks, scheduling 336
dumped model
loading 297, 298
data structures, Python
dictionary 20 lists 17, 18 sets 20, 21 tuples 18-20 decorator 56 default values 105, 106 dependencies 125
dumping 296 dynamic default values 107
E
eager loading 161 efficient prediction endpoint
implementing 298-301
404 error, raising 129 class methods, using as 131, 132 object, obtaining 128 using, in WebSocket 211-213 using, on path decorator 133, 134 using, on whole application 136 using, on whole router 134, 135 dependency injection 50, 123-125 dictionaries 20
ellipsis syntax 63 email addresses
validating, with Pydantic types 108-110
email notifications 325 endpoint
creating 56 running, locally 56 securing, with access tokens 189
Pydantic objects, converting into 115-117
environment variables
file uploads
setting 246-249 setting, with .env file 249, 250 using 246-249 estimators 286
handling 73-76
first in, first out (FIFO) strategy 315 five-fold cross-validation 269 fixtures
chaining, with pipelines 288-292
creating, for reusing test logic 226-228
event loop 51
F
FastAPI 9, 55, 56
CORS, configuring 191-196 security dependencies 178-180 WebSockets, creating with 205-208
foreign key 143 for loop statement 24 form data 72, 73 forward reference 158 f-strings 15 function dependency
creating 125 using 126, 127
FastAPI application
functions
database servers, adding 256 deploying, on serverless platform 253-255 deploying, on traditional server 261, 262
defining 26, 27 defining, with *args and **kwargs 27, 28
FastAPI application, deploying
with Docker 256
G
Dockerfile, writing 257, 258 Docker image, building 259 Docker image, deploying 260 Docker image, running locally 259, 260 prestart script, writing 258, 259
gauge metric 367 generator functions 33 generators 33, 34 generic CamelCase types
reference link 147
FastAPI, with HTTPX
testing tools, setting up 228-232
generics 45 Google Artifact Registry
features 268 Field customization, Pydantic
reference link 107 file-like interface 74 files, storing and serving in object
storage 344, 345
reference link 260
Google Cloud Platform (GCP) 214 Google Cloud Run reference link 261 Google Cloud SQL reference link 256
image-generation system, running 349-352 object storage helper, implementing 345-348 object storage helper, using in worker 348 pre-signed URL, generating
on server 348, 349
Grafana 376
bar chart graph, adding 384, 385 configuring, to collect metrics 376-379 metrics, monitoring in 376 metrics, visualizing in 380-384
Gunicorn
integrated development
adding, as server process for deployment 252, 253
environment (IDE) 7
is None 22 iterator 24
H
hashing passwords 182, 183 headers 76, 77 Heroku Postgres
reference link 256 histogram metric 367 holdout set 269 Homebrew package
URL 3
J
Joblib 296
results, caching with 301-303 trained model, persisting with 296
join query 143
K
HTTP authentication 178 HTTP errors
keyword arguments 27
raising 88-90
HTTPie command-line utility
L
installing 9-12 Hugging Face 307
computer vision model, using with 308-312 URL 308
label 268 lazy loading 161 list comprehensions 31, 32 lists 17
I
idiomatic constructions
immutable 18 mutable 18
logging module 355 reference link 355
generators 33, 34 list comprehensions 31, 32
login endpoint
implementing 186-188
image-generation system
logs 354, 359
running 349-352 image-generation task
REST API, implementing 336, 337
image processor 310 indexing, pandas
adding, with Loguru 355-357 as JSON objects 361 context, adding to 361 levels 355 Loguru 355
reference link 282
inheritance
used, for avoiding repetition 40, 41
configuring, as central logger 362-365 logs, adding with 355-357 reference link 359
M
mixins 42 models 310
machine learning (ML) 267, 268
model validation 268, 269 supervised learning 268 unsupervised learning 268
magic methods
creating 181, 182 training, with scikit-learn 285-288 validating, with cross-validation 292, 293
model validation 268, 269 model variations
__call__ method 39, 40 __eq__ method 38, 39 __gt__ method 38, 39 implementing 36 __lt__ method 38, 39 operators 39 __repr__ method 37 __str__ method 37
creating, with class inheritance 110-112
modules
using 28, 29
MongoDB
reference link 173
Motor, used for communication with
MongoDB database 166
mapped_column arguments
reference link 147
marker 225 masking 284 message brokers 214 Method Resolution Order (MRO) 43 metric names
database connection 167, 168 documents, deleting 172, 173 documents, inserting 168, 169 documents, nesting 173, 174 documents, obtaining 169-172 documents, updating 172, 173 models compatible with MongoDB,
creating 166, 167
conventions 371
multi-dimensional data
metrics 366
pandas DataFrames, using for 282-284
exposing 368 measuring 368 monitoring, in Grafana 376 visualizing, in Grafana 380-384
multiple WebSocket connections
handling 213-219 messages, broadcasting 213-219
mypy
metrics, Prometheus counter metric 366 gauge metric 367 histogram metric 367 reference link 368
Microsoft Azure Container Instances
reference link 45 used, for type checking 45 used, for type hinting 43, 45
N
reference link 261
Microsoft Azure Container Registry
reference link 260
Midjourney 326
namespace package reference link 30 negative indexing 17 NoSQL databases 142-144
NumPy
pandas Series
arrays, manipulating with 276, 277 working 271 NumPy arrays adding 278 creating 272-274 elements, accessing 274-276 multiplying 278 sub-arrays 274-276
NumPy user guide reference link 280
using, for one-dimensional data 280-282
parameterized dependency
creating 129 using 130, 131
parametrize
tests, generating with 224-226
pass statement 41 path 56 path operation function 56 path operation parameters 79
O
response model 81-83 status code 79-81 path parameters 59-61
object detection results
displaying, in browser 320-324 object-oriented programming 35
advanced validation 63-65 allowed values, limiting 62, 63
pip
class, defining 35, 36 inheritance, used for avoiding
Python packages, installing with 8
pipelines 288
repetition 40, 41
magic methods, implementing 36 multiple inheritance 42, 43
object storage 344 one-dimensional data
estimators, chaining with 288-292 preprocessors, chaining with 288-292
Pipenv 8 Poetry 8 POST endpoints
pandas Series, using for 280-282
tests, writing for 233, 234
operations, pandas reference link 284
optional fields 105, 106
Postman 9 preflight requests 194 preprocessors
chaining, with pipelines 288-292
P
package managers 4 packages
using 28-30
pandas 280 pandas DataFrames
primary key 143 private methods 36 Prometheus 366 Prometheus FastAPI Instrumentator 369 Prometheus metrics 366 Prometheus metrics, adding to Dramatiq 373-375
using, for multi-dimensional data 282-284
custom metrics, adding 375, 376
Prometheus metrics, adding to
FastAPI 369, 370
custom metrics, adding 370, 371 multiple processes, handling 372, 373 publish-subscribe (pub-sub) pattern 214 Pydantic 99 URL 59 used, for adding custom data validation 112
indentation 15, 16 key aspects 14 modules, using 29 packages, structuring 29, 30 packages, using 28 scripts, running 14
Python scripts
running 14, 15
Pydantic models 68, 217 Pydantic objects
Python virtual environment
creating 7, 8
converting, into dictionary 115-117 instance, creating from sub-class
object 117-119
Q
instance, updating partially 119, 120 working with 115
query parameters 65-67 queue 331
Pydantic types
reference link 109 used, for validating email addresses 108-110 used, for validating URLs 108-110
pyenv
R
Redis 215 URL 215
reference link 4 used, for installing Python distribution 4-6
pytest
used, for unit testing 222
Python 3.10 6 Python 3.11 6 Python dependencies managing 250-252 Python distribution
registration routes
implementing 183, 184 regression problems 268 relational databases 142-144 relationships 142 representational state transfer (REST) API 9, 59, 336
request body 67-69
multiple objects 69, 70
installing, with pyenv 4, 5, 6 Python Package Index (PyPi)
request object 78, 79 Request object from Starlette
URL 7
Python programming
reference link 79 request parameters
Boolean logic, performing 21 built-in types 16, 17 data structures 17 flow, controlling 23 functions, defining 26, 27
handling 59 path parameters 59-61 query parameters 65-67
response
customizing 79
response parameter 84 cookies, setting 85, 86 headers, setting 84, 85 status code, setting dynamically 87, 88
REST API endpoints
tests, writing for 232, 233
REST endpoint
implementing, to perform object
detection on single image 312-314
database migration system, setting up with Alembic 161-165
objects, deleting 155, 156 objects, filtering 153-155 objects, gathering 153-155 objects, inserting into database 152, 153 objects, updating 155, 156 ORM models, defining 146-148 Pydantic models, defining 148, 149 relationships, adding 157-161
routers 95
S
project, structuring 95-97
square brackets 17 Stable Diffusion 325, 326 Stable Diffusion, used for generating images from text prompts
same-origin policy 191 schemas 148 scikit-learn 285
model implementation, in Python script 327-329
Python script, executing 329-331
models, training with 285-288
security dependencies, FastAPI 178-180 serverless platform
standard field types 100-104 Starlette
URL 59
FastAPI application, deploying on 253-255
sessions 150 sets 20, 21 shell 5 singular body values 70 sinks
configuring 357-359 sized aliases, NumPy reference link 273
startup event 152 static type checkers 43 status code 79, 80 stop words 290 structured logging 360 supervised learning 268 Swagger
URL 58
snake case 27 sockets 204 SQLAlchemy Core 145 SQLAlchemy ORM model
creating 181
SQLAlchemy ORM, used for
T
tables 142 tensors 310 Term Frequency-Inverse Document Frequency (TF-IDF) 290
communication with SQL database
reference link 290
database connection 149-152
tests
URLs
and global fixtures, organizing 231 generating, with parametrize 224-226 logic, reusing by creating fixtures 226-228 writing, for POST endpoints 233, 234 writing, for REST API endpoints 232, 233 writing, for WebSocket endpoints 241-243
validating, with Pydantic types 108-110
user
retrieving 184
user agent 78 user and password
storing, securely in database 181
tokens 178 traditional server
Uvicorn 57 Uvicorn, as process manager
FastAPI application, deploying on 261, 262
reference link 253
trained model
dumping 296, 297 persisting, with Joblib 296
V
transformers library 308 tuples 18-20 two-fold cross-validation 269 two-way communication principles
virtual environments 7
W
with WebSockets 204
web-queue-worker
type annotations 43 working 44, 45
type checking
architecture 325, 331, 332
Web Server Gateway Interface (WSGI) 51 WebSocket endpoints
with mypy 45
tests, writing for 241-243
type data structures 45-47 type hinting
with mypy 43-45
type signature
using, with Callable 48, 49
WebSockets
concurrency, handling 209, 210 creating, with FastAPI 205-208 dependencies, using 211-213 implementing, to perform object detection
on stream of images 314-317
U
multiple WebSocket connections,
handling 213-219
unit testing, with pytest 222-224 test logic, reusing by creating
stream of images, sending from
browser 317-320
fixtures 226-228
two-way communication principles 204
tests, generating with parametrize 224-226 Universally Unique IDentifier (UUID) 334 unpacking syntax 20 unsupervised learning 268
whitespace indentation 15 Windows Subsystem for Linux (WSL)
reference link 3 worker 325, 331
Packtpub.com
Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.
Why subscribe?
Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals
Improve your learning with Skill Plans built especially for you
Get a free eBook or video every month
Fully searchable for easy access to vital information
Copy and paste, print, and bookmark content
Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub. com for more details.
At www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.
Other Books You May Enjoy
If you enjoyed this book, you may be interested in these other books by Packt:
Applied Geospatial Data Science with Python
David S. Jordan
ISBN: 978-1-80323-812-8
Understand the fundamentals needed to work with geospatial data
Transition from tabular to geo-enabled data in your workflows
Develop an introductory portfolio of spatial data science work using Python
Gain hands-on skills with case studies relevant to different industries
Discover best practices focusing on geospatial data to bring a positive change in your environment
Explore solving use cases, such as traveling salesperson and vehicle routing problems
Building Data Science Solutions with Anaconda
Dan Meador
ISBN: 978-1-80056-878-5
Install packages and create virtual environments using conda
Understand the landscape of open source software and assess new tools
Use scikit-learn to train and evaluate model approaches
Detect bias types in your data and what you can do to prevent it
Grow your skillset with tools such as NumPy, pandas, and Jupyter Notebooks
Solve common dataset issues, such as imbalanced and missing data
Use LIME and SHAP to interpret and explain black-box models
Packt is searching for authors like you
If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.
Share Your Thoughts
Now you’ve finished Building Data Science Applications with FastAPI, Second Edition, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.
Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.
Download a free PDF copy of this book
Thanks for purchasing this book!
Do you like to read on the go but are unable to carry your print books everywhere?
Is your eBook purchase not compatible with the device of your choice?
Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.
Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.
The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily
Follow these simple steps to get the benefits:
1. Scan the QR code or visit the link below
https://packt.link/free-ebook/9781837632749
2. Submit your proof of purchase
3. That’s it! We’ll send your free PDF and other benefits to your email directly



9 Testing an API Asynchronously with pytest and HTTPX
In software development, a significant part of the developer’s work should be dedicated to writing tests. At first, you may be tempted to manually test your application by running it, making a few requests, and arbitrarily deciding that “everything works.” However, this approach is flawed and can’t guarantee that your program works in every circumstance and that you didn’t break things along the way.
That’s why several disciplines have emerged regarding software testing: unit tests, integration tests, end-to-end tests, acceptance tests, and others. These techniques aim to validate the functionality of software from a micro level, where we test single functions (unit tests), to a macro level, where we test a global feature that delivers value to the user (acceptance tests). In this chapter, we’ll focus on the first level: unit testing.
Unit tests are short programs designed to verify that our code behaves the way it should in every circumstance. You may think that tests are time-consuming to write and that they don’t add value to your software, but this will save you time in the long run: first of all, tests can be run automatically in a few seconds, ensuring that all your software works, without you needing to manually go over every feature. Secondly, when you introduce new features or refactor the code, you’re ensuring that you don’t introduce bugs to existing parts of the software. In conclusion, tests are just as important as the program itself, and they help you deliver reliable and high-quality software.
In this chapter, you’ll learn how to write tests for your FastAPI application, both for HTTP endpoints and WebSockets. To help with this, you’ll learn how to configure pytest, a well-known Python test framework, and HTTPX, an asynchronous HTTP client for Python.
In this chapter, we’re going to cover the following main topics:
An introduction to unit testing with pytest
Setting up the testing tools for FastAPI with HTTPX
Writing tests for REST API endpoints
Writing tests for WebSocket endpoints
Testing an API Asynchronously with pytest and HTTPX
Technical requirements
For this chapter, you’ll require a Python virtual environment, just as we set up in Chapter 1, Python Development Environment Setup.
For the Communicating with a MongoDB database using Motor section, you’ll need a running MongoDB server on your local computer. The easiest way to do this is to run it as a Docker container. If you’ve never used Docker before, we recommend that you refer to the Getting started tutorial in the official documentation at https://docs.docker.com/get-started/. Once you have done this, you’ll be able to run a MongoDB server using this simple command:
$ docker run -d --name fastapi-mongo -p 27017:27017 mongo:6.0
The MongoDB server instance will then be available on your local computer at port 27017.
You’ll find all the code examples of this chapter in the dedicated GitHub repository at https:// github.com/PacktPublishing/Building-Data-Science-Applications-with- FastAPI-Second-Edition/tree/main/chapter09.
An introduction to unit testing with pytest
As we mentioned in the introduction, writing unit tests is an essential task in software development to deliver high-quality software. To help us be productive and efficient, a lot of libraries exist that provide tools and shortcuts dedicated to testing. In the Python standard library, a module exists for unit testing called unittest. Even though it’s quite common in Python code bases, many Python developers tend to prefer pytest, which provides a more lightweight syntax and powerful tools for advanced use cases.
In the following examples, we’ll write a unit test for a function called add, both with unittest and pytest, so that you can see how they compare on a basic use case. First, we’ll install pytest:
(venv) $ pip install pytest
Now, let’s see our simple add function, which simply performs an addition:
chapter09_introduction.py
def add(a: int, b: int) -> int: return a + b
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction.py
An introduction to unit testing with pytest
Now, let’s implement a test that checks that 2 + 3 is indeed equal to 5 with unittest:
chapter09_introduction_unittest.py
import unittest
from chapter09.chapter09_introduction import add
class TestChapter09Introduction(unittest.TestCase): def test_add(self): self.assertEqual(add(2, 3), 5)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_unittest.py
As you can see, unittest expects us to define a class inheriting from TestCase. Then, each test lives in its own method. To assert that two values are equal, we must use the assertEqual method.
To run this test, we can call the unittest module from the command line and pass it through the dotted path to our test module:
(venv) $ python -m unittest chapter09.chapter09_introduction_unittest . ---------------------------------------------------------------------- Ran 1 test in 0.000s
OK
In the output, each successful test is represented by a dot. If one or several tests are not successful, you will get a detailed error report for each, highlighting the failing assertion. You can try it by changing the assertion in the test.
Now, let’s write the same test with pytest:
chapter09_introduction_pytest.py
from chapter09.chapter09_introduction import add
def test_add(): assert add(2, 3) == 5
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_pytest.py
Testing an API Asynchronously with pytest and HTTPX
As you can see, it’s much shorter! Indeed, with pytest, you don’t necessarily have to define a class: a simple function is enough. The only constraint to making it work is that the function name has to start with test_. This way, pytest can automatically discover the test functions. Secondly, it relies on the built-in assert statement instead of specific methods, allowing you to write comparisons more naturally.
To run this test, we must simply call the pytest executable with the path to our test file:
(venv) $ pytest chapter09/chapter09_introduction_pytest.py =============== test session starts =============== platform darwin -- Python 3.10.8, pytest-7.2.0, pluggy-1.0.0 rootdir: /Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition, configfile: pyproject.toml plugins: asyncio-0.20.2, cov-4.0.0, anyio-3.6.2 asyncio: mode=strict collected 1 item
chapter09/chapter09_introduction_pytest.py . [100%]
================ 1 passed in 0.01s ===============
Once again, the output represents each successful test with a dot. Of course, if you change the test to make it fail, you’ll get a detailed error for the failing assertion.
It’s worth noting that if you run pytest without any arguments, it’ll automatically discover all the tests living in your project, as long as their name starts with test_.
Here, we made a small comparison between unittest and pytest. For the rest of this chapter, we’ll stick with pytest, which should give you a more productive experience while writing tests.
Before focusing on FastAPI testing, let’s review two of the most powerful features of pytest: parametrize and fixtures.
Generating tests with parametrize
In our previous example, with the add function, we only tested one addition test, 2 + 3. Most of the time, we’ll want to check for more cases to ensure our function works in every circumstance. Our first approach could be to add more assertions to our test, like so:
def test_add(): assert add(2, 3) == 5 assert add(0, 0) == 0 assert add(100, 0) == 100 assert add(1, 1) == 2
An introduction to unit testing with pytest
While working, this method has two drawbacks: first, it may be a bit cumbersome to write the same assertion several times with only some parameters changing. In this example, it’s not too bad, but tests can be way more complex, as we’ll see with FastAPI. Second, we still only have one test: the first failing assertion will stop the test and the following ones won’t be executed. Thus, we’ll only know the result if we fix the failing assertion first and run the test again.
To help with this specific task, pytest provides the parametrize marker. In pytest, a marker is a special decorator that’s used to easily pass metadata to the test. Special behaviors can then be implemented, depending on the markers used by the test.
Here, parametrize allows us to define several sets of variables that will be passed as arguments to the test function. At runtime, each set will generate a new and independent test. To understand this better, let’s look at how to use this marker to generate several tests for our add function:
chapter09_introduction_pytest_parametrize.py
import pytest
from chapter09.chapter09_introduction import add
@pytest.mark.parametrize("a,b,result", [(2, 3, 5), (0, 0, 0), (100, 0, 100), (1, 1, 2)]) def test_add(a, b, result): assert add(a, b) == result
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_pytest_parametrize.py
Here, you can see that we simply decorated our test function with the parametrize marker. The basic usage is as follows: the first argument is a string with the name of each parameter separated by a comma. Then, the second argument is a list of tuples. Each tuple contains the values of the parameters in order.
Our test function receives those parameters in arguments, each one named the way you specified previously. Thus, you can use them at will in the test logic. As you can see, the great benefit here is that we only have to write the assert statement once. Besides, it’s very quick to add a new test case: we just have to add another tuple to the parametrize marker.
Now, let’s run this test to see what happens by using the following command:
(venv) $ pytest chapter09/chapter09_introduction_pytest_parametrize.py ================ test session starts ================ platform darwin -- Python 3.10.8, pytest-7.2.0, pluggy-1.0.0 rootdir: /Users/fvoron/Development/Building-Data-Science-Applications- with-FastAPI-Second-Edition, configfile: pyproject.toml
Testing an API Asynchronously with pytest and HTTPX
plugins: asyncio-0.20.2, cov-4.0.0, anyio-3.6.2 asyncio: mode=strict collected 4 items
chapter09/chapter09_introduction_pytest_parametrize.py .... [100%]
================ 4 passed in 0.01s ================
As you can see, pytest executed four tests instead of one! This means that it generated four independent tests, along with their own sets of parameters. If several tests fail, we’ll be informed, and the output will tell us which set of parameters caused the error.
To conclude, parametrize is a very convenient way to test different outcomes when it’s given a different set of parameters.
While writing unit tests, you’ll often need variables and objects several times across your tests, such as app instances, fake data, and so on. To avoid having to repeat the same things over and over across your tests, pytest proposes an interesting feature: fixtures.
Reusing test logic by creating fixtures
When testing a large application, tests tend to become quite repetitive: lots of them will share the same boilerplate code before their actual assertion. Consider the following Pydantic models representing a person and their postal address:
chapter09_introduction_fixtures.py
from datetime import date from enum import Enum
from pydantic import BaseModel
class Gender(str, Enum): MALE = "MALE" FEMALE = "FEMALE" NON_BINARY = "NON_BINARY"
class Address(BaseModel): street_address: str postal_code: str city: str country: str
class Person(BaseModel): first_name: str
An introduction to unit testing with pytest
last_name: str gender: Gender birthdate: date interests: list[str] address: Address
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_fixtures.py
This example may look familiar: it was taken from Chapter 4, Managing Pydantic Data Models in FastAPI. Now, let’s say that we want to write tests with some instances of those models. Obviously, it would be a bit annoying to instantiate them in each test, filling them with fake data.
Fortunately, fixtures allow us to write them once and for all. The following example shows how to use them:
chapter09_introduction_fixtures_test.py
import pytest
from chapter09.chapter09_introduction_fixtures import Address, Gender, Person
@pytest.fixture def address(): return Address( street_address="12 Squirell Street", postal_code="424242", city="Woodtown", country="US", )
@pytest.fixture def person(address): return Person( first_name="John", last_name="Doe", gender=Gender.MALE, birthdate="1991-01-01", interests=["travel", "sports"], address=address, )
def test_address_country(address):
Testing an API Asynchronously with pytest and HTTPX
assert address.country == "US"
def test_person_first_name(person): assert person.first_name == "John"
def test_person_address_city(person): assert person.address.city == "Woodtown"
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ introduction_fixtures_test.py
Once again, pytest makes it very straightforward: fixtures are simple functions decorated with the fixture decorator. Inside, you can write any logic and return the object you’ll need in your tests. Here, in address, we instantiate an Address object with fake data and return it.
Now, how can we use this fixture? If you look at the test_address_country test, you’ll see some magic happening: by setting an address argument on the test function, pytest automatically detects that it corresponds to the address fixture, executes it, and passes its return value. Inside the test, we have our Address object ready to use. pytest calls this requesting a fixture.
You may have noticed that we also defined another fixture, person. Once again, we instantiate a Person model with dummy data. The interesting thing to note, however, is that we actually requested the address fixture to use it inside! That’s what makes this system so powerful: fixtures can depend on other fixtures, which can also depend on others, and so on. In some way, it’s quite similar to dependency injection, as we discussed in Chapter 5, Dependency Injection in FastAPI.
With that, our quick introduction to pytest has come to an end. Of course, there are so many more things to say, but this will be enough for you to get started. If you want to explore this topic further, you can read the official pytest documentation, which includes tons of examples showing you how you can benefit from all its features: https://docs.pytest.org/en/latest/.
Now, let’s focus on FastAPI. We’ll start by setting up the tools for testing our applications.
Setting up testing tools for FastAPI with HTTPX
If you look at the FastAPI documentation regarding testing, you’ll see that it recommends that you use TestClient provided by Starlette. In this book, we’ll show you a different approach involving an HTTP client called HTTPX.
Setting up testing tools for FastAPI with HTTPX
Why? The default TestClient is implemented in a way that makes it completely synchronous, meaning you can write tests without worrying about async and await. This might sound nice, but we found that it causes some problems in practice: since your FastAPI app is designed to work asynchronously, you’ll likely have lots of services working asynchronously, such as the database drivers we saw in Chapter 6, Databases and Asynchronous ORMs. Thus, in your tests, you’ll probably need to perform some actions on those asynchronous services, such as filling a database with dummy data, which will make your tests asynchronous anyway. Melding the two approaches often leads to strange errors that are hard to debug.
Fortunately, HTTPX, an HTTP client created by the same team as Starlette, allows us to have a pure asynchronous HTTP client able to make requests to our FastAPI app. To make this approach work, we’ll need three libraries:
HTTPX, the client that will perform HTTP requests
asgi-lifespan, a library for managing the lifespan events of your FastAPI app programmatically
pytest-asyncio, an extension for pytest that allows us to write asynchronous tests
Let’s install those libraries using the following command:
(venv) $ pip install httpx asgi-lifespan pytest-asyncio
Great! Now, let’s write some fixtures so that we can easily get an HTTP test client for a FastAPI application. This way, when writing a test, we’ll only have to request the fixture and we’ll be able to make a request right away.
In the following example, we are considering a simple FastAPI application that we want to test:
chapter09_app.py
import contextlib
from fastapi import FastAPI
@contextlib.asynccontextmanager async def lifespan(app: FastAPI): print("Startup") yield print("Shutdown")
app = FastAPI(lifespan=lifespan)
@app.get("/")
Testing an API Asynchronously with pytest and HTTPX
async def hello_world(): return {"hello": "world"}
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app.py
In a separate test file, we’ll implement two fixtures.
The first one, event_loop, will ensure that we always work with the same event loop instance. It’s automatically requested by pytest-asyncio before executing asynchronous tests. You can see its implementation in the following example:
chapter09_app_test.py
@pytest.fixture(scope="session") def event_loop(): loop = asyncio.new_event_loop() yield loop loop.close()
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_test.py
Here, you can see that we simply create a new event loop before yielding it. As we discussed in Chapter 2, Python Programming Specificities, using a generator allows us to “pause” the function’s execution and get back to the execution of its caller. This way, when the caller is done, we can execute cleanup operations, such as closing the loop. pytest is smart enough to handle this correctly in fixtures, so this is a very common pattern for setting up test data, using it, and destroying it after. We also use the same approach for lifespan functions in FastAPI.
Of course, this function is decorated with the fixture decorator to make it a fixture for pytest. You may have noticed that we set an argument called scope with the session value. This argument controls at which level the fixture should be instantiated. By default, it’s recreated at the beginning of each single test function. The session value is the highest level, meaning that the fixture is only created once at the beginning of the whole test run, which is relevant for our event loop. You can find out more about this more advanced feature in the official documentation: https://docs. pytest.org/en/latest/how-to/fixtures.html#scope-sharing-fixtures- across-classes-modules-packages-or-session.
Setting up testing tools for FastAPI with HTTPX
Next, we’ll implement our test_client fixture, which will create an instance of HTTPX for our FastAPI application. We must also remember to trigger the app events with asgi-lifespan. You can see what it looks like in the following example:
chapter09_app_test.py
@pytest_asyncio.fixture async def test_client(): async with LifespanManager(app): async with httpx.AsyncClient(app=app, base_url="http://app. io") as test_client: yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_test.py
Only three lines are needed. The first difference with fixtures we’ve seen so far is that this is an async function. In this case, notice that we used the @pytest_asyncio.fixture decorator instead of @pytest.fixture. It’s the async counterpart of this decorator provided by pytest-asyncio so async fixtures are correctly handled. In previous versions, using the standard decorator used to work but it’s now discouraged.
Then, we have two context managers: LifespanManager and httpx.AsyncClient. The first one ensures startup and shutdown events are executed, while the second one ensures that an HTTP session is ready. On both of them, we set the app variable: this is our FastAPI application instance we imported from its module, chapter09.chapter09_app import app.
Notice that we once again used a generator here, with yield. This is important because, even if we don’t have any more code after, we need to close the context managers after we use our client. If we used return, Python would have immediately closed them and we would end up with an unusable client.
Organizing tests and global fixtures in projects In larger projects, you’ll likely have several test files to keep your tests organized. Usually, those files are placed in a tests folder at the root of your project. If your test files are prefixed with test_, they will be automatically discovered by pytest. Figure 9.1 shows an example of this.
Besides this, you’ll need the fixtures we defined in this section for all your tests. Rather than repeating them again and again in all your test files, pytest allows you to write global fixtures in a file named conftest.py. After putting it in your tests folder, it will automatically be imported, allowing you to request all the fixtures you define inside it. You can read more about this in the official documentation at https://docs.pytest.org/en/latest/ reference/fixtures.html#conftest-py-sharing-fixtures-across- multiple-files.
Testing an API Asynchronously with pytest and HTTPX
As mentioned previously, Figure 9.1 shows the test files in the tests folder:
Figure 9.1 – Structure of a project with tests
That’s it! We now have all the fixtures ready to write tests for our REST API endpoints. That’s what we’ll do in the next section.
Writing tests for REST API endpoints
All the tools we need to test our FastAPI application are now ready. All these tests boil down to performing an HTTP request and checking the response to see whether it corresponds to what we expect.
Let’s start simply with a test for our hello_world path operation function. You can see it in the following code:
chapter09_app_test.py
@pytest.mark.asyncio async def test_hello_world(test_client: httpx.AsyncClient): response = await test_client.get("/")
assert response.status_code == status.HTTP_200_OK
Writing tests for REST API endpoints
json = response.json() assert json == {"hello": "world"}
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_test.py
First of all, notice that the test function is defined as async. As we mentioned previously, to make it work with pytest, we had to install pytest-asyncio. This extension provides the asyncio marker: each asynchronous test should be decorated with this marker to make it work properly.
Next, we request our test_client fixture, which we defined earlier. It gives us an HTTPX client instance ready to make requests to our FastAPI app. Note that we manually type hinted the fixture. While not strictly required, it’ll greatly help you if you use an IDE such as Visual Studio Code, which uses type hints to provide you with convenient auto-completion features.
Then, in the body of our test, we perform the request. Here, it’s a simple GET request to the / path. It returns an HTTPX Response object (which is different from the Response class of FastAPI) containing all the data of the HTTP response: the status code, the headers, and the body.
Finally, we make assertions based on this data. As you can see, we verify that the status code is indeed 200. We also check the content of the body, which is a simple JSON object. Notice that the Response object has a convenient method called json for automatically parsing JSON content.
Great! We wrote our first FastAPI test! Of course, you’ll likely have more complex tests, typically ones for POST endpoints.
Writing tests for POST endpoints
Testing a POST endpoint is not very different from what we’ve seen earlier. The difference is that we’ll likely have more cases to check whether data validation is working. In the following example, we are implementing a POST endpoint that accepts a Person model in the body:
chapter09_app_post.py
class Person(BaseModel): first_name: str last_name: str age: int
@app.post("/persons", status_code=status.HTTP_201_CREATED) async def create_person(person: Person): return person
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_post.py
Testing an API Asynchronously with pytest and HTTPX
An interesting test could be to ensure that an error is raised if some fields are missing in the request payload. In the following extract, we wrote two tests – one with an invalid payload and another with a valid one:
chapter09_app_post_test.py
@pytest.mark.asyncio class TestCreatePerson: async def test_invalid(self, test_client: httpx.AsyncClient): payload = {"first_name": "John", "last_name": "Doe"} response = await test_client.post("/persons", json=payload)
assert response.status_code == status.HTTP_422_UNPROCESSABLE_ ENTITY
async def test_valid(self, test_client: httpx.AsyncClient): payload = {"first_name": "John", "last_name": "Doe", "age": 30} response = await test_client.post("/persons", json=payload)
assert response.status_code == status.HTTP_201_CREATED
json = response.json() assert json == payload
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter09/chap- ter09_app_post_test.py
The first thing you may have noticed is that we wrapped our two tests inside a class. While not required in pytest, it could help you organize your tests – for example, to regroup tests that concern a single endpoint. Notice that, in this case, we only have to decorate the class with the asyncio marker; it will be automatically applied on single tests. Also, ensure that you add the self argument to each test: since we are now inside a class, they become methods.
These tests are not very different from our first example. As you can see, the HTTPX client makes it very easy to perform POST requests with a JSON payload: you just have to pass a dictionary to the json argument.
Of course, HTTPX helps you build all kinds of HTTP requests with headers, query parameters, and so on. Be sure to check its official documentation to learn more about its usage: https://www. python-httpx.org/quickstart/.
Writing tests for REST API endpoints
Testing with a database
Your application will likely have a database connection to read and store data. In this context, you’ll need to work with a fresh test database in each run to have a clean and predictable set of data to write your tests.
For this, we’ll use two things. The first one, dependency_overrides, is a FastAPI feature that allows us to replace some dependencies at runtime. For example, we can replace the dependency that returns the database instance with another one that returns a test database instance. The second one is, once again, fixtures, which will help us create fake data in the test database before we run the tests.
To show you a working example, we’ll consider the same example we built in the Communicating with a MongoDB database with Motor section of Chapter 6, Databases and Asynchronous ORMs. In that example, we built REST endpoints to manage blog posts. As you may recall, we had a get_database dependency that returned the database instance. As a reminder, we show it again here:
database.py
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase
# Connection to the whole server motor_client = AsyncIOMotorClient("mongodb://localhost:27017") # Single database instance database = motor_client["chapter6_mongo"]
def get_database() -> AsyncIOMotorDatabase: return database
https://github.com/PacktPublishing/Building-Data-Science-Appli- cations-with-FastAPI-Second-Edition/tree/main/chapter6/mongodb/ database.py
Path operation functions and other dependencies would then use this dependency to retrieve the database instance.
For our tests, we’ll create a new instance of AsyncIOMotorDatabase that points to another database. Then, we’ll create a new dependency, directly in our test file, that returns this instance. You can see this in the following example:
chapter09_db_test.py
motor_client = AsyncIOMotorClient( os.getenv("MONGODB_CONNECTION_STRING", "mongodb:// localhost:27017") )
Testing an API Asynchronously with pytest and HTTPX
database_test = motor_client["chapter09_db_test"]
def get_test_database(): return database_test
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
Then, in our test_client fixture, we’ll override the default get_database dependency by using our current get_test_database dependency. The following example shows how this is done:
chapter09_db_test.py
@pytest_asyncio.fixture async def test_client(): app.dependency_overrides[get_database] = get_test_database async with LifespanManager(app): async with httpx.AsyncClient(app=app, base_url="http://app. io") as test_client: yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
FastAPI provides a property called dependency_overrides, which is a dictionary that maps original dependency functions with substitutes. Here, we directly used the get_database function as a key. The rest of the fixture doesn’t have to change. Now, whenever the get_database dependency is injected into the application code, FastAPI will automatically replace it with get_test_database. As a result, our endpoints will now work with the test database instance.
app and dependency_overrides are global Since we are directly importing app from its module, it’s instantiated only once for the whole test run. It means that dependency_overrides is common for every test. Keep this in mind if someday you want to override a dependency for a single test: once you’ve set it, it’ll be set for the rest of the execution. In this case, you can reset dependency_overrides by using app.dependency_overrides = {}.
Writing tests for REST API endpoints
To test some behaviors, such as retrieving a single post, it’s usually convenient to have some base data in our test database. To allow this, we’ll create a new fixture that will instantiate dummy PostDB objects and insert them into the test database. You can see this in the following example:
chapter09_db_test.py
@pytest_asyncio.fixture(autouse=True, scope="module") async def initial_posts(): initial_posts = [ Post(title="Post 1", content="Content 1"), Post(title="Post 2", content="Content 2"), Post(title="Post 3", content="Content 3"), ] await database_test["posts"].insert_many( [post.dict(by_alias=True) for post in initial_posts] )
yield initial_posts
await motor_client.drop_database("chapter09_db_test")
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
Here, you can see that we just had to make an insert_many request to the MongoDB database to create the posts.
Notice that we used the autouse and scope arguments of the fixture decorator. The first one tells pytest to automatically call this fixture even if it’s not requested in any test. In this case, it’s convenient because we’ll always ensure that the data has been created in the database, without the risk of forgetting to request it in the tests. The other one, scope, allows us, as we mentioned previously, to not run this fixture at the beginning of each test. With the module value, the fixture will create the objects only once, at the beginning of this particular test file. It helps make the test fast because, in this case, it doesn’t make sense to recreate the posts before each test.
Once again, we yield the posts instead of returning them. This pattern allows us to delete the test database after the tests run. By doing this, we’re making sure that we always start with a fresh database when we’ve run the tests.
Testing an API Asynchronously with pytest and HTTPX
And we are done! We can now write tests while knowing exactly what we have in the database. In the following example, you can see tests that are used to verify the behavior of the endpoint retrieving a single post:
chapter09_db_test.py
@pytest.mark.asyncio class TestGetPost: async def test_not_existing(self, test_client: httpx.AsyncClient): response = await test_client.get("/posts/abc")
assert response.status_code == status.HTTP_404_NOT_FOUND
async def test_existing( self, test_client: httpx.AsyncClient, initial_posts: list[Post] ): response = await test_client.get(f"/posts/{initial_posts[0]. id}")
assert response.status_code == status.HTTP_200_OK
json = response.json() assert json["_id"] == str(initial_posts[0].id)
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
Notice that we requested the initial_posts fixture in the second test to retrieve the identifier of the post that truly exists in our database.
Of course, we can also test our endpoints by creating data and checking whether it was correctly inserted into the database. You can see this in the following example:
chapter09_db_test.py
@pytest.mark.asyncio class TestCreatePost: async def test_invalid_payload(self, test_client: httpx. AsyncClient): payload = {"title": "New post"} response = await test_client.post("/posts", json=payload)
assert response.status_code == status.HTTP_422_UNPROCESSABLE_ ENTITY
Writing tests for REST API endpoints
async def test_valid_payload(self, test_client: httpx. AsyncClient): payload = {"title": "New post", "content": "New post content"} response = await test_client.post("/posts", json=payload)
assert response.status_code == status.HTTP_201_CREATED
json = response.json() post_id = ObjectId(json["_id"]) post_db = await database_test["posts"].find_one({"_id": post_ id}) assert post_db is not None
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ db_test.py
In the second test, we used the database_test instance to perform a request and check that the object was inserted correctly. This shows the benefit of using asynchronous tests: we can use the same libraries and tools inside our tests.
That’s all you need to know about dependency_overrides. This feature is also very helpful when you need to write tests for logic involving external services, such as external APIs. Instead of making real requests to those external services during your tests, which could cause issues or incur costs, you’ll be able to replace them with another dependency that fakes the requests. To understand this, we’ve built another example application with an endpoint for retrieving data from an external API:
chapter09_app_external_api.py
class ExternalAPI: def __init__(self) -> None: self.client = httpx.AsyncClient(base_url="https://dummyjson. com")
async def __call__(self) -> dict[str, Any]: async with self.client as client: response = await client.get("/products") return response.json()
external_api = ExternalAPI()
@app.get("/products")
Testing an API Asynchronously with pytest and HTTPX
async def external_products(products: dict[str, Any] = Depends(external_api)): return products
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_external_api.py
To call our external API, we’ve built a class dependency, as we saw in the Creating and using a parameterized dependency with a class section of Chapter 5, Dependency Injection in FastAPI. We use HTTPX as an HTTP client to make a request to the external API and retrieve the data. This external API is a dummy API containing fake data – very useful for experiments like this: https://dummyjson.com.
The /products endpoint is simply injected with this dependency and directly returns the data provided by the external API.
Of course, to test this endpoint, we don’t want to make real requests to the external API: it may take time and could be subject to rate limiting. Besides, you may want to test behavior that is not easy to reproduce in the real API, such as errors.
Thanks to dependency_overrides, it’s very easy to replace our ExternalAPI dependency class with another one that returns static data. In the following example, you can see how we implemented such a test:
chapter09_app_external_api_test.py
class MockExternalAPI: mock_data = { "products": [ { "id": 1, "title": "iPhone 9", "description": "An apple mobile which is nothing like apple", "thumbnail": "https://i.dummyjson.com/data/products/1/ thumbnail.jpg", }, ], "total": 1, "skip": 0, "limit": 30, }
async def __call__(self) -> dict[str, Any]: return MockExternalAPI.mock_data
Writing tests for WebSocket endpoints
@pytest_asyncio.fixture async def test_client(): app.dependency_overrides[external_api] = MockExternalAPI() async with LifespanManager(app): async with httpx.AsyncClient(app=app, base_url="http://app. io") as test_client: yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ app_external_api_test.py
Here, you can see that we wrote a simple class called MockExternalAPI that returns hardcoded data. All we have to do then is override the original dependency with this one. During the tests, the external API won’t be called; we’ll only work with the static data.
With the guidelines we’ve seen so far, you can now write tests for any HTTP endpoints in your FastAPI app. However, there is another kind of endpoint that behaves differently: WebSockets. As we’ll see in the next section, unit testing WebSockets is also quite different from what we described for REST endpoints.
Writing tests for WebSocket endpoints
In Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI, we explained how WebSockets work and how you can implement such endpoints in FastAPI. As you may have guessed, writing unit tests for WebSockets endpoints is quite different from what we’ve seen so far.
For this task, we’ll need to tweak our test_client fixture a little bit. Indeed, HTTPX doesn’t have built-in support to communicate with WebSockets. Hence, we’ll need to use a plugin, HTTPX WS. Let’s install it with the following command:
(venv) $ pip install httpx-ws
To enable support for WebSockets on our test client, we’ll change it like this:
chapter09_websocket_test.py
from httpx_ws.transport import ASGIWebSocketTransport
@pytest_asyncio.fixture async def test_client(): async with LifespanManager(app): async with httpx.AsyncClient( transport=ASGIWebSocketTransport(app), base_url="http:// app.io" ) as test_client:
Testing an API Asynchronously with pytest and HTTPX
yield test_client
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ websocket_test.py
You can see that, instead of directly setting the app argument, we set transport with a class provided by HTTPX WS. This class provides support to test apps with WebSockets endpoints. Other than that, nothing changes. It’s worth noting that testing standard HTTP endpoints will still work correctly, so you can use this test client for all your tests.
Now, let’s consider a simple WebSocket endpoint example:
chapter09_websocket.py
@app.websocket("/ws") async def websocket_endpoint(websocket: WebSocket): await websocket.accept() try: while True: data = await websocket.receive_text() await websocket.send_text(f"Message text was: {data}") except WebSocketDisconnect: await websocket.close()
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ websocket.py
You may have recognized the “echo” example from Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI.
Now, let’s write a test for our WebSocket using our test client:
Chapter09_websocket_test.py
from httpx_ws import aconnect_ws
@pytest.mark.asyncio async def test_websocket_echo(test_client: httpx.AsyncClient): async with aconnect_ws("/ws", test_client) as websocket: await websocket.send_text("Hello")
message = await websocket.receive_text() assert message == "Message text was: Hello"
https://github.com/PacktPublishing/Building-Data-Science-Applica- tions-with-FastAPI-Second-Edition/tree/main/chapter09/chapter09_ websocket_test.py
As you can see, HTTPX WS provides the aconnect_ws function to open a connection to a WebSocket endpoint. It expects the path of your WebSocket endpoint and a valid HTTPX client in an argument. By using test_client, we’ll make requests directly against our FastAPI application.
It opens a context manager, giving you the websocket variable. It’s an object that exposes several methods to either send or receive data. Each of those methods will block until a message has been sent or received.
Here, to test our “echo” server, we send a message thanks to the send_text method. Then, we retrieve a message with receive_text and assert that it corresponds to what we expect. Equivalent methods also exist for sending and receiving JSON data directly: send_json and receive_json.
This is what makes WebSocket testing a bit special: you have to think about the sequence of sent and received messages and implement them programmatically to test the behavior of your WebSocket.
Other than that, all the things we’ve seen so far regarding testing are applicable, especially dependency_ overrides, when you need to use a test database.
Summary
Congratulations! You are now ready to build high-quality FastAPI applications that have been well tested. In this chapter, you learned how to use pytest, a powerful and efficient testing framework for Python. Thanks to pytest fixtures, you saw how to create a reusable test client for your FastAPI application that can work asynchronously. Using this client, you learned how to make HTTP requests to assert the behavior of your REST API. Finally, we reviewed how to test WebSocket endpoints, which involves a fairly different way of thinking.
Now that you can build a reliable and efficient FastAPI application, it’s time to bring it to the whole world! In the next chapter, we’ll review the best practices and patterns for preparing a FastAPI application for the world before studying several deployment methods.
Summary